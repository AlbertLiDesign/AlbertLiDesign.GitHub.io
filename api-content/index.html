{"posts":[{"title":"【Mesh is Art】第一章 常见几何表示方法","content":"第一章 常见几何表示方法 很多三维建模相关专业的学生在学习建模软件时常常会感到困惑，由于不了解三维建模中各种隐含的规定，缺乏三维建模的相关知识，常常误以为是软件的bug，最后往往变成了“国家一流软件安装与卸载师”，三维建模的学习过程也逐渐变成了“从入门到放弃”的过程。那么我们应该如何去学习三维建模呢？我们可以先从计算机中常见的几何表示方法开始，本节将讨论了计算机中不同的三维几何表示方法，了解这些方法可以对当前计算机建模软件的建模方式有一个宏观的认识，这有助于在完成设计或计算任务前选择恰当合适的软件进行使用和学习。当然，对于有些方法可能过于抽象，读者不需要完全理解，有一个大体的认识即可。 参数表达与隐式表达 曲面通常有两种表示方法：参数表达 (Parametric representations) 和隐式表达 (Implicit representations)。根据《Polygon Mesh Processing》中的解释，参数曲面是由向量值的参数化函数来定义的，它将二维参数域映射到了曲面上。而隐式曲面是由一个方程来定义的，方程左侧为表示曲面上点的坐标的三维参数的标量表达式，等式右边为0。为方便理解，我们以一个平面上的单位圆为例， 它的参数表达为隐式表达为通俗的理解是，参数表达在曲面上的点都可以找到明确的坐标定义，而隐式表达不会显式地表达明确的点的位置，而是去表达这些点的位置所满足的关系，它是通过点坐标不同分量之间的关系来定义的。例如在单位圆示例中，参数表达直接表示了所有点的坐标，即，而隐式表达只表示出了两个分量所满足的关系，如果我们想要知道每个点的坐标，需要通过给定的关系来计算得到。 由于两种表达方式的定义方法不同，它们也有着不同的特性。由于参数表达直接记录了点的坐标，因此是一种很直观的表示方法，计算机可以直接读取坐标来显示出几何。此外，由于它是通过参数方程来表示曲面的，所以很多在曲面上的三维问题可以被转化成二维参数域上的问题。但是判断位置关系等问题对它来说十分棘手，需要通过一些算法来解决，计算成本较高。例如要想判断一个点是否在物体内部，对于参数表达的曲面来说是一件成本较高的事。然而这对于隐式表达就会十分容易，因为它可以直接将点的坐标代入到方程中，通过计算即可知道该点与物体之间的位置关系，当然，隐式表达的缺点就是表达不直观，仅根据表达式我们很难知道它所表示的物体，尤其是对于复杂几何，其显式地表现出几何的成本过高。因此两种表示方法各有优劣，我们需要根据实际需求和两种表示方法的特点来选取成本较低的表示方法。下面我们来逐一介绍常用的几何表示方法。 参数表达 隐式表达 定义 由参数方程定义，明确地表达了点的位置 由隐式方程定义，定义了点坐标中不同的分量所满足的关系 优点 1. 表示直观；2. 方便将三维问题转化为二维参数域上的问题 求交等问题成本更低 缺点 在求交等问题上成本过高 表达不直观 参数表达方法 点云 让我们从相对容易理解的参数表达开始入手，通常在描述一个三维模型时，最基本的描述方式是使用点去描述，但是要想表达出一个三维模型，需要大量且密集的点，这些点称之为点云 (Point Cloud) 。三维空间中一个点由三个方向的坐标构成，通常记作 (x,y,z)，因此，点云可以被视作是一堆点的集合，存储时即可用一系列 (x,y,z) 坐标来表示。注意 描述三维模型的点云数据是无序排布的，这意味着点云在三维空间中，仅表示一系列点，点与点之间没有任何联系。 点云是表示三维几何模型中最简单的方法，它可以表示任意类型的几何物体，代价是必须拥有足够密集的点云信息才能避免物体失真。 点云通常是由三维扫描得到的，三维扫描仪通过记录空间中被扫描物体的点云信息，再使用表面重建算法来得到一个完整的三维扫描模型的。因此，点云通常会被转化成网格来做后续处理。 样条曲面 在当今的CAD软件中，**样条曲面 (Spline Surfaces) 是最常用的表示方法之一，尤其是在机械、工业设计等领域，对曲面质量要求较高的行业中应用广泛。样条曲面表达一般指非均匀有理B样条 (NURBS)，**它是由分段多项式或有理B样条基函数来描述的。NURBS建模可以使用户设计出质量极高的曲面，但是对于一些较复杂的几何形体，它需要通过多张曲面拼接来实现，为了保证曲面质量，用户需要处理大量由拼接引起的曲面连续性问题，这带来了高昂的建模成本。另一个缺点是，添加更多顶点操作需要通过分割参数域来实现，这使得曲面的局部修改与设计变得相对困难。对于NURBS建模的更多细节，可以参阅[Farin 97,Piegl and Tiller 97,Prautzsch et al. 02]。 多边形网格 在CAD软件中，除样条曲面表达外，另一种被广泛应用的三维模型表示方法就是多边形网格 (Polygon Mesh)。它通常是指由三角形或四边形拼接而成的几何模型，几乎任何几何形状都可以通过拆分成无数三角形或四边形面片来表达。由于网格在图形学领域被广泛研究，各种处理算法层出不穷，因此它已经变成了当前图形学最普遍的图形表示方法。无论是游戏场景建模、概念设计建模还是分析与模拟，都是通过编辑和处理网格单元来实现的。最基础的网格数据结构将网格所描述的信息分成了两个部分：几何信息 (Geometry Information) 和拓扑信息 (Topology Information) 。几何信息描述了网格上的顶点的坐标，拓扑信息描述了网格上的顶点的连接关系。图2展示了一个网格模型的实例，左侧网格模型上的顶点按照顺序标记了它们的索引 (Index) 。右侧几何信息中，按顺序描述了每一个顶点的三维坐标，例如0号顶点的坐标为{-10.834683, 19.656748, 0}，由于这是一个平面网格模型，因此Z坐标均为0。拓扑信息描述了每一张网格面由哪些顶点构成，其中T表示三角形 (Triangles)，Q表示四边形 (Quadrangle)，例如T{0; 2; 1} 描述了第0张网格面片为三角形，它是由第0、2、1号点按顺序构成的三角形。 细分曲面 细分曲面是一种通过粗糙网格来控制的 References [1] Botsch, M., Kobbelt, L., Pauly, M., Alliez, P. and Lévy, B., 2010. Polygon mesh processing. CRC press.[2] Piegl, L. and Tiller, W., 2012. The NURBS book. Springer Science &amp; Business Media.[3] Farin, G., 2014. Curves and surfaces for computer-aided geometric design: a practical guide. Elsevier. ","link":"https://albertlidesign.github.io/post/mesh-is-art-di-yi-zhang-chang-jian-ji-he-biao-shi-fang-fa/"},{"title":"Mesh is Art (4): Subdivision","content":" 前言 上文我们介绍了半边网格的底层架构，介绍了点、半边和面所携带的信息。本文我们来探讨一个半边网格的应用——网格细分。网格细分是一种高效地表达几何的方法。 网格细分简述 在 Mesh is Art（1) 中，我们讲解了网格数据大多数由三角形或四边形组成。网格细分技术为分割曲面提供了解决方案。这种技术的核心是关注如何通过细分算法（Subdivision）计算来用大量的较小的网格面来替代原来的曲面，从而细分并优化输入的基础网格面。并且由此产生的精密网格还可以使用相同的算法来产生更精密的网格，如此可以反复迭代。随着迭代步的增加，网格的数量也不断地翻倍，从而更加逼近于精确、光滑的初始曲面。 网格细分技术已经被广泛地应用于游戏和动画产业当中，在项目中常采用非常粗糙的基础网格来表达一个物体（大概只需要几百个多边形），因此存储时所占用的计算机资源非常少。我们知道动画和游戏中常常需要渲染模型，对于模型的渲染时，并不是所有模型都要达到几十万网格面级别再去渲染，这样不仅增加了模型场景中的负担，还浪费了许多时间。在一个场景中，通常只有距离摄像机近、需要突出表达细节的模型才会被渲染地非常清晰，其他背景、场景往往不需要那么高精度的表达。因此，网格细分技术变得十分重要，设计师只需要建立一些粗糙网格模型，当该模型需要被表达时，将其细分，不需要表达细节时，就保持不变。 在建筑设计中，许多建筑在设计阶段是由NURBS曲面这样的光滑连续曲面建模技术来设计的。然而，这些模型在几何学方面很容易操作（很容易在计算机中建模），但它们很难被精确地制造出来，由于加工尺度问题，在实际项目中不可避免地要将它们分割成小构件，从而制作出能安装在结构上的表面嵌板（如图3）。在建造中，即使最后完成时是一个单体结构，通常也需要采用离散单元来拼合的传统方法。因此，这些项目在建模时经常使用离散网格来表示曲面（如图4、5），来代替一张参数化连续的NURBS曲面（至少在工程和制造阶段上是这样）。在 Mesh is Art（2） 中，也简述了一个由细分算法主导的建筑设计实例。 网格细分除了在建造上表达曲面的优势外，还常被用于分析和优化。 在幕墙优化中，细分曲面常用来控制嵌板的尺寸以及辅助其他优化算法。比如当设计师设计了一张曲面，需要将该曲面铺上嵌板，设计师可以使用细分算法对该曲面不断地迭代细分，直至嵌板的尺寸符合可加工的尺寸，接着可进行更进一步地平板优化和规格优化，当然这两个问题先放到后面了。 在对于需要执行多目标优化的曲面来说，网格细分同样非常有用。在设计和施工的过程中，通常要对建筑曲面进行优化来提高其性能。结构优化通常要求在最大限度节省材料的前提下尽可能小地改变形状，优化的实现可以通过将结构构件仅放置在所需要的地方来尽可能减少结构构件的数量，或者通过调整结构构件以便有效地将载荷传递到支撑来减少结构构件的大小。改变建筑物的形状可以减小风和雪带来的荷载，当然也可以通过考虑热环境、通风和采光来优化建筑的场地、形状和遮阳，从而提高建筑的环境性能。 这些优化目标都需要对建筑模型采用一种特殊表式方法来分析其性能。这些分析需要大量离散的类似网格的表示方法，并且所需要的网格的密度会取决于正在执行的分析类型。例如有限元结构分析可能会要求网格的离散方式能够反映给出的支撑结构，计算风流分析可能需要更精密的分析网格，而太阳能生成计算会相对精确，可能只需要比较粗糙的网格来表示即可。连续壳体的有限元结构分析需要形状函数来在两节点间插入几何结构，而细分曲面也可用于此。 网格细分可以看作是一种圆滑的过程，细分得到的网格子顶点坐标是由周围相邻父顶点的平滑平均值产生。用这种方式，网格的坐标被圆滑，形成更接近初始的近似曲面。这一点已经被证明（Zorin et al.，2000），这种生成的近似曲面在非边界和非奇异顶点处都是C2连续的，这意味着形态表面（没有缝隙）、表面切线（没有折痕）以及切线的变化率（在视觉反射中没有变形）都没有突变（这三点分别是C0，C1和C2连续性的判断标准）。这也是细分网格看上去特别美观的原因。 圆滑过程的另一个优点是，细分不仅能可以圆滑顶点坐标，任何一组与顶点关联的数据都会被圆滑，如果一个初始的粗糙网格中每一个顶点附带着一个颜色的信息，所有连续细分的网格都能够自动匹配圆滑曲面上的颜色。可以类似地应用于更多实际的参数，例如顶点携带的贴图信息、应力值、立面渗透率或透明度值，百叶窗角度或者覆层的偏移距离。在基础网格上，这些数据可以被定义在所需要的顶点上，细分网格后，这些信息将完全以与顶点位置相同的C2连续的方式均匀地分布在整个曲面上。 细分算法原理 实现网格细分有多种方案，有基于三角形网格的，基于四边形网格，也有基于高阶多边形及混合网格的。细分算法的基本过程首先是从一张网格面开始，用某种方式在网格上添加点，并对拓扑结构进行优化。本文中着重讨论两个细分算法的原理及C#实现。 Loop细分算法 原理 Loop细分算法（Loop，1992）是一种应用在三角形网格的算法，如图6所示，这是从一个球形网格中提取的一张网格面，蓝色顶点为父顶点，即网格面原有的顶点，红色顶点为新创建的子顶点，每个三角形上的每条边都被新引入的子顶点分割成了两段，然后每一个三角形都被四个新的小三角形代替。 内部子顶点：设内部边的两端点v0v_0v0​，v1v_1v1​，其相对的两个顶点为v2v_2v2​，v3v_3v3​，要创建的子顶点为vnewv_{new}vnew​，则 vnew=38(v0+v1)+18(v2+v3)v_{new} = \\frac{3}{8}(v_0+v_1)+\\frac{1}{8}(v_2+v_3) vnew​=83​(v0​+v1​)+81​(v2​+v3​) 内部父顶点：设内部父顶点v0v_0v0​的相邻点为v1v_1v1​，v2v_2v2​，...，vnv_nvn​，更新后的父顶点为vnewv_{new}vnew​，w为权重值，nnn为顶点的价（该顶点的相邻点的数量），则 w=1n(58−(38+14cos2πn)2)w=\\frac{1}{n}(\\frac{5}{8}-(\\frac{3}{8}+\\frac{1}{4}cos\\frac{2\\pi}{n})^2) w=n1​(85​−(83​+41​cosn2π​)2) vnew=(1−nw)v0+w∑i=1nviv_{new} = (1-nw)v_0+w\\sum_{i=1}^{n} {v_i} vnew​=(1−nw)v0​+wi=1∑n​vi​ 在细分算法里曲面的边缘通常需要被特殊处理，比如一条只通过一个面的边（面的边缘）。如图8-a所示，沿着边界创建的子顶点会被放置在边界上的边的中点。如图8-b所示，边界上的边的父顶点会被放置在以原始位置为 34\\frac{3}{4}43​ 权重值和相邻两个顶点位置各占 18\\frac{1}{8}81​ 权重值的位置，而不会考虑内部邻近顶点的位置。 边界子顶点：设边界边的两个端点为v0v_0v0​和v1v_1v1​，要创建的子顶点为vnewv_{new}vnew​，则 vnew=v0+v12v_{new} = \\frac{v_0+v_1}{2} vnew​=2v0​+v1​​ 边界父顶点：设边界父顶点v0v_0v0​的相邻点为v1v_1v1​和v2v_2v2​，更新后的父顶点为vnewv_{new}vnew​，w为权重值，nnn为顶点的价（该顶点的相邻点的数量），则 vnew=34v0+18(v1+v2)v_{new} = \\frac{3}{4}v_{0}+\\frac{1}{8}(v_1+v_2) vnew​=43​v0​+81​(v1​+v2​) 代码实现 在Mesh is Art（3）中，我们讲解了半边网格的特性，即半边网格在处理邻域查找问题上尤为高效。因此在实现网格细分算法中，半边网格是最适合的网格数据结构，这也是为什么我一直在强调使用半边数据结构，类似细分一样大量使用网格邻域查找的算法还有很多。简略版的Loop细分的C#源码如下： public Mesh Loop(Mesh m) { PlanktonMesh P = m.ToPlanktonMesh(); PlanktonMesh NP = new PlanktonMesh(); PlanktonMesh TP = P; for (int v = 0; v &lt; P.Vertices.Count; v++) { // Loop int[] Neighbours = P.Vertices.GetVertexNeighbours(v); int Valence = Neighbours.Length; double Beta = 0.625 - (Math.Pow((3 + 2 * Math.Cos((2 * Math.PI) / Valence)), 2) * 0.015625); double Mult = Beta / Valence; Point3d NewPosition = P.Vertices[v].ToPoint3d() * (1 - Beta); foreach (int Neighbour in Neighbours) { NewPosition += P.Vertices[Neighbour].ToPoint3d() * Mult; } NP.Vertices.Add(NewPosition); } for (int HE = 0; HE &lt; P.Halfedges.Count; HE += 2) { int Pair = P.Halfedges.GetPairHalfedge(HE); int ThisOpp = P.Halfedges[HE].PrevHalfedge; int PairOpp = P.Halfedges[Pair].PrevHalfedge; int HeSV = P.Halfedges[HE].StartVertex; int PrSV = P.Halfedges[Pair].StartVertex; // split the halfedges in the topology lookup mesh TP.Halfedges.SplitEdge(HE); // ensure that each face has a starting halfedge on an even vertex if (P.Halfedges[Pair].AdjacentFace &gt; -1) TP.Faces[P.Halfedges[Pair].AdjacentFace].FirstHalfedge = TP.Halfedges.Count - 1; // Loop Point3d NewPosition = P.Vertices[HeSV].ToPoint3d() * 0.375 + P.Vertices[PrSV].ToPoint3d() * 0.375 + P.Vertices[P.Halfedges[ThisOpp].StartVertex].ToPoint3d() * 0.125 + P.Vertices[P.Halfedges[PairOpp].StartVertex].ToPoint3d() * 0.125; NP.Vertices.Add(NewPosition); } for (int f = 0; f &lt; P.Faces.Count; f++) { // add new triangulated faces int[] FaceVerts = TP.Faces.GetFaceVertices(f); NP.Faces.AddFace(FaceVerts[0], FaceVerts[1], FaceVerts[5]); NP.Faces.AddFace(FaceVerts[2], FaceVerts[3], FaceVerts[1]); NP.Faces.AddFace(FaceVerts[4], FaceVerts[5], FaceVerts[3]); NP.Faces.AddFace(FaceVerts[1], FaceVerts[3], FaceVerts[5]); } return NP.ToRhinoMesh(); } Catmull-Clark细分算法 原理 Catmull-Clark细分是一种基于四边形的网格细分算法，它所产生的网格能够达到C2C^{2}C2连续，奇异点处为C1C^1C1连续。和Loop类似，其实就是父的更新法则和子顶点的创建法则有不同。对于内部顶点，位于面中的子顶点创建法则为该面顶点的加权平均，位于边界的子顶点的创建法则是内部边两端点的 38\\frac{3}{8}83​ ，过该边的两张网格面的其他顶点占 116\\frac{1}{16}161​ 。父顶点的更新法则分两种，与父顶点相邻的顶点的权重为β/k（β=3/2k，k为顶点的价），与父顶点共面却不相邻的顶点的权重为γ/k（γ=1/4k），父顶点挪动前的权重为1-β-γ。边界顶点的处理和Loop算法完全相同，这里不再多做解释了。 位于面中的内部子顶点：设四边形的四个顶点分别为v0v_0v0​，v1v_1v1​，v2v_2v2​，v3v_3v3​，要创建的子顶点为vnewv_{new}vnew​，则 vnew=v0+v1+v2+v34v_{new} = \\frac{v_0+v_1+v_2+v_3}{4} vnew​=4v0​+v1​+v2​+v3​​ 位于内部边上的子顶点：设内部网格边的两端为v0v_0v0​和v1v_1v1​，与该边相邻的两个四边形顶点分别为v0v_0v0​-v1v_1v1​-v2v_2v2​-v3v_3v3​和v0v_0v0​-v1v_1v1​-v4v_4v4​-v5v_5v5​要创建的子顶点为vnewv_{new}vnew​，则 vnew=38(v0+v1)+116(v2+v3+v4+v5)v_{new} = \\frac{3}{8}(v_0+v_1)+\\frac{1}{16}(v_2+v_3+v_4+v_5) vnew​=83​(v0​+v1​)+161​(v2​+v3​+v4​+v5​) 内部父顶点：设内部父顶点v0v_0v0​的相邻点为v1v_1v1​，v2v_2v2​，...，v2nv_2nv2​n，更新后的父顶点为vnewv_{new}vnew​，β和γ均为权重值，nnn为顶点的价（该顶点的相邻点的数量），则 β=\\frac{3}{2n}$$$$γ=\\frac{1}{4n}$$$$v_{new} = (1-β-γ)v_0+\\frac{β}{n}\\sum_{i=1}^{n} {v_{2i}}+\\frac{γ}{n}\\sum_{i=1}^{n} {v_{2i-1}} 边界子顶点：设边界边的两个端点为v0v_0v0​和v1v_1v1​，要创建的子顶点为vnewv_{new}vnew​，则 vnew=v0+v12v_{new} = \\frac{v_0+v_1}{2} vnew​=2v0​+v1​​ 边界父顶点：设边界父顶点v0v_0v0​的相邻点为v1v_1v1​和v2v_2v2​，更新后的父顶点为vnewv_{new}vnew​，w为权重值，nnn为顶点的价（该顶点的相邻点的数量），则 vnew=34v0+18(v1+v2)v_{new} = \\frac{3}{4}v_{0}+\\frac{1}{8}(v_1+v_2) vnew​=43​v0​+81​(v1​+v2​) 代码实现 简略版的Catmull-Clark细分的C#源码如下： public Mesh CatmullClark(Mesh m) { PlanktonMesh P = m.ToPlanktonMesh(); PlanktonMesh NP = new PlanktonMesh(); PlanktonMesh TP = m.ToPlanktonMesh(); for (int v = 0; v &lt; P.Vertices.Count; v++) { // Catmull-Clark int[] InHEs = P.Vertices.GetIncomingHalfedges(v); double Beta = 3.0 / (2 * InHEs.Length); double Delta = 1.0 / (4 * InHEs.Length); double BetaK = Beta / InHEs.Length; double DeltaK = Delta / InHEs.Length; Point3d NewPosition = P.Vertices[v].ToPoint3d() * (1 - Beta - Delta); foreach (int InHE in InHEs) { NewPosition += P.Vertices[P.Halfedges[InHE].StartVertex].ToPoint3d() * BetaK; NewPosition += P.Vertices[P.Halfedges[P.Halfedges[InHE].PrevHalfedge].StartVertex].ToPoint3d() * DeltaK; } NP.Vertices.Add(NewPosition); } for (int HE = 0; HE &lt; P.Halfedges.Count; HE += 2) { int Pair = P.Halfedges.GetPairHalfedge(HE); int ThisOpp = P.Halfedges[HE].PrevHalfedge; int PairOpp = P.Halfedges[Pair].PrevHalfedge; int HeSV = P.Halfedges[HE].StartVertex; int PrSV = P.Halfedges[Pair].StartVertex; // split the halfedges in the topology lookup mesh TP.Halfedges.SplitEdge(HE); // ensure that each face has a starting halfedge on an even vertex if (P.Halfedges[Pair].AdjacentFace &gt; -1) TP.Faces[P.Halfedges[Pair].AdjacentFace].FirstHalfedge = TP.Halfedges.Count - 1; // Catmull-Clark Point3d NewPosition = P.Vertices[HeSV].ToPoint3d() * 0.375 + P.Vertices[PrSV].ToPoint3d() * 0.375 + P.Vertices[P.Halfedges[P.Halfedges.GetPairHalfedge(P.Halfedges[HE].NextHalfedge)].StartVertex].ToPoint3d() * 0.0625 + P.Vertices[P.Halfedges[P.Halfedges[HE].PrevHalfedge].StartVertex].ToPoint3d() * 0.0625 + P.Vertices[P.Halfedges[P.Halfedges.GetPairHalfedge(P.Halfedges[Pair].NextHalfedge)].StartVertex].ToPoint3d() * 0.0625 + P.Vertices[P.Halfedges[P.Halfedges[Pair].PrevHalfedge].StartVertex].ToPoint3d() * 0.0625; NP.Vertices.Add(NewPosition); } for (int f = 0; f &lt; P.Faces.Count; f++) { int CenterIdx = NP.Vertices.Count; NP.Vertices.Add(P.Faces.GetFaceCenter(f).ToPoint3d()); int[] FaceVerts = TP.Faces.GetFaceVertices(f); for (int nf = 0; nf &lt; FaceVerts.Length; nf += 2) { int LastVert = nf - 1; if (nf == 0) LastVert = FaceVerts.Length - 1; NP.Faces.AddFace(FaceVerts[nf], FaceVerts[nf + 1], CenterIdx, FaceVerts[LastVert]); } } return NP.ToRhinoMesh(); } ","link":"https://albertlidesign.github.io/post/MeshIsArt4/"},{"title":"Mesh is Art (3): Half-Edge Data Structure","content":" #前言 在前文中，作为预热，我们大概说明了网格的某些特性在实际工程中的应用，由于后面还有更多的网格算法的讲解，要想理解这些算法，我们需要一套基本的处理网格的工具，所以这次继续回到关于网格的数据结构上，从底层架构讲起。在第一篇，我们讲述了半边数据结构，这是一种在相邻元素查找问题上非常高效实用的数据结构，广泛应用于很多网格编辑算法中。本文将重点剖析这种数据结构所带来的优势以及组成原理。 #半边数据结构的底层架构 半边数据结构（Half-Edge Data Structure）是一个以网格边为基础的数据结构，在这种数据结构中，我们认为网格的每条边被划分为两条方向相反的半边（HalfEdge，如图1）。为了帮助大家更好的理解，还请跟着我的思路，从几何的角度一步步体验这种数据结构的妙处。不难想到，如果我们对一张网格面中的所有边按照顺序做同样的操作，我们可以在每一张网格面的内部得到一个闭合的、首尾相接的“回路”。接下来我们对半边网格所包含的元素分别做讲解。 ##半边 类似向量，每一条半边都有它的起点（Start Vertex）和终点（End Vertex），首先，起点和终点对调，我们可以得到一条半边的反向半边（Opposite Halfedge），其次，根据终点我们可以找到以一条半边的终点为起点的半边，同理，也可以找到以一条半边的起点为终点的半边，这样就定义了一条半边的下一条半边（Next Halfedge）和上一条半边（Prev Halfedge）。 这里，我们可能会抛出一个问题，正如我们第一篇所讲到的，网格是有流形网格和非流形网格的，那么我们能否用半边网格去表示非流形网格？所谓非流形网格，最简单的例子就是一条网格边连接了三张网格面，但是每条边只能被划分成方向相反的两条半边。如图3， 我们可以很轻松地对两张面画出它们的半边，但是对于那条非流形边（三张面的交线），如果按照半边网格的画法，它需要三个方向的半边，但是方向只有正负，我们没有办法画出第三张面的半边。因此，半边网格是无法表示非流形网格的。 接着，我们继续来挖掘这种数据结构所带来的便利。如前文所述，这是一种查找相邻元素极其高效的数据结构。因为半边的划分，我们能在每一张网格面的内部**得到一个闭合的、首尾相接的“回路”，**这这里其实我们就已经将一张网格面内部的半边看作是这张网格面的属性了。举个例子，如图4，半边1-&gt;0，0-&gt;2，2-&gt;3和3-&gt;1，它们属于绿色网格面。那如果我们要检索与绿色网格面相邻的网格面，我们只需要求边2-&gt;3的反向半边所在的网格面即可。以此类推，我们会发现，我们可以将相邻网格面的索引作为一条半边的属性，**对于一个封闭流形网格，每条边的半边都会伴随一张唯一的相邻网格面（Adjacent Face）的索引，对于开放流形网格，我们可以让这个索引的值设为-1。**这样一来，检索相邻网格面几乎变成了零成本，因为这个数据从一开始就已经记录好了。 了解了这些信息，我们已经很容易去定义半边了，在Plankton（Daniel Piker开发的C#半边网格库）中，一条半边的构造方法只需要起始点（StartVertex）、相邻网格面的索引（AdjacentFace）、下一条半边（NextHalfedge）和上一条半边（PrevHalfedge）。 注意：终点不再被需要，因为它可以通过检索下一条半边的起点来得到。反向半边也不需要，因为半边是成对存在的，获取反向半边可以通过已知半边的索引来查找。（C#代码：halfedgeIndex % 2 == 0 ? halfedgeIndex + 1 : halfedgeIndex - 1;） ##顶点 先来思考一个问题，在最常用的面-顶点网格数据结构中，当多个顶点重合时，我们会用网格焊接（Weld）来处理重复的顶点，并求这些顶点的法向量的合向量，使网格看起来更圆滑。那么半边网格有没有这个概念呢？答案是没有的，举个例子，如图4，在面-顶点网格数据结构中，3号点理应是两个重合点，分别属于绿色网格面和红色网格面，如果半边网格也是两个重合点，那么边2-&gt;3应该会产生4条半边，就变成了两张独立的网格面。因此在一个半边网格内不存在重合点的情况。 顶点除了常规网格数据结构中记录的XYZ三个坐标外，还有一个**以此顶点为源点的半边连接指向信息，即出半边（Outgoing Halfedge）。**此外，半边数据结构还规定了，**对于位于边界的顶点，出半边必为边界半边。**但是我们知道，只要不在网格边界内部的顶点都会与多条半边相连，半边数据结构的顶点不存在重合点情况，那么一个连接多条半边的顶点的出半边是什么呢？ 为了探究这个问题，我们做一个小测试，如图5所示，调用了Plankton半边网格库，把顶点索引、选取顶点和该顶点的出半边显示出来，以便观察规律。 测试代码如下： private void RunScript(Mesh x, int y, ref object Point, ref object OutgoingHalfEdge, ref object VertexIndices, ref object OutgoingStartPoint) { PlanktonMesh pm = x.ToPlanktonMesh(); Point = pm.Vertices[y].ToPoint3d(); Point3d startpoint = pm.Vertices[pm.Halfedges[pm.Vertices[y].OutgoingHalfedge].StartVertex].ToPoint3d(); Point3d endpoint = pm.Vertices[pm.Halfedges[pm.Halfedges[pm.Vertices[y].OutgoingHalfedge].NextHalfedge].StartVertex].ToPoint3d(); OutgoingHalfEdge = new Vector3d(endpoint - startpoint); OutgoingStartPoint = startpoint; List&lt;Point3d&gt; vertices = new List&lt;Point3d&gt;(); for (int i = 0; i &lt; pm.Vertices.Count; i++) { vertices.Add(pm.Vertices[i].ToPoint3d()); } VertexIndices = vertices; } 首先，根据“位于边界的顶点，出半边必为边界半边”这条性质，我们先确定出半边的走向，如图6所示。图中，我们查看2号点的出半边，是由2号点指向1号点，出半边是边界半边，所以是位于三角面片的外侧，以此类推可以画出整张网格面的半边走向。 下面我们来添加一张网格面，就以2号点为连接点，添加3号点，生成第二张面（如图7）。我们可以看到出半边马上由2-&gt;1变成了2-&gt;3，这还是因为“位于边界的顶点，出半边必为边界半边”。 然后我们继续添加4号点，构成面2-3-4，发现出半边继续变化，由2-&gt;3变成了2-&gt;4（图8），紧接着最后一步，添加面0-2-4，此面添加后，2号点由边界顶点变成了内部顶点，它的出半边没有变化，出半边锁定在了将它变成内部顶点前的状态（图9）。 通过以上尝试，我们发现了，对于内部顶点，其出半边为其被封闭前（添加最后一个面片即被封闭）的出半边。 ##面 半边数据结构的网格的构建通常是通过面列表来创建的，也就是说，正常的构建半边数据结构网格是通过一个一个面片的添加来构建的。半边网格的面仅包含半边网格的第一条半边（FirstHalfedge）的索引。因为每条半边对应一个起点，添加面时要按顺序添加顶点，只凭第一个顶点无法确定面的法向，但是顶点对应的半边可以。另外一点，因为半边已经记录了下一条半边和上一条半边的信息，面就不再需要顶点的拓扑信息了。因此第一条半边决定了“环路”的方向，决定了面的法向，还决定了顶点的拓扑关系。 最后我们再思考最后一个问题，**半边数据结构能表达边数多于4的多边形网格吗？**答案是肯定的，根据半边数据结构的底层架构我们能看到这种数据结构下已经不再受限于面片中的边的数量，一切关系都由半边紧密连接。 ##总结 **顶点(Vertex)：**包含顶点坐标，出半边（OutgoingHalfedge）的索引。 **半边(HalfEdge)：**包含起始点（StartVertex）、邻接面(AdjacentFace)、下一条半边(NextHalfedge)、上一条半边(PrevHalfedge)的索引。 **面片(Face)：**包含一条起始边（FirstHalfedge）的索引。 ##特性 （1）半边数据结构不能表达非流形网格； （2）半边数据结构能够表达边数&gt;4的多边形网格； （3）半边数据结构多用来处理相邻元素检索的问题。 ","link":"https://albertlidesign.github.io/post/meshisart3/"},{"title":"GAMES101(5): Geometry","content":"课程链接：GAMES101-现代计算机图形学入门-闫令琪 课程讲师：闫令琪 本系列笔记为本人根据学习该门课程的笔记，仅分享出来供大家交流，希望大家多多支持GAMES相关讲座及课程，如涉及侵权请联系我删除：albertlidesign@gmail.com 几何的表示方法 几何分为隐式几何（Implicit geometry）和显式几何（Explicit geometry）。我们有不同的方式来表示不同的几何。我们从隐式几何表示开始。 Implicit Geometry 隐式几何表示方法不会表达明确的点的位置，而是去表达这些点满足的关系，也就是说，对于一些满足一定关系的点，我们可以通过隐式几何来确定它们所在的满足一定关系的表面上。举个例子，比如一个单位球，在三维空间中可以表示成x2+y2+z2=1x^2+y^2+z^2 =1x2+y2+z2=1，即给出任何一个(x,z,y)(x,z,y)(x,z,y)我们都可以判断该点是否在定义的表面上，因此它就是球的隐式表示。球的显式表示方法，最简单的就是将其拆成若干个三角形或四边形网格。另外一个例子将这个概念推广了，我们定义任何一个(x,y,z)(x,y,z)(x,y,z)满足的关系，那这个关系自然就是一个函数了，也就是说，只要一个点满足这个函数，那么我们就可以说这个点在这个表面上，因此球的例子可以改成f(x,y,z)=x2+y2+z2−1=0f(x,y,z) = x^2+y^2+z^2-1=0f(x,y,z)=x2+y2+z2−1=0。因此我们可以直接定义一个函数，只要能找到一个点的x,y,zx,y,zx,y,z满足这个函数，就认为这个点在表面上，只要能找出所有满足关系的点，我们就能把这个表面画出来。如上图所示，对于函数f(x,y)f(x,y)f(x,y)，蓝色区域为函数值为负的区域，红色区域为函数值为正的区域，白色轮廓线就是f(x,y)=0f(x,y)=0f(x,y)=0的区域。 隐式几何有好处也有坏处，例如f(x,y,z)=(2−x2+y2)2+z2−1f(x,y,z) = (2-\\sqrt{x^2+y^2})^2+z^2-1f(x,y,z)=(2−x2+y2​)2+z2−1，我们如何找到令f(x,y,z)=0f(x,y,z)=0f(x,y,z)=0的点呢？这是一个相对困难的事，也很难看出来，我们将其结果绘出如上图所示，它其实是一个圆环，仅从函数来看很难看出它是一个圆环的结构，也就是说隐式几何的表示不直接。 当然隐式几何也有好处，它判断一个点与表面的关系非常容易，只需要将点代入到函数中，根据结果的正负值即可判断该点在表面内、在表面上还是在表面外。例如f(x,y,z)=x2+y2+z2−1f(x,y,z) = x^2+y^2+z^2-1f(x,y,z)=x2+y2+z2−1，我们判断点(34,12,14)(\\frac{3}{4},\\frac{1}{2},\\frac{1}{4})(43​,21​,41​)与表面的位置关系，将其代如得f(x,y,z)=−18f(x,y,z) = -\\frac{1}{8}f(x,y,z)=−81​，因此可知该点在表面内。 Explicit Geometry 相对地，图形学还有显式几何表达，比如之前我们用到的三角形，就是真的将表面显式地表达出来。还有一种显式的表达方法，听起来不直观，称为通过参数映射的方法定义的表面。例如我们在平面上定义一个空间，上面任意一个点用(u,v)(u,v)(u,v)表示，并且我们可以定义，对于任何该空间中的点的坐标都可以映射到对应的三维空间中的点，也就是说可以定义一个函数，输入的是(u,v)(u,v)(u,v)，输出的是空间中的(x,y,z)(x,y,z)(x,y,z)，如果把所有的(u,v)(u,v)(u,v)都计算一遍，就可以找到对应空间中的表面，例如下图所示的马鞍面。因此显式表示方法，要么直接给出，要么通过参数映射的方法来定义表面。 举个例子，f(u,v)=((2+cosu)cosv,(2+cosu)sinv,sinu)f(u,v) = ((2+cosu)cosv, (2+cosu)sinv, sinu)f(u,v)=((2+cosu)cosv,(2+cosu)sinv,sinu)，（因为我们给出了(u,v)(u,v)(u,v)的对应(x,y,z)(x,y,z)(x,y,z)的具体表示方法，因此这是一种显式表示方法）我们想求出在表面上的点。求出后会发现这是一个与之前的案例同样的圆环。可见，隐式的表示方法并不容易想象出表面的实际样子，但是对于显式的表示来说很容易。 但是，显式表达中，检测点与表面的关系会相应的变难，例如f(u,v)=(cosusinv,sinusinv,cosv)f(u,v) = (cos u sin v, sin u sin v, cos v)f(u,v)=(cosusinv,sinusinv,cosv)，去判断点(34,12,14)(\\frac{3}{4},\\frac{1}{2},\\frac{1}{4})(43​,21​,41​)与表面的位置关系就会变得很难。 因此，有些问题适合隐式的表示方法，有些问题适合显式的表示方法，没有最好最完美的表示方法，我们要根据需要去选择。Best Representation Depends on the Task! &quot;I hate meshes. I cannot believe how hard this is. Geometry is hard.&quot; -- David Baraff (Senior Research Scientist, Pixar Animation Studios) Implicit Representations in Computer Graphics 隐式的表示方法还有很多，在这里再介绍几种。 Algebric Surfaces(Implicit) 最简单最直接的是Algebric Surfaces(Implicit)，即使用数学公式去表示表面。前面提到，数学公式表示曲面的严重问题就是不直观，圆环的表达就已经很不直观了，对于一个复杂形状，想要表达出来就极其困难。 Constructive Solid Geometry(Implicit) 另外一种表示方法是CSG (Constructive Solid Geometry, Implicit) 表示方法，它是通过基本几何的基本运算来定义新的几何，例如一个圆柱和球，做布尔运算，通过几何之间的求交集、叉积和并集就能得到新的较复杂的几何，如下图所示。这种操作得到了非常广泛的应用，在不同的建模软件中都支持这种方法，通过这种方法可以把简单的几何变成复杂的几何，仅通过几何之间的相互关系来得到，最后还可以写成表达式，因此它仍然是隐式的方法。 Distance Functions(Implicit) 还有一种表示方法叫**距离函数 (Distance Functions, Implicit) 表示方法。对于任何一个几何，我们不去描述它的表面，而是去描述空间中的点到这个表面的最近距离，先看一个例子，如下图所示，当两个球逐渐靠近时，两个球的形状发生变化，融合在了一起，最终变成一个球，在这个过程中，形状和拓扑结构都发生了变化，这就是通过对几何的距离函数做融合所形成的结果。 重申一下，距离函数是指，空间中的任何一个点到想要表达的表面的最近距离，这个距离可以是正的也可以是负的，即有向距离（Signed Distance）**例如在表面外为正，表面内为负，这样空间中任何一个点都可以定义出一个值，把这两个不同的物体的距离函数都算出来以后，就可以把两个距离函数做一个融合，再恢复出原来的物体，就可以做出融合的效果。 举个例子如上图所示，该例子就是对距离函数的应用，在图A和图B中，是两张不同的图，我们认为是表示某种几何的边界，假设一个物体，挡住了能看到的视口的13\\frac{1}{3}31​，另一个物体（假设是前一个物体经过移动后）挡住了视口的23\\frac{2}{3}32​，如果我们要求从视口A到视口B的中间状态，要想做简单的线性的融合，得到的图左边13\\frac{1}{3}31​为A，中间13\\frac{1}{3}31​为B，右边13\\frac{1}{3}31​为白色。这就是两张图做简单线性Blend的结果，它并不能表述运动信息，我们希望得到的是左边一般是黑的右边一半是白的，那么怎样才能得到正确的结果呢？我们要先对空间中的所有点求图A的有向距离，即图A的边界为0，求点到这个边界的最短距离，越靠近边就越接近0，然后再对图B做相同的计算，这两张图的结果会是渐变的图，最后我们将这两张图做一个Blend的操作，就能离可得到上图右下方的结果。如果我们把它恢复成原本对应的形状，Blend它们的SDF就等于是在Blend它们的边界。 通过距离函数，我们可以表达出一些复杂的、圆滑的几何，如下图所示。那么距离函数得到的函数，我们要如何把它恢复成表面呢？只需要把距离函数对应为0的位置全找出来即可。但是距离函数不太容易写成一种解析的形式，但是只要我们通过某种方法表示出来就可以了。 Level Set Methods(Implicit) **水平集方法(Level Set Methods, Implicit)**和距离函数几乎完全一样，仅仅是函数的表述是写在格子上的，只需要找到在中间找到值为0的点，就能把这个函数描述出来，当然也可以找到其他的曲线，例如通过f(x)=0.1f(x) = 0.1f(x)=0.1得到另外一条曲线。 这个概念其实在地理上早已得到广泛的应用，即等高线，它就是为了描述一个函数在不同的位置有相同的值，在这里对于这样一个例子来说很简单。当然水平集也可以定义在三维中的格子，而这就与我们前面说的纹理关联上了，如果有一个三维的纹理表述的是人体不同位置的密度，那么我们如何从这些三维信息中提取表面呢？我们可以让这个密度函数f(x)f(x)f(x)等于某个值，比如000，我们可以找出所有f(x)=0f(x)=0f(x)=0的点，就能表示出一个表面，这就是水平集在三维空间中的运用。 再比如水滴滴入水面造成水花的过程，我们该如何去描述它，这里也可以通过水平集的方法将水滴和水滴融合在一块，并提出融合之后的表面的样子。 Fractals(Implicit) 几何还有一种特殊的描述方法，称为分形（Fractals）。分形就是自相似的意思，即自己的一个部分和整体非常相似，在计算中和递归一个道理。例如雪花如果放大看会发现每一个六边形边都会有更小的六边形，即不断地自我重复所形成的形状。下图中中间的图是一种西兰花，仔细观察会发现它有很多凸起，如果放大去看会发现每一个凸起里又有很多更小的凸起，所以它是一个自带分形的植物，它是自然界中分形的例子。它在渲染中会引起强烈的走样，因为变化频率太高了，因此这样的几何对渲染来说是一个非常大的挑战。 Implicit Representations - Pros &amp; Cons 接着总结一下隐式表达的优劣。好处有：描述容易（比如用一个函数），有利存储；有利查询（判断位置关系）；利于做光线求交（当然对显式来说也并不难）；对于简单的物体可以严格地描述出来，没有采样误差；容易控制拓扑变化等。坏处就是难以描述复杂形状的几何。这也是为什么我们还需要显式的表达。 Explicit Representations in Computer Graphics 显式表达也有很多种方法，例如三角形表达、贝塞尔曲面、细分曲面、NURBS、点云等。 Point Cloud (Explicit) 最简单的显式几何表示方法是点云，点云的表示不考虑表面，全部都表示成点，只要点足够多，自然而然就看不到点与点之间缝隙，图像上就能看到一个表面。表示一个点很容易，用(x,y,z)(x,y,z)(x,y,z)就够了，点云就是一个点的列表，例如下图右侧，雕塑的上半部分点云的密度非常大，因此就能看到物体的表面，随着点云变得越来越稀疏，就看不到物体的表面。所以如果想用点云来表示一个非常复杂的模型，就需要特别密集的点。当然，理论上它可以表示任何类型的几何，只要点足够密即可。通常来说人们会做一些三维空间中的扫描，其输出就是点云，但这就会涉及到一个问题，给定足够多的点云，如何把它们变成三角形面，这里就有很多研究了。正常情况下，如果点云密度很低，自然而然就不太容易将其表达成表面，这也是为什么人们用点云去表示的情况不是特别多。 Polygon Mesh (Explicit) 在图形学中，得到最广泛应用的就是多边形面（通常是三角形或者四边形），它非常易于表示，任何形状都可以拆成很多很多小的三角形，如下图所示，类似胶囊的几何，两端三角形较密集，较规则，中间部分较稀疏，较细长。不过显然，使用三角形表达就自然涉及到一些连接关系，这就相比于点云造成了一些困难，但也有更多的研究。重申一下，多边形是图形学中最广泛运用的图形表示方法。 这里既然提到了三角形面，就顺便提一下我们平常如何表示三角形面形成物体的。如下图所示，这是一种特定的文件格式，这一种文件可以存储一个物体或一个场景，这种文件称为The Wavefront Object File (.obj)，注意这里的文件虽然后缀是.obj但是和编译时生成的.obj文件不是一回事。它其实是一个文本文件，这个文本文件里，只是把空间中的顶点坐标、法线、纹理坐标分开表示，然后再把它们组织起来。下图所示示例，这个文件描述了一个立方体，一个立方体总共有8个空间点，它们分别被用'v'表示的三个坐标来表达，也就是左图中的第3-10行。然后，一个立方体有6个面，所以它只有6种不同的法线，因此文件同样定义了6种不同的法线，为右图的第27-34行，这里有8行是因为在自动建模中有冗余的地方，比如29行和30行不考虑数值精度的时候其实是一回事，其实只定义了6个法线。再然后，它定义了24个纹理坐标（一个面有4个点），为左图第12-25行，当然这里也有冗余，其实不用定义这么多。最后，这个文件定义了它们之间的连接关系，也就是说哪三个点形成了一个三角形，用'f'(face)表示，每一个数值的含义是为 v / vt / vn的索引，比如第36行的含义是：使用第5个顶点、第1个顶点和第4个顶点构造一个三角形，并且这三个顶点分别用第1个、第2个和第3个纹理坐标，并且都使用第1个法线。这样一来我们就可以通过这种形式来定义一个网格了。 Curves 下面我们从曲线开始，来讲解显式表示的其他方法。我们从一个例子开始看，如图为一个动画，在动画中摄像机会在空间中沿着某一路径运动，并且会改变它的朝向，这些曲线我们可以定义好它。不光是相机路径，模型移动的路径也可以使用曲线来定义。 除此之外，使用曲线还可以定义一些字体，通过添加控制点的方法来定义曲线，这种曲线如果被无限放大，我们会看到任何地方都是光滑的，不会出现格子的情况，这就是我们接下来要说的贝塞尔曲线，一种显式的表示方法。 Bezier Curves 贝塞尔曲线的目的是用一系列控制点去定义某一条曲线，这些控制点会定义这条曲线所满足的一些性质，比如说从p0p_0p0​开始，并且沿着从p0p_0p0​到p1p_1p1​的方向为切线向前走，同样道理这个曲线会在p3p_3p3​处结束，并且结束时，其切线一定是沿着p2p_2p2​到p3p_3p3​的方向向外的。（图中会发现切线的长度也被定义了，即系数333，这里后面会做解释）总之，通过这四个点，我们可以定义曲线的起始点和终点一定为p0p_0p0​和p3p_3p3​，并且起始方向和结束方向为p0p1p_0p_1p0​p1​和p2p3p_2p_3p2​p3​，这样就会得到一条唯一的曲线。当然，曲线并不一定经过中间的控制点，这取决于我们如何定义它，只定义曲线一定经过控制点集中的起、止点。 de Casteljau Algorithm 那么我们怎样才能画出一条贝塞尔曲线呢？我们可以用任意多个点（当然数量大于等于2）去画出一条贝塞尔曲线，这里用到的算法就是de Casteljau Algorithm。如下图所示为一条由三个点形成的贝塞尔曲线，称为二次贝塞尔曲线（quadratic Bezier）。 画出这条曲线前，我们知道b0b_0b0​决定了这条曲线从哪开始，b1b_1b1​决定了它往哪个方向弯曲，b2b_2b2​决定了曲线的终点。 我们假设这条曲线的起点为时刻t=0t=0t=0，它的中点为t=1t=1t=1，那么如果我们想画出这条曲线，实际上就是求出这条曲线上的点在[0,1][0,1][0,1]中任意一个时刻ttt所处的位置。de Casteljau Algorithm就是告诉我们该如何找出这个点，它将画出整个曲线的方法转化成了找一个点的问题。 如上图所示，三个点形成了两条线段b0b1b_0b_1b0​b1​和b1b2b_1b_2b1​b2​，假设方向也是输入顺序。假设给定了时间ttt，ttt在b0b1b_0b_1b0​b1​上，那我们认为b0b_0b0​是000，b1b_1b1​是111，ttt假设约等于13\\frac{1}{3}31​，那么我们就找出线段b0b1b_0b_1b0​b1​上13\\frac{1}{3}31​的点b01b_0^1b01​。同样道理，我们在b1b2b_1b_2b1​b2​上也能找出位于13\\frac{1}{3}31​处的点，记为b11b_1^1b11​，这样一来我们就找到了三个点所形成的两条线段上的两个点。 那么如果我们把新得到的两个点连起来，再去找这条线段b01b11b_0^1b_1^1b01​b11​上位于13\\frac{1}{3}31​处的点，这时发现找到这里就结束了，因为无法再找出更多的线段了，那么这一个点就是贝塞尔曲线在时间ttt所在的位置。最后我们需要枚举所有可能的时间ttt，即可画出曲线。 显式表示要么是通过直接定义，要么通过参数来表示，这里的ttt就是一个参数，因此贝塞尔曲线是一种显式表示方法。可见de Casteljau Algorithm是一个很简单的递归算法，我们可以一直找，直到找到最后一个点。 同样地，如果给定了4个点，用这4个点来定义一条贝塞尔曲线，这条曲线必经过b0b_0b0​和b3b_3b3​，那么我们该如何画出它呢？假设找一个时间t=0.5t=0.5t=0.5，得到了b01b_0^1b01​，对于每条线段都能找到点，然后再连起来，原本四个点三条线段，连起来后变成了三个点两条线段，同样地，再对这两个点取t=0.5t=0.5t=0.5的位置，即可得到b02b_0^2b02​和b12b_1^2b12​，连起来后变成了两个点一条线段，最后再取t=0.5t=0.5t=0.5的位置，得到b03b_0^3b03​，该点就是贝塞尔曲线在t=0.5t=0.5t=0.5时的位置。可见，算法每次递归，线段的数量和点的数量会减一，不断递归下去，直到最后剩下一个点。 Algebraic Formula 算法已经解释清楚，但只是一个直观形式的解释，下面尝试能否从直观的解释推出代数的形式。贝塞尔曲线是由控制点和时间ttt来决定的，因此它们之间一定有一种代数表示的方法。如下图所示，我们在每两个点之间寻找ttt的位置，就相当于在这两个点之间做了线性插值，所以整个过程是不断地进行线性插值得到的点。 因此我们可以将其显式地写出来，例如二次贝塞尔曲线可以写成： b01(t)=(1−t)b0+tb1b11(t)=(1−t)b1+tb2b02(t)=(1−t)b01+tb11b_0^1(t) = (1-t)b_0+tb_1 \\\\ b_1^1(t) = (1-t)b_1+tb_2 \\\\ b_0^2(t) = (1-t)b_0^1+tb_1^1b01​(t)=(1−t)b0​+tb1​b11​(t)=(1−t)b1​+tb2​b02​(t)=(1−t)b01​+tb11​ 展开后可以得到： b02(t)=(1−t)2b0+2t(1−t)b1+t2b2b_0^2(t) = (1-t)^2b_0+2t(1-t)b_1+t^2b_2b02​(t)=(1−t)2b0​+2t(1−t)b1​+t2b2​ 我们发现，给定时间ttt，f(t)f(t)f(t)其实是b0b_0b0​、b1b_1b1​和b2b_2b2​的组合，因此任意一个点必须要由控制点的坐标来决定，并且与ttt有关。我们令(1−t)=s(1-t)=s(1−t)=s，则公式变成： b02(t)=s2b0+2tsb1+t2b2b_0^2(t) = s^2b_0+2tsb_1+t^2b_2b02​(t)=s2b0​+2tsb1​+t2b2​ 这就发现了贝塞尔曲线各项前面的系数其实就是(s+t)n(s+t)^n(s+t)n的展开式。例如三点二阶的贝塞尔曲线，各控制点的系数就是(s+t)2(s+t)^2(s+t)2的展开式。 总结归纳，给定n+1n+1n+1个控制点，我们可以得到一个nnn阶的贝塞尔曲线，它在任意时间ttt都是给定控制点的线性组合，它组合的系数是一个跟时间有关的多项式，这个多项式叫做Bernstein多项式（其实就是一个描述二项分布的多项式）。 最后简化一下，任意阶数的贝塞尔曲线的控制点的系数是由Bernstein多项式给定的，然后贝塞尔曲线是这些控制点与对应控制点系数的加权。通过这样一个性质，我们完全可以不必限制控制点在平面内，在空间中仍然可以得到贝塞尔曲线，只需要把控制点输入成三维坐标，同样使用Bernstein多项式来插值即可。 Bernstein多项式其实是对111自己的多项式展开，这也是为什么如果我们把多项式中的系数加起来最后都等于111。 Properties of Bezier Curves 贝塞尔曲线有很多不错的性质： 规定了起点和终点，例如b(0)=b0,b(1)=b3b(0)=b_0, b(1) = b_3b(0)=b0​,b(1)=b3​ 对于三次贝塞尔曲线（Cubic Bezier），它的起始位置的切线一定是b′(0)=3(b1−b0)b&#x27;(0) = 3(b_1-b_0)b′(0)=3(b1​−b0​)，终点位置的切线为b′(1)=3(b3−b2)b&#x27;(1) = 3(b_3-b_2)b′(1)=3(b3​−b2​)，如果控制点数不是444，那么系数就不是333了。 仿射变换下有一个好性质，如果要对一条贝塞尔曲线做仿射变换，只需要对它的控制点做仿射变换，再重新绘制出来即可。因此不需要把曲线上每个点的位置都记录下来。但是对于投影变换就不行了。 凸包性质，凸包是计算几何的中的概念，该性质是说，贝塞尔曲线上的任何一个点一定在几个控制点形成的凸包内，例如四个点形成了一个四边形，那么画出来的曲线一定在这个四边形内。 如下图所示，凸包的概念为能够包围一系列给定顶点的最小的凸多边形称为凸包（Convex Hull）。一个直观的比喻是，假设有一块板，下图中的黑点代表钉子，我们可以用橡皮筋拉开把这些钉子全部包住，然后松手，最后橡皮筋会收缩在这些物体形成的外框，这个框就是凸包。 Piecewise Bezier Curves 我们可以使用贝塞尔曲线，但是它也有一定的问题，这也是为什么我们要引入逐段的贝塞尔曲线（Piecewise Bezier Curves）。 我们来看一个例子，当n=10n=10n=10时，也就是给定了111111个点，我们就可以画出一条贝塞尔曲线出来，如下图所示，这条曲线能画出来但是并不直观，它变成了一条很平滑的曲线，没有控制点输入时那么剧烈的变化。也就是说，当控制点多的时候，这个曲线不容易控制，就得不到想要的形状。 因此人们就想到，我们可以不用这么多控制点来定义一条贝塞尔曲线，可以使用多段贝塞尔曲线来定义，这样每次只用很少的控制点，最后再连起来，这样就解决了问题。人们更倾向于每444个控制点来定义一条贝塞尔曲线，也即是用Pievewise cubic Bezier来定义曲线。如图所示，它是每444个控制点定义的贝塞尔曲线拼接而成的，在发生剧烈变化的地方就是多段贝塞尔曲线的交点，因为贝塞尔曲线一定经过起点和终点。 图中黑色点就是多段贝塞尔曲线的交点（起点和终点，必须经过的点），蓝色点是控制点，本来应该是所有控制点都连在一起的，软件中给隐藏掉了中间两点之间的连线，这样对于Pievewise cubic Bezier来说，一条贝塞尔曲线内的控制点就可以当成一个控制手柄，这正是Photoshop、Illustrator等软件的钢笔工具带来的画曲线的能力。那么怎么保证连起来的曲线是光滑的呢？在物理上，曲线光滑不光滑是看切线方向是否光滑，即导数要连续，不只是方向还有大小，那么对于第一段曲线来说，终点的方向是由最后两个点来定义的，对于第二段曲线来说，起点处的方向是由前两个点来定义的，因此只要第一条曲线的倒数第二个控制点和第二条曲线的第二个控制点共线并且到终点的距离相等，曲线就能光顺地过渡。 Continuity 如果给定两段贝塞尔曲线，都是由444个控制点构成，那么在几何上两个曲线都通过中间的黑点，图中展示的是切线上的连续，另外只要两条曲线的终点和起点接触的连续，就是几何上的连续。 用数学来表示： 第一段的终点与第二段的起点相等，称为C0C^0C0连续，即an=b0a_n=b_0an​=b0​。即几何上，只要两曲线接上了，就达到了C0C^0C0连续，即几何连续。 交点左右的两个控制点与交点共线并且到交点的距离相等时，称为C1C^1C1连续，即an=b0=12(an−1+b1)a_n=b_0 = \\frac{1}{2}(a_{n-1}+b_1)an​=b0​=21​(an−1​+b1​)，即切线连续。 再高阶的导数，例如C2C^2C2连续称为曲率连续，高阶的导数就对控制点有着更高的要求。注意，切线连续看上去似乎已经很好了，但是在制造业上来说，往往有更高的要求，要保证C2C^2C2连续甚至C3C^3C3连续。 Other types of splines 简单再介绍一下，一个概念叫做样条曲线（splines），一条连续的曲线是由一系列控制点控制的，它能够满足一定的连续性，与阶数无关。下图为早期人们画曲线时，会使用一根树枝然后用一些工具将其固定住。简单来说，一个可控的曲线就称为样条曲线。 B-splines 样条曲线中，一种被广泛应用的曲线称为B样条曲线（B-splines, basis splines），即基函数样条，它是对贝塞尔曲线的一种扩展，它比贝塞尔曲线的能力更强。比如前面当n=10n=10n=10时的贝塞尔曲线，改变一个点时，整个曲线都会发生变化，而在设计上这样的性质往往不能被接受，比如在曲线中绝大部分点都调整到了一个精确的位置，只有一处需要做更改，那么如果动了这一个点，整个曲线都要改，设计上就很难操作了，也就是说，设计师需要有一种性质叫做局部性，即改变一个点时，这个点所影响的其他点的范围。分段贝塞尔曲线就具有局部性，B-splines就是一种不需要分段的可以控制局部点的样条曲线。关于B样条曲线和NURBS（非均匀有理B样条）的知识可能是图形学中最复杂的部分，如果有兴趣深入学习的话可以访问Prof.Shi-Min Hu的课程 https://www.bilibili.com/video/av66548502?from=search&amp;seid=65256805876131485 Surface 曲面会比曲线稍微复杂一些，但是相对更好理解。首先，我们把曲线的概念延伸到一个平面上，比如下图中的贝塞尔曲面，它是由若干个块拼起来的，怎样在拼起来的同时保证连续性是几何上比较复杂的问题。 那么我们如何从贝塞尔曲线得到贝塞尔曲面呢？对于下图所示的曲面，可以理解成由一个平面，施加一个向上的力，拖拽上去得到的结果，它是由4×44×44×4个控制点得到的，图中的黑点可以理解为拖拽时施加力的作用点。 具体的实现方法就是在两个方向上分别运用贝塞尔曲线，首先在平面上定义4×44×44×4个控制点，它有444行444列，先从行来看，这四根曲线上的点在不同的时间ttt有不同的位置，如果我们把这四个曲线上的控制点，认为是另外一个方向的贝塞尔曲线的控制点，我们就可以画出这条贝塞尔曲线，在这根线不断地扫掠地过程中就可以得到一个曲面。通过这种方式我们可以定义非常复杂的曲面。 在一维对曲线的控制时，我们使用的是时间ttt，在曲面中，有两个方向的曲线，所以我们使用(u,v)(u,v)(u,v)。 因此，我们也可以通过(u,v)(u,v)(u,v)来找到曲面上的点。贝塞尔曲面是显式表示就是因为它是通过参数映射来实现的。 如果需要更多了解Surface，可以访问Prof.Shi-Min Hu的课程 https://www.bilibili.com/video/av66548502?from=search&amp;seid=65256805876131485 Mesh 一般来说，描述面最普遍的方法还是采用网格。既然我们用三角形来描述模型，那我们就需要了解一些关于网格的操作，例如 Subdivision （用更多的三角形来得到更光滑的模型），Simplification（减少网格面，以节省存储量），Regularization（使三角形都接近于正三角形）。 首先我们从最基本的操作Subdivision（细分）开始，即如何增加三角形的数量，把用三角形表示的曲面更加光滑。如上图所示，三角形数量很少的时候看上去就会棱角分明，我们希望能够变得更加光滑，对于现在的显卡来说，三角形的数量已经不是很大的问题了，这样一来，如果有一个模型，我们希望它的细节能够更丰富，我们就可以引入更多的三角形，就好像将一个图形提高它的分辨率以丰富它的细节。 第二个操作就是Simplification（简化），如上图所示，如果有一个网格过于复杂了，它在渲染中位于远处不需要这么多网格，我们就可以用更少的网格来表示。问题在于我们删除部分网格面后如何保持一些基本的连接关系，例如牛角的面减少后并不会使这个牛角出现破损或者断掉，因此需要有一系列方法来指导这一算法。 第三个操作就是Regularization（正则化），三角形有大有小，形状不一，在渲染的时候会造成各种各样的不便，通常应对这种情况会对模型进行这一操作，使三角形更接近于正三角形。 Subdivision 我们从细分开始说，之前在说位移贴图的时候就提到过细分，我们可以在物体表面应用贴图，这个贴图表示了顶点的位置移动量，即通过贴图定义了一个高度场，我们可以将不同的高度应用到不同的顶点上以实现顶点被移动过的模型，但是这要求网格的面数非常多才能赶上纹理本身要求的频率。我们当时提到了需要做细分，甚至动态的细分，那么细分怎么实现呢？首先我们可以看到，细分不止是引入了更多的三角形，它还让三角形的位置发生了变化使模型变得更加光滑，因此我们说的细分其实是两步操作：增加三角形，变光滑。如下图所示 Loop Subdivision 我们以Loop Subdivision算法为例，这个细分分为两步操作，第一增多三角形数量，给定一个三角形，连接三条边的中点，即可得到中间一个三角形，而该三角形又把原本的三角形分成了三个部分，这样一步算法就把原本的一个三角形分成了四个三角形。第二步，我们需要调整三角形的位置，其实是调整顶点的位置，特别对于Loop细分，我们会把三角形的顶点区分开，分为新顶点和老顶点，新的顶点就是我们在边上取的中点，老的顶点就是原本三角形的顶点，Loop细分就是对两种顶点分别采用不同的规则来改变它们的位置。 BTW: 图形学中有很多命名的方法，这里的Loop不是指“循环”，而是这个算法的创始人的family name。 增加三角形的过程很简单，那么下面就是如何把老的顶点和新的顶点分别移动来使模型变得更加光滑。 我们先看怎么更新新的顶点的位置，即边的中点。如下图所示，我们来看这个白色顶点，首先它肯定在一条边上，然后只要这条边表示的不是模型的边界那么它一定会被两个三角形共享，我们将两个三角形共享的边上的顶点分别称为A,BA,BA,B，把不共享的两个顶点称为C,DC,DC,D，那么Loop细分定义的规则中，这个白点的位置要变为Vnew=38(A+B)+18(C+D)V_{new} = \\frac{3}{8}(A+B)+\\frac{1}{8}(C+D)Vnew​=83​(A+B)+81​(C+D)，这其实就是一种简单的加权平均，即离点近一些的A,BA,BA,B贡献更大一些，而C,DC,DC,D贡献相对较小，这样一个新的顶点的位置是周围几个点的位置的平均，这就起到了一个平滑的作用。 对于老的顶点，如下图所示，对于白色顶点有6个三角形拼在一起，为了更新老的顶点的位置，我们需要将它与它相邻的老顶点关联起来。Loop定义了一个规则是它采取一部分周围顶点的位置，接着它还会部分地保留自己的位置。我们定义一下顶点的度nnn，在图论中，顶点所连接的边的数量就称为顶点的度，也即是说这里白色顶点连接了6条边，即n=6n=6n=6。此外我们再定义一个权重数uuu，因此这个白点的位置应该更新至vnew=(1−n∗u)∗vori+u∗∑nvneighborv_{new} = (1-n*u) * v_{ori} + u* \\sum _n{v_{neighbor}}vnew​=(1−n∗u)∗vori​+u∗∑n​vneighbor​。如果一个顶点连接了很多三角形，那它的会更受它的相邻顶点的影响，如果它只连接了两个三角形，那么意味着这个顶点非常重要，自身的权重会更高。 通过这样的方法，我们就能得到如下图所示结果，每一个三角形被拆成了四个小三角形，并且顶点的位置都经过了细微调整，使模型变得光滑。因此Loop细分做了两个工作：先细分，再光滑。 Catmull-Clark Subdivision (General Mesh) Catmull-Clark Subdivision是Catmull（2020图灵奖得主）与Clark共同发明的算法。我们之所以提及这个算法是因为Loop细分只能对三角形网格进行操作，下图网格中既有三角形又有四边形的一般网格情况，就需要使用Catmull-Clark细分。我们先来定义一些概念，我们称Quad face为四边形面，而非四边形面当然就是Non-quad face，例如图中的两个三角形面。接着我们定义，顶点的度不为444的为奇异点（Extraodinary vertex）。 Catmull-Clark Subdivision是既在每一条边取中点，也在每一张面上取中点，并且把边上的中点和面上的中点连起来，这样左上角的一个四变形变成了四个四边形，三角形就细分成了三个四边形。分析一下会发现，经过了一次细分后，非奇异点的顶点仍然是非奇异点，原本的两个度为555的奇异点仍然是奇异点，并且在三角形面上新增的点都变成了新的奇异点，因此细分一个三角形会产生一个度为333的新的奇异点，这样就有了四个奇异点。新的奇异点产生是因为三角形细分出来要和原本的三条边相连，推广一下也就是说，只要在非四边形面内新增点做细分，由于该点要和每条边相连，因此必定产生奇异点，和几条边相连，其度就为几。此外，我们还发现，经过这样一次细分之后，所有非四边形面全部都消失了，因为三角形被分成了三个四边形。因此，这样的细分会在每一个非四边形面上引入一个奇异点，并且会将这些非四边形面转换为四边形面。那么如果我们再做细分操作，所有的面都已经是四边形面了，也就是说奇异点数不会再增加了，这也告诉了我们，Catmull-Clark Subdivision会在第一次细分后，增加了非四边形面的数量个奇异点，之后不再增加。 例如我们再做一次细分，由于所有面都已经是四边形了，因此奇异点数量不增加。大家会看到在细分的过程中，点会发生位置变化，并且变得越来越光滑。 关于顶点更新规则，它对于面的中心点、边的中心点及原始顶点会有不同的更新规则，其具体规则如图所示，定义看上去很复杂，实际仍然是图像的模糊操作没有什么特别大的区别，简单来说就是一个加权平均的规则。 通过Catmull-Clark Subdivision可以产生各种各样细分的结果。Loop细分只能用于三角形面，Catmull-Clark可以用于各种不同的面。注意下图中四边形细分结果不对称是因为定义了折痕（Crease）。 Simplification 下面开始讲解如何进行简化操作。这一算法的目标是在保持整体形态不发生剧烈变化的前提下减少三角形的数量，以方便其他算法提升性能。例如下图所示，如果这个骷髅模型距离摄像机镜头非常远，我们看不到很多细节，就没有必要使用30,000个三角形去表示它，使用3,000个甚至300个就足以，这可以大幅度减少计算量，因此在不同的情况下会我们要选择不同复杂程度的几何模型，以提高程序的效率。这个算法和我们之前提到过的Mipmap有关，层次结构的几何和层次结构的图像是相似的，只不过几何更难去实现，它要求更多的平滑过渡，避免突变。 这里提供某一种方法，叫做边坍缩（Edge collaping），即将边变成一个点。 Simplification问题的关键在于“要坍缩哪些边”？下图中左侧网格，我们把中间三个顶点都替换成一个顶点，但是我们应该把这个顶点放在什么位置才能保证蓝色网格和灰色网格基本保证轮廓形状一致呢？左图中体现出做放在平均位置的结果，显然结果是不对的，结果会过于扁平圆滑。接着人们引入了一种误差的度量，称为二次误差度量（Quadric Error Metrics），即将这个点放在某一位置时最小化二次误差。二次误差在机器学习中和L2距离非常相似，新的顶点与和它相关联的面的各顶点的距离的平方和，我们要让这个值达到最小，因此这就是一个非常清晰的优化过程。 有了这样的二次度量，我们就可以在坍缩任意一条边之后，都把坍缩后的顶点移动至一个二次误差度量最小的位置。这样，可以假设每一条边都计算出坍缩后对应的误差，然后将这些误差最小的边开始进行坍缩。但是要注意，如果我们坍缩了一条边为顶点，那么这条边所连接的其他边会相应地变长，如上图所示，因此坍缩后，坍缩边周围的边会受到影响，其二次度量误差也会发生变化。因此，在坍缩完最小的二次度量误差后，要对其影响到的边做更新，因此需要某一种数据结构，能够保证取到最小值后可以动态地以最小的代价来更新受影响的元素，这个数据结构就叫做优先队列，或者叫堆。 简单而言，具体步骤如下： 对于每一条边，打上一个分数，这个分数就是它坍缩后的二次度量误差 取最小误差的边，做坍缩 更新受影响的边的二次度量误差 再来反思这一方法，我们其实是在不断地通过找局部最优解的方法来找全局最优解，这种方法其实是一个典型的贪心算法。 ","link":"https://albertlidesign.github.io/post/games101_5_geometry/"},{"title":"Mesh is Art (2): Triangles","content":" #​前言 前文我们大概讲述了网格的定义、在不同领域的应用、基本元素以及它的数据结构。其中数据结构那一小节里欲言又止，是因为想详细地弄清楚一种数据结构的特性就必须有大量的实际操作经验，本文就来通过一个实际工程案例来讲讲网格的一些基本概念以及它们在实际工程中是如何体现的。 上一篇我说过，网格是一种思维，为了更好地佐证这一点这次我就拿个实际案例来使大家感受这句话。据说，最高效的学习方法是按照“提出问题→作出假设→反复尝试→得出结论”这样的过程来执行的，我也看多过很多文章写作方式是在模拟这套模式，这次我也来试着写一下，希望能为设计师和工程师们带来启发。 #从富勒球结构说起 我想一提到这个人所有人第一反应就是他的轻质圆形穹顶结构（具体点是一个三角网架搭建起来的球形穹顶结构）。我不是结构师，在这里我不过多探讨这个结构是否合理或者怎么施工做到的，也没有查阅过这个结构的相关资料，今天我们只从网格的角度上来讨论这个结构的形。 我当时看到这个结构，脑子里就产生了小问题，问题一，这个球体上的三角形是全等三角形吗？问题二，这个球体上的三角形是均匀的吗？“均匀”这个词比较模糊，换言之，三角形都有顶点对吧，每个顶点可能连接着不同数量的三角形，在这里我们把“均匀”描述成，每个顶点连接的三角形的数量是相等的吗？如果不相等，那怎样的分布看起来会比较好？ 这两个问题的意义很是大的：如果这个球面上的三角形全等，意味着它们的边长都相等，意味着在施工上可以批量加工等长的杆件；如果三角形都是均匀的，意味着施工上的节点链接三角单元的数量是固定的，那么可能只需要使用固定数量且角度可变的连接节点即可。无论怎么说，这两个问题关系着大量人工作业的成本。讲真，这两个问题要是深究起来，没有很好的离散几何底子是不太容易算出来这两个问题的答案的。不过好在计算机发达的今天，我们可以借助三维建模软件去感性地思考这些问题，当然，如第一篇前言所说，我无法精准地证明这件事，但是对于设计师而言，我想有一个近似确定的概念就够了（严格追究下去不是我们的工作）。不过不管怎么说，我们得先有一套描述问题的方法，先跟大家来“约定俗成”一些概念，以确保后面说我的想法时能更加方便理解。 #相关网格概念 首先，前面照片也看到了，这个球壳并不是真的球，其实准确的说是一个球体被一个平面切割后保留的部分。那么它与平面切割的交界处就产生了边界，那么我们称这个边界上的点（可以认为是工程上的节点，网格上叫顶点）为边界顶点，在边界上的（工程上的）杆件称为边界边，其他就是内部顶点和内部边，这点概念还是很好理解的，下面是相对严格一些的定义。 在一个网格中，如果的一条边只属于一个面，称这条边为边界边(boundary edge)；如果一个顶点属于边界边则称此顶点为边界顶点(或边界点，boundary vertex)；至少包含一个边界顶点的面称为边界面(boundary face)。 非边界的边、顶点和面分别称为内部边(internal edge)、内部顶点(internal vertex)和内部面(internal face)。网格中具有公共边的两个顶点互为邻接顶点。 上文中我们经常提到“某个顶点所连接三角面的个数”，这段话非常长而且非常绕口，我们直接称其为网格顶点的价(Valence)。这个概念非常重要，它经常作为判定网格质量的依据之一。更严格的定义是：顶点的价(Valence)是指与该顶点通过公共边相连的顶点个数。如前文所说，在幕墙领域，价数统一，意味着施工上的节点链接三角单元的数量是固定的，那么可能只需要使用固定数量且角度可变的连接节点即可（这里再重复一遍）。 通过图4我们会发现，一个网格顶点的价变化多端，但是凭借我们强大的识别能力一眼就能看出，这张图中，价为6的顶点是“主流”，价为5和7的顶点是“非主流”。为了区分这些顶点，我们把“主流”顶点称为正则顶点，“非主流”顶点称为奇异顶点。这在多通四边管网格的交错点会特别明显地体现出来，如图5所示。 ​人是很聪明的动物，儿时玩过“大家来找茬”游戏，这时候很能派上用场，我想再复杂的网格，你也一眼就能识别出哪些顶点是奇异顶点，所以我想我也不用给出严格定义了，一眼就能看出来，试试图6。 当然网格还有其他重要的概念，比如非流行、法向等等，我们放到后面再将，现在应该回到我们的富勒球结构问题了。 #问题的本质 数学老师从小教导我们，研究一个问题，必须要研究到这个问题的最本质。通过上文我们了解了网格这么多的概念，不难想到，这个问题的最核心问题是奇异点的数量和分布。我们先抛个问题，即一个球体的三角剖分可能没有奇异点吗（把一个几何体划分成若干三角形组合的图形叫做三角剖分）？答案当然是否定的，至少我没见过。对于我们这些没有代数拓扑和微分几何知识的人来说，让我们证明这个问题是不可能的。但是我们可以想像啊！ 大家可以跟着我来推理，Ummmm，**网格没有奇异点→网格都是均匀的三角形→均匀三角形的话，那么应该是近似正三角形→近似的正三角形每个角都是近似60度的→正三角形要想密铺的话一定是由六个正三角形拼起来→六个三角形拼起来就是个正六边形。**OK，到这就关键了，我们可以把问题转化为，**能否只用六边形去密铺一个球体！**这显然是不可能的啊，要不然足球怎么不是全六边形的？ 上面的推理太快，再慢点说，看到图4时很多人发现了，顶点价为6的三角形是占主导的，这是巧合吗？当然不是，我们先假设一堆三角形的顶点交于一点，要想保证每个三角形三边的长度都比较均匀、近似相等，那这三个三角形都应该是趋近于正三角形的，又因为相交三角形汇于一点实现了密铺，则它们的角度之和是360度，平摊下来，每个角度都是趋近于60度的，那么它们的顶点价数就是趋近于6，尽管我没法从数学上证明这一点，但就是这个样子。所以，一般来说，控制过网格边长所产生的三角网格的正则顶点价数就是6（除非是仅由奇异顶点构成的图形，比如二十面体）。 既然只用六边形拼不出一个球，那怎样才能拼出一个球形呢？建筑学老师教导过我们，要学会利用前人的成果。数学家已经替我们证明了，12个五边面和20个六边面构成的足球啊，既均匀又合理（具体的证明去谷歌吧）。但是足球这个形状还不是最本质的，我们还可以继续剖析，因为五边形和六边形都不是最最基本的几何图形。刚才也提到了，六边形可以看作是六个三角形拼出来的图形，这里面的顶点都是正则的，所以奇异点不可能在这，那么奇异点只可能在五边形里了，OK，那我们顺次连接五边形的中点看看能得到什么（图8）。 我们发现，得到了一个**每个顶点价数都为5的正20面体（每个面都是三角面，图9），并且这个20面体的所有面都是全等的，也就是说边长是等长的！**没错，这就是最本质的图形，这个图形上的每个顶点都是奇异点！也就是最均匀的球拓扑的奇异点分布。 现在我们在每个大的三角面里面都填上小三角形（Loop细分算法），并将它们用拉普拉斯圆滑算法处理一下，如下面的动图所示（这两个算法不用较真，后面的文章会说）。因为小三角形都是在大三角形面上填上去的，所以无论细分多少次，奇异点的数量都不会变，每次细分都会继承原始网格的奇异点。 细分后的网格，看上去已经和富勒的球结构非常像了，我们这里再做一次对比看看，没错就是这个样子的。他是通过裁切细分圆滑后的二十面体网格得到的形式。 ​ 现在回答文章开头的问题，关于长度，由于细分和圆滑算法的不同，杆件长度会有细分改变，但是工程上完全可以根据需要将细分改变的误差区间加以控制。但是至少可以确定的是，全等三角形拼出的20面体，细分不会改变这20面体的基本形态，所以我们依然能够在细分后的球壳上找到之前大三角形的痕迹，每个大三角形依然保持着全等，也就是说，会有20组全等的杆件。因此，只需要将一个三角形里面的杆件和节点复制即可，也能使杆件的长度标准化。关于节点，除了奇异点处的节点为五通节点，其余所有节点一定是六通节点！ 图13是1928根杆件的长度分布表，然后将杆件的长度分布进行着色呈现出如图所示的结果，最短的杆件和最长的杆件误差在百分之17.7，这个数虽然很大，但是杆件分布范围非常规律，在施工上完全可以按照图中的颜色进行分组加工。 所以最终的结论是，杆件长度分布非常规律，施工上可以采取分组加工的方式进行标准化生产，节点上除了奇异点处的节点是五通节点，其他节点均为固定的六通节点。 相信通过这个项目大家理解了网格的这些概念，尤其是奇异点的重要性。当然这还只是开始，还有更多的骚操作和特性值得去探索。很多建筑问题的本质都是几何问题，而网格则是将工程问题转化成几何问题的纽带。关于网格更多的内容还请继续关注：AlbertLiDesign ​ ","link":"https://albertlidesign.github.io/post/meshisart2/"},{"title":"GAMES101(4): Texture Mapping","content":"课程链接：GAMES101-现代计算机图形学入门-闫令琪 课程讲师：闫令琪 本系列笔记为本人根据学习该门课程的笔记，仅分享出来供大家交流，希望大家多多支持GAMES相关讲座及课程，如涉及侵权请联系我删除：albertlidesign@gmail.com 如下图，我们可以看到两个台灯在照亮一个地板和一个球，我们是可以得到光的强度，但是比如球上面，自身有不同的颜色，尽管上面所有的点共用的是同一个着色模型，但是不同位置的漫反射系数发生了改变。对于地板来说也是如此，它有自己的漫反射系数，这个系数反映了木质的质感。因此我们希望在模型的不同位置定义不同的属性，这就要引入纹理映射的最基本的思路，它的根本作用是定义一个点的属性。 那么怎么定义任何一个点它的基本属性呢？我们需要理解我们要定义在物体表面上，那么我们应该怎么样去理解物体表面呢？首先，任何一个三维物体的表面其实都是二维的。例如下图，地球仪可以被展开成世界地图，这也就是说三维物体的表面其实是二维的，多个物体也可以被展开成多个平面，因此通过这种方式，我们可以和一张图来做一个一一对应关系。因此所谓纹理，其实就是一张图，这张图我们可以任意地裁切，用其中一部分，也可以拉伸、压缩等操作，最后把它蒙在三维物体的表面，这个过程就叫做纹理映射（Texture Mapping）。 来看一个具体的例子，左上角是一个渲染结果，即Blinn-Phong得到的结果，我们想得到有质感的模型该怎样做呢？根据刚才的思路，要把一张图贴在模型上，我们自然要知道怎么贴。三维空间中最基本的东西是三角形，那么三角形在物体上应该如何映射到纹理上呢？也就是物体上的某一个三角形在纹理上对应的位置在哪？对于任何一个三角形上的顶点都能找到它在纹理上的点。 对于任何一个模型，我们要能够将它展开成一个平面，并且希望其产生的三角形尽可能地少扭曲，这是一个很重大的研究方向叫做参数化（Parameterization），是几何上非常厉害的研究。我们这里不管怎么把三角形映射到纹理上这件事，就假设我们已经知道如何把三角形贴在纹理上，并且知道三角形上的顶点在纹理上的坐标。既然提到了纹理上的坐标，那我们就该在纹理上定义一个坐标系，这个坐标系通常会使用(u,v)(u,v)(u,v)来表示纹理上任何一个点。例如下图所示 一般来说，都认为纹理的范围是在[0,1][0,1][0,1]中，这可以方便处理，不管分辨率、长宽比是多少。 纹理可以应用在各种各样的物体表面，如果我们把纹理的坐标显示出来，就会看到下图结果 纹理映射就像贴瓷砖一样不断地重复纹理，最后的渲染结果就是下图 因此纹理可以重复多次，但是纹理在重复的过程中会产生缝隙，就容易被人发展破绽。但是上图这个例子中，它使用的纹理设计的好，使得这些纹理复制和重复的时候可以做到无缝衔接，这种纹理的设计需要各种各样的算法，其中一个叫Wang Tiled。 下一个问题就是，我们已经知道了三角形三个顶点对应的纹理坐标，那么我们如何知道三角形内部的点对应的纹理坐标的uv呢？这里就又涉及到了插值问题。 Barycentric coordinates 让我们来看看如何在三角形内部进行插值，为了实现这一方法，我们引入了一个叫做**重心坐标（Barycentric coordinates）**的概念。 为什么我们要在三角形内部做插值？ （1）首先是因为我们都是对三角形顶点进行操作的，我们希望在三角形內部可以平滑地过渡。也就是说，当顶点处被赋予一个值时，三角形内部地任何一个人也能得到一个过渡的值，这样这个值就能从一个顶点过渡到另外一个顶点。 （2）插值的内容可以有很多，比如贴图坐标，颜色，法向量等。比如在纹理映射中，我们可以把三角形顶点映射到贴图上对应的(u,v)(u,v)(u,v)，那么三角形内部的点对应的uvuvuv坐标就可以通过插值来计算得到。基本上可以说，插值可以对三角形的任意属性进行插值。 怎么做插值？ 使用重心坐标。首先，重心坐标是定义在一个三角形上的，即给定一个三角形，可以得到一套重心坐标。重心坐标是说，在给定三角形的平面内的任何一点都可以表示成三角形A,B,CA,B,CA,B,C三个点的线性组合，如下图所示，其中α,β,γ\\alpha , \\beta , \\gammaα,β,γ合在一起构成一个坐标用来表示三角形内的点(x,y)(x,y)(x,y)。也就是说，为了描述一个点的位置，我们不需要构建直角坐标系，给定任意三个点，只要有一个点在这三个点所在的平面上，我们就可以得到用这三个点的线性组合来表示出该点。 观察公式α+β+γ=1\\alpha + \\beta + \\gamma = 1α+β+γ=1，也就是说实际上我们只需要知道其中两个，就可以求出第三个。除此之外，还需注意，这个点如果在三角形内，那么要求α,β,γ\\alpha, \\beta, \\gammaα,β,γ必须都是非负的，换句话说如果满足三个值都是非负的且它们的和等于1，那么这个点一定在三角形内。 如下图示例，问AAA点自己的重心坐标是什么？我们可以从定义可知，令α=1,β=0,γ=0\\alpha = 1, \\beta = 0, \\gamma = 0α=1,β=0,γ=0来得到AAA点的重心坐标。 那么为什么重心坐标的三个值之和为111呢？这是为了限制所得到的点是在三角形所在的平面内。 如果在三角形内有任意一点，我们该如何计算它的重心坐标呢？其实可以根据这个点与三点的连线所划分出的三角形的面积来求得，如下图所示。 除此之外，根据重心的定义方法我们可以得到一个非常特殊的点，就是三角形的重心，重心有一个非常好的性质，即如果将重心与三角形的三点相连，会得到三个等面积的小三角形，所以重心的重心坐标就是(13,13,13)(\\frac{1}{3},\\frac{1}{3},\\frac{1}{3})(31​,31​,31​) 上面我们知道计算重心坐标需要计算面积，下面给出一个更为简化的方法，直接使用重心坐标的一般表达式（当然也可以用面积来推出这个公式）： 接下来我们就可以用重心坐标来做插值了。有了重心坐标，我们可以用这样的方法来计算顶点上的任意属性，可以是位置、纹理坐标、颜色、深度、材料属性等等。 需要注意的是，尽管重心坐标应用非常好用，但是它不能应用于投影。假如上图中三角形是空间中的三角形，但是如果我们把它投影到某一平面上，我们可以计算出三个点在投影之后的坐标，但是如果对投影之后的三角形来计算重心坐标就会得到一个不一样的坐标。这是说重心坐标不能保证投影后不变，如果我们想插值三维空间中的属性，就应该取三维空间中的坐标，来计算重心坐标而不能在投影之后再做。这就涉及到了关于深度的问题。在做光栅化，我们把三角形投影到了屏幕上，它会覆盖很多像素，这些像素都有中心，我们可以计算出中心所在的投影之后的三角形的位置，那么我们不可以在投影之后的三角形里面的深度做插值，而是**应该找到这个位置对应在三维空间中的坐标，然后在三维空间中计算出正确的插值，再把结果拿回来。至于怎么把投影到屏幕上的三角形再投影回去，应用逆变换就可以了。 因此在三维空间中的属性一定要在三维空间中做插值。**根本原因就是重心坐标在投影操作下会发生变化。 Applying Textures 了解了重心坐标，下一步就是去了解怎么把纹理应用在实际的渲染中。现在我们知道屏幕上的点在三角形上有一个位置（像素中心），我们也可以计算出贴图上任何一个点对应在三角形上的位置了，使用重心坐标做插值即可，我们只需要从纹理上查询对应的颜色，就可以得到屏幕上的点的颜色，我们可以将其视为Blinn-Phong模型中漫反射的系数kdk_dkd​，这就相当于把贴图贴在了物体上。但是这样简单的操作可能会产生一些问题。 问题一：Txture Magnification （过小的贴图） 假设要渲染一堵墙，定义渲染分辨率为4k，但是使用的纹理只有256×256256×256256×256，这时任意一个点去查找颜色的时候会查到一些非整数的值。也就是说，纹理太小了，纹理就会被拉大，拉大了就会出现如下图所示的现象。一个解决方法是，当查到非整数的值时，直接四舍五入RoundRoundRound成整数，这样在一定范围内，很多像素要查找的是相同的纹理上的像素(texel)，这样就会得到下左图结果。 那么我们如何才能得到上图右侧两张图的结果呢？本质上看就是如何把查询得到的一个非整数的值来做一个模糊的效果。 Bilinar Interpolation 双线性插值是其中一个方法。假设我们的高分辨率像素的中心映射到了一个非整数的位置上，下图4×44×44×4的格子为texels，假设映射到了下图红点处，那么我们如何知道纹理在这个点处的值是多少。 在四舍五入的方法中，相当于直接去找离它最近的texel的中心，那当然在该texel里的所有像素都显示了相同的颜色。一个巧妙的方法是先找该点邻近的四个texels。 接下来，再连接四个texels的中心，连成一个四边形，我们可以找出红点距离该四边形左下角的点的分别在水平和竖直方向上的距离，分别记作s,ts,ts,t，我们定义两个相邻texel的距离为1，因此这两个值一定在(0,1)(0,1)(0,1)之间。 然后我们定义一个操作，叫线性插值：lerp(x,v0,v1)=v0+x(v1−v0)lerp(x,v_0,v_1) = v_0 + x(v_1-v_0)lerp(x,v0​,v1​)=v0​+x(v1​−v0​)，如果我们用sss来作线性插值，我们可以求出u0=lerp(s,u00,u10)u_0 = lerp(s, u_{00}, u_{10})u0​=lerp(s,u00​,u10​)和u1=lerp(s,u01,u11)u_1 = lerp(s, u_{01}, u_{11})u1​=lerp(s,u01​,u11​)，水平向的插值完成后，我们还可以再对竖直方向做一次插值，使用ttt将两个值插值即可，即f(x,y)=lerp(t,u0,u1)f(x,y) = lerp(t,u_0,u_1)f(x,y)=lerp(t,u0​,u1​)。 因此我们发现，红点处的颜色综合考虑了它周围四个点的颜色，并且这个红点处的颜色是这四个点平滑过渡的颜色。由于做了水平向和竖直向两类线性插值（顺序无关），因此称为双线性插值（Bilinar Interpolation）。 使用双线性插值方法，就消除了因为贴图过小而造成的锯齿效果，但是它的质量并不是最好的，比如在问题一中的图，还有一种方法称为Bicubic插值方法，它和Bilinar插值的区别在于，它取了周围16个texels做插值，只不过每次用4个做插值，这就有三次的插值。因此它的计算量要大，消除锯齿的效果更好。 问题二：Txture Magnification （过大的贴图） 那么如果纹理过大会怎样呢？纹理大了会引起更严重的问题，如下图所示，假设一张面贴了一张纹理，纹理是格子，如果我们还是用像素的中心找纹理坐标，再把这个值写回像素，我们就会得到下右图结果，远处效果为摩尔纹，近处为锯齿，即走样问题。 我们来分析一下问题在哪，因为近处，覆盖的纹理上的区域相对较小，在远处，一个像素覆盖了很大的一个区域。也就是说屏幕上的像素覆盖了纹理上的区域的大小是各不相同的。之前我们做抗锯齿使用了MSAA，也就是对一个像素使用更多的样本来采样，这里我们同样也可以这么做，这样得到的结果也是可以的，但是计算量过大。 走样问题就是信号变化过快，我们采样的频率跟不上信号变化的频率。在这个问题上体现在， 当纹理特别大的时候，一个像素里面可能包含很大的频率，这样就需要更高频的采样方法才能够跟上纹理变化的频率。如果我们不想用这么多的采样点改怎么办？我们这里可以避免采样，原本我们做采样是像素在纹理上覆盖很大一块区域，但如果我们立刻就可以知道这个区域里的平均值是多少就好了。我们要解决的问题就是，对于任何一个区域我们立刻就能求出它的平均值，我们可以使用Mipmap。 Mipmap Mipmap是一个在图形学中广泛运用的经典概念，它能做范围查询（fast, approx, square）。这个算法快，但是只能做近似的、方形范围查询。Mipmap就是从一张图生成一系列图，例如有一张纹理为128×128128×128128×128，称之为第0层纹理，我们可以生成更多更高层的纹理，每一层都是上一层缩小到一半的结果。例如第0层为128×128128×128128×128，第一层为64×6464×6464×64，第二层为32×3232×3232×32，直到最后变成一个像素，这样一共就有logloglog层。 我们可以在渲染之前把这些Mipmap都生成，问题在于，我们生成了这所有的Mipmap相比于原本的图，占用了多大的存储量呢？答案是1+14+116+...=431+\\frac{1}{4} + \\frac{1}{16} +... = \\frac{4}{3}1+41​+161​+...=34​，也就是说，原本的图存储量是111，生成它的所有Mipmap只比原来多占用了原本存储量的13\\frac{1}{3}31​。 接下来我们要用Mipmap近似地在一个正方形区域内做范围查询，要立刻得到范围内的平均值。可以想象到，任何一个像素都可以映射到纹理上一个区域，那我们该如何得到这个区域呢？很简单，例如图中我们像素上的蓝色和红色采样点，蓝色点有它的邻居，红色点也有它的邻居，如果我们想算红点所占据的像素的覆盖面积，我们可以取它自己的中心和它邻居的中心分别投影到贴图空间上去，在屏幕空间中，所有点到其邻点的距离都是一个像素，那么我们能求得它们映射到纹理贴图上的距离，映射后会得到一个不规则的区域，我们可以使用一个正方形来近似这个不规则的区域。 下面的问题是，我们如何根据计算好的Mipmap来计算这个边长为LLL的正方形的区域的平均值？我们只需求log2Llog_2Llog2​L，即可求出这个正方形应该所在的层数，这样就能查出这个区域的平均值了。 这样一来，离摄像机近的点就会在很低层去查询，离摄像机远的就会在很高的层去查询。但是会发现，不同层之间点的颜色可能不是连续的，因为我们只算了离散的若干层，例如我们算了第111层，算了第222层，但是不知道第1.81.81.8层的结果。 怎么解决这个问题呢？还是插值，我们得到了第111层和第222层，那我们对这两层内部分别使用双线性插值，做出来之后我们可以把这两层双线性插值的值合在一块就可以在层与层之间再做一次插值，这样就是第三个不同的插值，这里我们叫三线性插值（Trilinear Interpolation）。这样，在纹理的内部，不管坐标是否为整数坐标都可以双线性插值出一个平滑过渡的值，在层与层之间也可以插值出一个平滑过渡的值，这样就可以对于任何一个像素中心，做一次查询就可以得到所覆盖的区域的平均值。使用三线性插值，我们得到了下图所示结果（由于几何造成的问题暂时忽略），它在游戏、实时渲染领域得到了非常广泛的运用，因为它可以得到一个完全连续的表达，并且开销很小。 回到我们前面的示例，Mipmap是否能够真的解决问题呢？我们假设一个像素做512个采样点来得到的结果是一个准确的结果，如下图所示。 那如果我们使用Mipmap，得到如下结果，会发现在远处Mipmap把所有的细节全部都忽略掉了，远处出现了完全不应该糊掉的区域，我们称之为Overblur。为什么会出现这种情况呢？因为它只能查询一个方块的区域内的平均值，如果不是方形那就没办法。 Antisotropic Filtering 有一个办法可以部分解决Mipmap产生的问题，就是各向异性过滤（Antisotropic Filtering）。它的效果要比Mipmap要好。Mipmap本身是将原始的一张图，将其长宽各不断地缩小一半，Mipmap其实是计算反映在对角线上的图片，而各向异性过滤比Mipmap多了不均匀的水平和竖直的压缩，各向异性就是水平向和竖直向都有压缩，如下图所示，比如对于卫星来说，每一行都是高度不变，宽度变，每一列都是高度变，宽度不变。也就是说通过这种方式的预计算，我们可以查询到任何一个被压扁的图上的一个位置，这样我们就可以查询到一个矩形的区域而不被限制在正方形。 这是因为屏幕上的像素映射到纹理上以后很有可能是一个不规则矩形，如果我们近似成一个正方形，就会求一个很大的区域，这样就会造成overblur。如果我们使用了各向异性过滤，就可以得到一个矩形区域，自然得到的结果就会好很多。但是假如一个像素对应到贴图中是一个斜45度的矩形，各向异性过滤仍然不能很好的解决问题，因此它只是部分解决问题。 因此人们又发明了另外一种方法，称为EWA过滤，它是将映射后的不规则的形状拆成很多不同的圆形去覆盖这个不规则形状，比如一个椭圆可以被拆成三层椭圆进行多次查询来得到结果，但是代价就是查询量大。 注意，各向异性过滤的额外开销是原本的3倍，而Mipmap仅为原本的13\\frac{1}{3}31​。在游戏中我们经常能看到有个“多少xxx”的选项，意思就是计算多少层，比如2x2x2x就是方向上压缩一次，4x4x4x就是各方向压缩两次，随着xxx的增加最后存储量达到原始的333倍，它和显存关系很大和计算力关系不大，这意味着如果显存足够可以把各向异性过滤开到最高，对性能几乎不会有影响。 到目前为止，除了阴影没讲解，其他整个渲染过程都已经完成了。 Apllications of textures 根据前面的内容，我们知道给定一个网格，我们可以做着色，例如Flat Shading得到一个个格子的shading，也可以做Phone Shading。接着可以对它做各种贴图，下面讲解一些高级的纹理应用。 首先纹理就是一张图，我们说它可以做各种各样的应用，在现代GPU贴图，我们可以把纹理理解成一块内存，并且我们可以对纹理上的这块区域进行一个范围查询（或着说过滤），并且查询速度非常快。因此纹理完全可以理解成一块数据，可以做不同类型的查询，没有必要完全限制在一个图像上。从这个角度出发，它可以表示的东西就太多了。 环境光照 环境光照，也有叫环境光映射或者环境贴图。假如我们站在一个房间里面往四面八方看，会发现有来自四面八方的光，如果我们把任何一个方向的光记录下来，就能得到环境贴图（环境光照），因此我们可以用这幅图来做渲染，使它能够反射出任何方向来的光，例如下图中的茶壶反映出了窗户。因此我们可以用纹理去描述环境光，这就比只用一个点光源要好很多，具体的计算方法后面再说。 正常来讲，我们记录环境光信息不能只记录方向，物体在空间内的不同位置会有不同的环境光效果，但是这里在环境光贴图中只记录环境光的方向。 环境光是怎么得到的呢？我们可以在空间中放一个镜面球，它所反射出来的就是环境光。因此我们可以把环境光存储在一个球上，然后把它展开得到环境光贴图。这就是Spherical Environment Map。 但是如果我们把它展开，这里会有一个扭曲问题，这可以参考世界地图，南极洲的实际大小受到了球展开时扭曲的影响。后来，人们就发现了一个办法来解决这个问题，他们将球用一个包围盒包住，接着我们用从球心到球上某一位置的连线作延长，直到接触到包围盒的表面上，这样就将这些信息存到了立方体的表面上，就能得到六张图，因为一个立方体可以展开成六个面。因此，我们可以把环境光记录在一个立方体所对应的各个表面上再展开，立方体的各个面都是均匀的，因此它能够避免扭曲，这一方法称为Cube Map。但是这一方法也有它的问题，给定一个方向，如果要找对应的颜色则需要先判断它在立方体的哪张面上，增加了计算量。 凹凸贴图 纹理的另一个很重要的应用是凹凸贴图。之前我们用纹理是为了替换Blinn-Phong模型里的参数kdk_dkd​，除此之外，纹理还可以定义任何属性。比如它可以定义在一个模型上的点的相对高度。一个球假设它原本有一个基础的表面，然后纹理可以定义这个表面上的点沿着法线方向的相对高度。 比如上右图所示，一个类似橘子的球，如果我们用三角形来表示则需要无数个三角形面，而如果我们用纹理来定义橘子表面上的凹凸质感，就可以定义任何一个点的相对高度。我们知道，相对高度变了意味着法线就会变，法线变换后着色就会变化，人们在看到一定程度的明暗变化后就会认为这里有凸起的质感，因此，使用贴图可以人为地制作出假的法线，从而得到一个假的着色结果最终欺骗人眼，就能既做出凹凸贴图的质感又能避免复杂的几何。这就是凹凸贴图的基本原理。 我们再具体来看，我们知道了，通过凹凸贴图，我们可以定义一个复杂的纹理，但是并不改变几何信息，也就是说三角形数不变。然后我们对每一个像素的法线做一个扰动，通过定义的不同位置的高度，根据邻近不同位置的高度差来重新计算它的法线。也就是说，纹理定义的是任何一个点相对高度的移动。如上图所示，黑色线为原始的物体表面，然后我们运用了一个贴图，来告诉我们这个点的相对高度应该如何变化。对于任何一个点ppp，它原本的法线应该是垂直于黑色表面的，但是凹凸贴图改变了它的相对高度，其法线自然也发生了改变，变成了垂直于黄色线表面。那么我们应该如何计算它的变化呢？ 如上图所示，我们先从一维来考虑，假设原本的表面是一个平面，蓝色为凹凸贴图定义出来的，那么原本点ppp的法线应该是(0,1)(0,1)(0,1)。现在要求凹凸贴图变化后的点ppp的法线，只需先求出该点在凹凸贴图定义的函数上的点的导数dp=c[h(p+1)−h(p)]dp = c[h(p+1)-h(p)]dp=c[h(p+1)−h(p)]，这里定义了ccc为一个常数，来表示凹凸贴图的影响。这样我们就得到了切线。接着，我们知道法线就是垂直于切线的方向，只需要将切线逆时针旋转90°即可得到法线，法线为n(p)=(−dp,1).normalized()n(p) = (-dp, 1).normalized()n(p)=(−dp,1).normalized()。 所以整个思路就是，我们用凹凸贴图来定义切线，再通过切线来计算法线。 那么在三维中，假设表面ppp上的法线n(p)=(0,0,1)n(p) = (0,0,1)n(p)=(0,0,1)，那么贴图如何影响该点的法线呢？我们仍然可以先求出它的梯度，即dpdu=c1[h(u+1)−h(u)]\\frac{dp}{du} = c_1[h(u+1)-h(u)]dudp​=c1​[h(u+1)−h(u)]和dpdv=c2[h(v+1)−h(v)]\\frac{dp}{dv} = c_2[h(v+1)-h(v)]dvdp​=c2​[h(v+1)−h(v)]，这表明了点ppp在uuu和vvv两个不同方向上的变化，这样就表示出了切线方向，最后法线方向即为n=(−dpdu,−dpdv,1).normalized()n = (-\\frac{dp}{du}, -\\frac{dp}{dv}, 1).normalized()n=(−dudp​,−dvdp​,1).normalized()。注意，这里我们定义了一个局部坐标系，即认为所有的点一开始的法线都是n(p)=(0,0,1)n(p) = (0,0,1)n(p)=(0,0,1)，我们是在这个坐标系中对法线进行了更改，最后再将这个法线变换到世界坐标系中。 Displacement mapping 除了凹凸贴图，还有一个更现代化的做法，叫做位移贴图（Displacement mapping）。它和凹凸贴图一样，都是通过纹理去定义任何一个点的相对高度差，因此输入完全一样，使用了完全相同的纹理，只不过位移贴图会真的将顶点做移动，而不是通过移动位置换算成法线变化来实现假的效果。凹凸贴图不会改变物体的几何，这会在物体的边缘处“露馅”，边缘处的凹凸质感不会被表达出来，凸起的部位也不会投影到自身。而位移贴图会实际地改变物体的几何，它不会有上述问题，但是它对模型的三角形数量有着严格要求，即三角形的数量要足够多来使采样率比纹理定义的频率高（又是采样问题）。 前面所说的各种纹理的应用都是二维的，即我们仍然将纹理当图来看，其实纹理也可以是三维的，如下图所示，有一个球，如果我们将其砍掉一半，我们希望能够看到它的内部是怎样的，这就要求纹理能够定义空间中任何一个点的值。可想而知，实际上这样的纹理的图并没有被生成，只是定义在一个三维空间中的噪声函数，这样给定空间中任何一个点都能算出它的值是多少，这个噪声可以经过一系列处理，就可以变成我们需要的样子，比如大理石的质感。 纹理还可以记录一些之前已经算好的信息，我们已经知道了怎么做着色，但是还不了解阴影。下图我们发现，最右侧眉毛凸起的部位遮挡住了眼圈，这就是说这里产生了阴影，我们之前算shading的时候会考虑不到这里。我们是可以通过**环境光遮蔽（Ambient Occlusion）**来计算出这里的阴影，这里先简单提一句，这个方法就是先计算好这些阴影，再把它写进一张贴图，最后再贴回来，所谓贴回来也就是相乘，可见就是1不可见就是0，也就是说，着色的结果乘以计算好的环境光遮蔽的纹理就能得到右图的结果了。也就是说很多计算我们可以提前去做，然后用纹理来记录这些信息，之后就取决于如何解释。 最后，纹理还可以运用到体积渲染中去，原本我们说光照模型只考虑一个表面，然而在医学里，我们会用CT呈像扫描人体最后返回出三维空间的信息，我们可以通过记录这些信息来做渲染，这些信息我们也称其为纹理。 ","link":"https://albertlidesign.github.io/post/games101_4_texture_mapping/"},{"title":"Games101(3): Shading","content":"课程链接：GAMES101-现代计算机图形学入门-闫令琪 课程讲师：闫令琪 本系列笔记为本人根据学习该门课程的笔记，仅分享出来供大家交流，希望大家多多支持GAMES相关讲座及课程，如涉及侵权请联系我删除：albertlidesign@gmail.com 关于光栅化还有一点关于Z-Buffering的内容，我们在这里先做一个补充。前面的课程我们知道了屏幕就是一堆像素，如何光栅化一个三角形，一些采样理论的知识和如何进行反采样，就是先做模糊再做采样。但是上一讲我们只说了频域分析，介绍了先模糊再采样是正确的，但是没有讲为什么先采样再模糊是错的。这里给大家一个提示：先采样就是把信号的频谱进行搬移，它会有频谱的混叠，然后模糊就是在混叠之后截断信号，这里就会发现混叠的信号截断了还是混叠的。（采样=频谱搬移，模糊=截断） Z-Bufferring 众所周知，空间中会有好多三角形，三角形各自离相机的距离也不一样，那么我们该如何将这些三角形画在屏幕上并且它们的遮挡关系是对的呢？近处的永远要遮挡远处的。在这里我们使用的方法就是深度缓存（Z-Buffering）。 Painter's Algorithm 场景中有很多不同的物体，既然要把这些物体放在屏幕上就要涉及到顺序的问题，以保证这张图是对的。一个比较直观的方法是像画水粉、油画一样，先把最远的物体绘制出来，再把近的物体覆盖在远的物体上，层层叠加来形成最终的结果。 画家先画远处的山，再画近处的草地，最后再在草地上画上树。这样由远及近地对物体做光栅化的算法就称之为画家算法（Painter's Algortihm）。例如如果我们画一个立方体，可以先画最远的面，然后再画周围的四个面，最后绘制最前方的面，这样就能出正确的结果。但是仔细思考会发现，如果绘制的顺序必须要非常严格，如果错了会造成多余的线被绘制。因此在一定程度上说，这个算法是可以的，它要求对所有物体的深度进行排序，排序的复杂度为O(nlogn)O(n logn)O(nlogn)（对于nnn个三角形）。但是当三个空间三角形构成一组互承关系时，三个两两之间存在了覆盖关系，这样就没法定义它们之间的深度关系，也就无法对它们进行排序。因此为了解决这个问题，人们提出了Z-Buffer。 Z-Buffer 在图形学里，人们广泛采用了Z-Buffer，引入了深度缓存的概念。这一算法其实就是避免空间中三角形的排序，它是从像素的角度来检测能看到的三角形，对于每一个像素去记录它所看见的最浅的深度的物体，也就是距离相机最近距离的物体。在图形学中，我们不光能渲染出最后的结果（Frame Buffer），在生成这个图像的同时，我们还能得到一个深度图（Depth Buffer），它记录了每个像素所看到的几何图形的最浅深度的信息。我们利用深度图来维护遮挡信息。 在变换中，我们提到，我们始终假设相机是放在原点，并且看向−Z-Z−Z，这样所有的点的ZZZ坐标都是负的，并且它们越小，说明离得越远。为了方便理解，现在我们换一个概念，我们把它看作是点到摄像机的距离，这样ZZZ就总是正的，那么ZZZ值越小，物体越近。 如上图所示，对于一个像素会记录地板的深度和物体的深度，假设对于有一个能看到物体的像素，它先记录了地板的深度，接着再记录物体的深度，如果物体的深度比地板要小，那么意味着这个物体要遮挡住地板，这个点就要画上看到物体的颜色，然后右侧的深度图也会做相应的更新。这一流程的伪代码如下： // first step: initialize depth buffer to infinite for (each triangle T) for(each sample (x,y,z) in T) if (z&lt;zbuffer[x,y]) // closest sample so far framebuffer[x,y] = rgs; // update color zbuffer[x,y] = z; // update depth else ; // do nothing, this sample is occluded 注意：RRR为无限大的一个值。初始化时要把所有的像素定义为无限大（或者足够大的一个值）。 在使用画家算法时，我们要把所有的三角形做一次排序，这需要花费O(nlogn)O(n logn)O(nlogn)的时间。而深度缓存，每个三角形都覆盖常数个像素的话，无非就是考虑每个三角形所覆盖的常数个数的像素，也就是O(n)O(n)O(n)的算法复杂度。它并没有进行排序，只是不断地更新结果，记录当前所看到的最小深度值，因此只花了线性的时间。 这里要注意，不会出现两个不同的三角形在同一个像素上有着相同的深度，因为深度缓存算法与顺序是没有关系的了，不管通过什么顺序绘制三角形，最后的结果都会是相同的。在图形学中，我们都是用浮点型来表示的，浮点型和浮点型判断相等是非常困难的事，基本上可以认为两个浮点数是完全不会相同的。此外，这一算法可以应用在几乎所有的硬件中。 之前提到，为了做反走样，我们使用了MSAA算法，对一个像素取很多采样点，对于这些不同的采样点，如果我们要运用深度缓存，就要对每一个采样点检测它所能看到的深度，因此Z-Buffer还得考虑到它不是对每个像素记录深度，实际上是对每个采样点做记录。 Shading 到目前为止，我们已经学习了如何将物体随着相机，变换到相机位于原点看向−Z-Z−Z方向时的位置，即View Transformation，接着我们可以将模型映射到二维的从(0,0)(0,0)(0,0)到(w,h)(w,h)(w,h)的屏幕上，接着根据二维屏幕中的像素与三角形之间通过采样求出像素的值，也就是光栅化。 目前所能做的事表现出来就是这样的 而我们期待的结果是这样的，它与我们目前做到的结果的区别在于着色。 在下面的渲染图中，可以看到杯子中有茶、蛋挞、葡萄，不同的物体会有不同的颜色，在不同的光照下，这些物体上的颜色会有变化，这都是着色的问题。 着色 (Shading）在Merriam-Webster字典里的意思为： shad·ing, noun, The darkening or coloring of an illustration or diagram with parallel lines or a block of color. 在我们这门课中，它的含义是：The process of applying a material to an object. 即对不同物体应用不同材质的过程。不同的材质和光照会发生不同的作用效果。 Blinn-Phong 最基础的着色模型为Blinn-Phong反射模型。通过下图几个茶杯，我们可以看到光源应该在右上方，在每个茶杯上有高光（Specular highlights），茶杯的其他部位的变化相对不明显，这部分叫做漫反射（Diffuse reflection）。对于最底下的茶杯的最左侧，光源从右上方照射，按说这个茶杯的最左侧不应该为黑色，因为这部分没有被光直接照射到，但为什么这里有颜色呢？是因为间接光或环境光（Ambient lighting），它是由光线在其他物体之间发生反弹从而照亮这部分区域的。任何一个点都会接收到来自环境的反射光，它很复杂，我们在后面会讲到。 我们看到一共有三部分，高光、漫反射和间接光，我们可以把这三部分分别做出来从而得到一个很相似的结果。在开始之前，我们需要定义一些东西。 我们现在考虑光照是对任何一个点而言，假设这个点叫做shading point，那么这个点的着色结果是什么？我们定义在一个极小的范围内，它是一个平面，即与它所在的曲面相切的平面 既然是平面，那么就有法线n⃗\\vec nn垂直于平面 同样我们还可以定义一个观测方向，我们规定从shading point到相机的方向为观测方向v⃗\\vec vv 同样道理，从shading point到光源的方向称为光照方向l⃗\\vec ll 注意，因为我们只关心这些向量的方向，所以它们都是单位向量，长度为111 我们还需要定义一些表面参数，比如颜色，亮度（shininess）等 Diffuse Reflection Blinn-Phone有三个不同的部分，我们从最简单的漫反射开始。当有一根光线打到物体表面上的某一点，光线会被均匀地反射到不同方向上去，这个过程叫漫反射。 当我们考虑shading point所在表面的朝向与光照方向有一定夹角的时候，会发现得到的明暗是不一样的。如图所示，假如光是离散的，有六根光线打到表面上，每一根光线代表一个固定的能量，如果表面和光线垂直的话会接收到所有的光，但是如果表面旋转到了某一个角度，比如60°60°60°，我们会发现只接受到了三根光线，那么就会变得暗一些。因此会发现表面的明暗会与光线与法线的夹角存在一定关系。为了量化，我们用shading point周围的单位面积来定义接收到的能量，它与夹角有关，这就是Lambert的余弦定律。 提到了接收能量也不得不提一下发散能量。不同的物体被光所照亮，光是一种能量，在这里我们认为它是一个点光源，它会无时无刻地在向外辐射出不同的能量，一个很聪明的观测方法是一个点向外发散能量，那么在某一时刻，它们一定集中在某一个球壳上。根据能量守恒定律，离中心近的球壳和离中心远的球壳的能量应该是完全相同的能量，但是随着球壳离中心越来越远，其表面积会越变越大，那也就意味着在某一个点，它的能量会越来越少。因此我们定义在距离为111的地方定义光的强度为III，如果传播到距离为rrr的球壳上时，它的能量为Ir2\\frac{I}{r^2}r2I​。这就告诉了我们，光在某一时刻某一位置所能传播的能力是与它与光源的距离的平方成反比的。光线传播的距离越长，所能接收到的能量越小。也就是说，只要知道一个点光源，又知道shading point离光源的距离，那么就能知道有多少光传播到了当前的shading point。 根据前面我们又知道了有多少光能被接收，我们就得到了diffuse的表示方法了： 假设我们有一个点光源，假设它与shading point的距离为rrr，我们定义在单位距离上它的强度为III，那么我们就能求出它到达shading point处的能量，我们有算出了有多少光能被接收，就能算出我们最后所看到的在这一点上的能量。做一个max(0,n⃗⋅l⃗)max(0, \\vec n · \\vec l)max(0,n⋅l)是说，当向量点乘为负数的时候，这是说光从背面穿过了物体达到了shading point，显然是不可能的，因此没有物理意义，因为我们考虑的是反射，因此当点乘为负时就认为是000。我们再来考虑一个问题，对于shading point，它自己本身为什么会有颜色，是因为这个点会吸收颜色，或者说能量，它反射出去的是不吸收的颜色。如果我们在能量被接收后定义一个系数kdk_dkd​，表示漫反射系数，如果为111就是最亮，如果为000那么表面就是黑的，那么如果我们把它表示成一个三通道的向量就可以表达它对RGB三个颜色的反射程度。 既然光打到shading point被反射到各个方向，那就意味着无论我们从哪观测它，所看到的结果应该是一样的，从公式上看也是如此，我们考虑的是光线与法线的夹角，因此漫反射跟观测角度无关完全无关。 （可以想想，你处在一个黑屋子里，眼前一个石膏球被照亮，无论你从哪个角度看这个石膏球，除了被照亮的部分其他地方都看不到，因此漫反射与观测角度无关） 需要注意的一点是：我们说的着色是在一个点上进行着色，如果想要得到一整张图就要着色很多次。 Specular Term 高光有一个特点：它的反射方向非常接近镜面反射的方向。如果是镜面，这个物体就是无限光滑的，我们可以根据入射方向和法线来求出它的出射方向，如图R⃗\\vec RR所示。如果物体是金属，那么这个物体就没有那么光滑，它的反射方向会沿着R⃗\\vec RR分布，当我们的观察方向和镜面反射方向接近的时候，我们就能看到高光了，其他时候我们都看不到高光。这就告诉了我们，高光项和我们观察的方向及镜面反射方向有关。 Blinn-Phong模型做了一个很聪明的事，因为当我们的观察方向和镜面方向接近的时候，其实就说明法线方向和**半程向量（Half Vector）**很接近。这是说，如果给定入射方向l⃗\\vec ll和出射方向v⃗\\vec vv，我们可以求它的角平分线方向，只需要将两个向量相加，根据平行四边形法则，然后再做归一化即可得到两向量的半程向量h⃗\\vec hh。如果此时h⃗\\vec hh和n⃗\\vec nn接近，一定程度上就可以反映v⃗\\vec vv和R⃗\\vec RR接近。这样的话，就能根据n⃗\\vec nn和h⃗\\vec hh的点乘的结果来算出高光，得出如下图所示公式。又因为通常高光都是白色，所以高光系数ksk_sks​为白色的值。当然，我们还要考虑有多少能量被吸收，也就需要加上一项l⃗\\vec ll和n⃗\\vec nn的点乘，这里没有考虑是因为Blinn-Phong模型是经验性模型，这里将其简化掉了，其主要关注的是是否能看到高光。 那么为什么要用半程向量而不是直接用v⃗\\vec vv和R⃗\\vec RR呢？当然可以，那个模型就被称作Phong Reflection Model，Blinn-Phong是它的一个改进。这是因为半程向量太好算了，而反射方向就不那么好算了，计算量要大很多。除此之外，观察上图公式，我们在n⃗\\vec nn和h⃗\\vec hh的点乘加了一个指数ppp，这是因为尽管向量之间的夹角余弦值能体现两个向量是否足够接近，但是容忍度太高了，比如45°45°45°时，它的余弦值仍然很大，如果我们只用夹角余弦去做高光的话会得到一个超级大的高光，看上去就很不自然，我们平常认为高光是非常亮的并且集中在很小的区域中。所以我们要对夹角余弦加上若干个指数就能得到较为合理的结果，正常情况下我们用的指数要达到[100,200][100,200][100,200]。 下面是一个实际的例子，显示了漫反射和高光项在一块的效果，我们会发现随着指数ppp的增长，高光会越来越小，因此指数ppp就是用来控制高光的大小的参数。 Ambient Term 环境光是一个非常复杂的东西，我们做一个非常大胆的假设（但是事实上不是这么回事），假设任何一个点所接收到的来自环境的光永远都是相同的，我们记作IaI_aIa​，再给定一个系数kak_aka​就可以直接近似地来得到环境光。观察图我们知道，环境光跟视点无关，跟光源位置也无关，并且和法线也没关系，因此环境光其实是一个常数，也就是某一种颜色。比如你看到一个物体，任何一个地方都有一个常数的颜色，总会得到一个“平”的结果。环境光的作用就是保证没有地方是黑的。但实际上如果我们需要很精确地计算它，就需要全局光照的知识。 现在我们把所有的项都加起来就可以看到一个完整的Blinn-Phong Reflection Model。我们知道Blinn-Phong是个着色模型，它对所有的点都进行了着色。 Shading Frequencies 接下来讨论着色频率的问题，首先我们有这三个球，这三个球有着完全相同的几何形状（一模一样的模型），这从观察边界可以得出，那么为什么着色之后我们得到的结果各不相同呢？这就是因为不同的着色频率，即着色运用到哪些点上。如果我们把着色运用到网格面上，一个平面只做了一次shading，就能得到下图最左侧的结果。中间的结果是对每一个网格面上的顶点进行着色的。先求出它们的法线再对每一个顶点做一次着色，在面的内部通过插值的方法算出来。最右侧的着色是对每一个像素进行着色的。也就是说，我们对每一个四边形或三角形的顶点求出一个法线，然后把法线的方向在三角形内部进行插值，然后就得到任何一个像素自己的法线方向，并且可以做着色。也就是说，如果着色运用到像素上就可以得到非常好的结果。 下面我们对这些方法来做一个正规的定义： 每一个三角形都是平面，每个三角形的法线都非常容易求出，只需要将三角形的两个边做叉积，这样就可以算出一个shading结果，但也自然在三角形内部不会有着色的变化，也就是每个三角形面内各点的颜色是完全一样的。当然这个结果不太好，但是有它自己的名字，称为Flat Shading。 我们可以在任意一个顶点处求出它的法线，对每个顶点做一次着色，然后通过插值计算出每个三角形内部的颜色，得到的结果要比Flat Shading要好，但是当三角形大一点的话，高光可能就看不见了，因此它的效果也是有局限性的。这样的着色叫Gouraud Shading。 如果我们对于每一个像素，求出各三角形顶点的法线，然后在每一个像素都插值出一个法线方向，再对每一个像素进行一次着色，就能得到相对比较好的结果，这个结果就叫做Phong Shading。注意Blinn-Phong是一种着色模型，这里的Phong Shading指的是着色频率。 这三种着色具体的区别其实也取决于具体的模型，并不是说Flat Shading就一定会很差，下图中，每一行的模型都是完全一样的，每一列是不同网格顶点数的区别，也就是说当我们的几何模型相对复杂的话，其实也可以用一些相对简单的着色模型，而且得到的结果还可以。着色频率取决于面、点数量。当然，Phong Shading的着色效果好，其计算量当然也比Flat Shading大很多（但也不绝对，如果面数超过了像素数那么Phong Shading可能更小），所以具体用哪种着色方法要取决于具体的物体。当面数不是特别多的情况下，Phong Shading能得到一个较好的结果。 Defining Per-Vertex Normal Vectors 在Gouraud Shading中我们要对每一个顶点求法线，那么该如何做呢？ 最好最简单的方法是，如果使用网格模型想拟合的模型，比如球模型，去求这几个网格点所在的球模型上的法线即可。但是它的应用情况会比较少。 第二种方法，一个顶点通常会位于多个三角形面上，即这多个三角形面共用该顶点，那么我们只需要求过该点的三角形面的法相的平均即可，也可以以为三角形面积为权重做加权平均。 Defining Per-Pixel Normal Vectors 下面一个问题是如何去定义一个逐像素的法线？我们要通过重心坐标的方法来进行插值，下面会详细讲，这里注意求出来的法线都要做一个归一化的处理，以保证它们的长度是一致的。 Graphics(Real-time Rendering) Pipeline 现在，把前面所有的知识合在一起就已经能够得到一个渲染的结果了。把这所有的东西合在一块就叫做图形管线（Graphics Pipeline），闫老师更愿意叫做实时渲染管线。当我们输入三维空间中的一些点，中间经历了什么样的过程，这个过程就叫Pipeline，其实就是一系列的操作。 如下图所示，下面整个的过程就是从三维场景到最后看到的二维像素的过程。而这个过程是已经在硬件里写好了，显卡所做的整个的操作就是这样的操作。 （1）输入三维空间中的点； （2）投影变换，我们将三维空间中的点投影到了屏幕上； （3）形成三角形 （4）屏幕是离散的，因此要通过光栅化来把三角形变成Fragments（OpenGL中的概念，类比于像素） （5）着色Fragments （6）显示 一个小问题：为什么说我们在投影到屏幕上再连接三角形而不是一开始输入三角形呢？其实是一样的，因为顶点无论如何变换，其连接关系是没有变的，因此在输入的时候用上连接关系形成三角形还是在投影之后形成三角形没有区别。 （1）Vertex Processing：对空间中每一个顶点做MVP变换。 （2）Rasterization：对每个像素采样判断是否在三角形内，即光栅化。 （3）Fragment Processing：判定像素是否可见（也可以归为光栅化）。 （4）Shading：Shading可以发生在顶点处理上也可以发生在Fragment处理上。 注意，这两部分是可编程的，即我们可以自己去决定如何运作，这部分代码称为Shader，它是控制这些顶点和像素如何着色的。 （5）Texture mapping Shader Programs 前面提到了Shader，我们这里更详细了解一下。现代的GPU允许用户通过编程来解决顶点和像素如何做着色，这就需要用户来自己写Shader，Shader本质上就是一个能在硬件上执行的程序。OpenGL作为图形学的API，可以用它来写Shader，它是对每一个像素所执行的通用的程序，因此不需要写For-Loop。如果我们写的是顶点的Shader，就叫做顶点着色器（Vertex-Shader），如果是对像素的操作就叫做片段（或像素）着色器（Fragment-Shader or Pixel-Shader）。 下面是一个具体的例子，像素着色器是要确定像素最后的颜色，即写清楚怎么计算像素的颜色并且输出出去。这个例子是简单的着色语言GLSL。 uniform sampler2D myTexture; // uniform指的是全局变量，定义了一个纹理 uniform vec3 lightDir; // 固定的光照方向 varying vec2 uv; varying vec3 norm; // 插值出来的法线 void diffuseShader() { vec3 kd; kd = texture2d(myTexture, uv); // 每一个像素可以拿到一个漫反射系数，具体操作跟纹理相关，暂时忽略 kd*=clamp(dot(-lightDir, norm), 0.0, 1.0); // 一个最最简单的漫反射的部分，用clamp限定到[0,1] gl_FragColor = vec4(kd, 1.0); // 将三维向量转四维向量，返回到gl_FragColor } 因此 Shader能够定义顶点、像素如何操作。现在就已经把整个实时渲染的基本思路涵盖到了，在这个基础上，就已经可以去学习一系列图形API了，会发现非常简单，所有的矩阵都不需要自己来做，都可以借助API来生成，可以很方便的写出来。这里推荐一个叫ShaderToy的网站，在这里可以只写着色器，即顶点和像素如何着色，就可以通过这个web来执行程序，就可以看出结果。Shader可以做到千变万化。 随着现在GPU的发展，显卡可以同时处理大量的几何，并且着色非常快，高度并行。现在的图形学就是向一个能够实时渲染超级复杂的场景发展。随着GPU的发展，有越来越多不同的着色器产生，比如一种叫Geometry Shader，它可以动态的产生三角形。还有一种Compute Shader可以做通用的GPU计算，称GPGPU。还需要提到，GPU分两种，一种是独立显卡，另一种是集成显卡。GPU本身可以理解为高度并行化处理器，CPU通产有8核、16核等，GPU的核数是CPU的很多很多倍，所以特别适合来做图形，因为很多像素的着色方法是一样的，这就非常利于做并行计算。 ","link":"https://albertlidesign.github.io/post/games101_3_shading/"},{"title":"Games101(2): Rasterization","content":"课程链接：GAMES101-现代计算机图形学入门-闫令琪 课程讲师：闫令琪 本系列笔记为本人根据学习该门课程的笔记，仅分享出来供大家交流，希望大家多多支持GAMES相关讲座及课程，如涉及侵权请联系我删除：albertlidesign@gmail.com 经过上一节我们将所有的物体都映射到了一个[−1,1]3[-1,1]^3[−1,1]3的立方体里，那么下一步我们该怎么办？下一步就是将这些物体画在屏幕上，这一步就叫做光栅化 (Rasterization)。 Canonical Cube to Screen 屏幕的定义 既然要画在屏幕上就需要把屏幕的概念定义好： 它是一个包含像素 (pixels) 的数组 (array) 数组的尺寸：分辨率 (resolution)，例如1920×1080(1080p)1920×1080 (1080p)1920×1080(1080p) 屏幕是一个典型的光栅成像设备 Raster在德语里就是screen，光栅化Rasterize == drawing onto the screen 像素pixel是&quot;picture element&quot;的简写，在这里我们定义一个像素是一个颜色均匀的小正方形，它包含(red,green,blue)(red, green, blue)(red,green,blue)三个值。 屏幕空间 屏幕空间就是在屏幕上建立一个坐标系，约定俗成地，以左下角为原点(0,0)(0,0)(0,0)，向右为XXX，向上为YYY，因此任何屏幕上的点都可以用(x,y)(x,y)(x,y)来表示。 每一个像素的坐标都是用(x,y)(x,y)(x,y)来表达，这里的xxx和yyy均为整数。例如图中蓝色的像素为(2,1)(2,1)(2,1) 如果我们定义屏幕的分辨率为width×heightwidth×heightwidth×height，则所有的像素的坐标的范围为从(0,0)(0,0)(0,0)到(width−1,height−1)(width-1,height-1)(width−1,height−1) 像素(x,y)(x,y)(x,y)的中心点位于(x+0.5,y+0.5)(x+0.5,y+0.5)(x+0.5,y+0.5)。例如蓝色像素的中心为(2.5,1.5)(2.5,1.5)(2.5,1.5) 整个屏幕覆盖的范围为从(0,0)(0,0)(0,0)到(width,height)(width,height)(width,height) 因此为了完成[−1,1]3[-1,1]^3[−1,1]3的立方体映射到屏幕这一操作，我们需要将物体的zzz坐标移去，将xyxyxy平面上的[−1,1]2[-1,1]^2[−1,1]2变换到[0,width]×[0,height][0,width]×[0,height][0,width]×[0,height]。我们只需将宽度和高度都除以222，然后再将它的中心从(0,0)(0,0)(0,0)移动到屏幕空间左下角，也就是移动宽度除以222和高度除以222的距离。视口变换的表达式如下： Mviewport=[width200width20height20height200100001]M_{viewport} = \\left[\\begin{matrix} \\frac{width}{2} &amp; 0 &amp; 0 &amp; \\frac{width}{2}\\\\ 0&amp; \\frac{height}{2} &amp; 0 &amp; \\frac{height}{2}\\\\0 &amp; 0 &amp;1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]Mviewport​=⎣⎢⎢⎡​2width​000​02height​00​0010​2width​2height​01​⎦⎥⎥⎤​ 到这一步时，我们已经得到平面上的图，三维空间中的网格模型经过以上变换，变成了屏幕空间中的多边形，我们需要把这些多边形进一步“打碎”，打成像素，变成每一个像素上的颜色值，这就是我们所说的光栅化。 Triangle Meshes 为什么选用三角形网格？ 它是最基本的多边形 任何多边形都可以被拆分成多个三角形 三角形网格的独特性质 三角形一定是平面图形。三点共面 三角形的内外定义清晰。可以通过向量叉积来定义一个点在三角形内还是三角形外 面内插值方便。只要定义三角形三个顶点不同的属性，就可在三角形内部做这个属性的渐变效果，也就是说，通过三角形面内的一个点和其他点的位置关系，可以得到一个插值。（重心坐标插值方法） 通过屏幕上三个顶点坐标，可以知道一个三角形在屏幕中的位置，如左图所示。为了绘制出这个三角形，我们需要判断每个像素的中心点是否在三角形内部，这里介绍一个最简单的方法：采样。 采样 采样就是给定一个连续的函数，在不同的点处求它的函数值。也就是说，采样就是把一个连续的函数离散化的过程。 for(int x=0; x&lt; xmax; ++x) output[x] = f(x); 采样是一个非常重要的概念，在图形学里会涉及到各种各样的采样，我们可以采样时间、面积、方向、体积等等。我们这里说的采样，是指利用像素中心对屏幕空间进行采样，也就是说我们需要求出某一个函数在屏幕中不同的像素中心的值。 我们这里的采样就是去判断每一个像素的中心是否在三角形的内部，因此我们可以定义出这个函数： inside(tri,x,y)={1Point(x,y)intrianglet0Otherwise,x,y not necessarily integersinside(tri, x, y) = \\begin{cases} 1 \\quad Point(x,y) in triangle t \\\\ 0 \\quad Otherwise \\end{cases} , x,y\\ not\\ necessarily\\ integersinside(tri,x,y)={1Point(x,y)intrianglet0Otherwise​,x,y not necessarily integers Rasterization = Sampling A 2D Indicator Function 我们使用两个for循环来遍历所有的像素，对每一个像素执行采样方法即可完成光栅化的过程： for (int x = 0; x &lt; xmax; ++x) for (int y = 0; y &lt; ymax; ++y) image[x][y] = inside(tri, x + 0.5, y + 0.5); 那么如何定义函数inside(tri,x,y)inside(tri, x, y)inside(tri,x,y)呢？也就是说我们需要去判断一个点是否在一个三角形内。我们可以用向量叉积的方法来判定，例如，如图所示，我们需要判断点QQQ是否在三角形(P0,P1,P2)(P_0,P_1,P_2)(P0​,P1​,P2​)内，我们需要执行如下过程（注意顶点顺序）： 计算P1P2×P1PQP_1P_2×P_1P_QP1​P2​×P1​PQ​，如果得到的向量的zzz值为正，则QQQ在向量P1P2P_1P_2P1​P2​的左侧，否则在右侧 计算P0P1×P0PQP_0P_1×P_0P_QP0​P1​×P0​PQ​，如果得到的向量的zzz值为正，则QQQ在向量P0P1P_0P_1P0​P1​的左侧，否则在右侧 计算P2P0×P2PQP_2P_0×P_2P_QP2​P0​×P2​PQ​，如果得到的向量的zzz值为正，则QQQ在向量P2P0P_2P_0P2​P0​的左侧，否则在右侧 当三个叉积向量全部为正或负（同号）时，该点在三角形内部。 这里可能会有一种情况，刚好点在三角形边界上时该如何判断？要么不做处理，要么特殊处理。即点到底在三角形111还是三角形222上，可以自己定义标准，可以定义点既在111又在222上，也可以定义点不在这两个三角形上。 上面我们提到了通过采样来进行光栅化的过程，那么我们想，我们已经写了一个二重循环，也就是说考虑一个三角形的光栅化就需要将所有的像素都跑一遍，其实是没必要。一个三角形其实只能覆盖一个相对较小的区域，比如图中左边第一列根本不在蓝色的区域，就不可能碰到三角形，也就根本不用去考虑这些像素。蓝色的区域我们称为三角形的包围盒（Bounding Box），更严格地说是一个轴向包围盒（Axis Aligned Bounding Box，AABB）。求三角形的包围盒非常简单，只需要使用三个点最小和最大的xxx值和yyy值来构造一个区域。这样只有当这个三角形很窄长并且旋转45度左右的时候，才会用一个很大的Bounding Box来包住 还有一种方法是将三角形覆盖的区域中，每一行都去找它的最左和最右，这样的话一个多余的像素都不会多考虑。 经过以上操作，我们得到了如图所示结果，我们会发现这个结果不太对，因为它有锯齿(Jaggies)，这是因为我们的采样率对信号来说是不够高的，所以产生了走样（Aliasing）。因此我们要使用抗锯齿（也称反走样）技术来进一步优化。 经过以上操作，我们得到了如图所示结果，我们会发现这个结果不太对，因为它有锯齿(Jaggies)，这是因为我们的采样率对信号来说是不够高的，所以产生了走样（Aliasing）。因此我们要使用抗锯齿（也称反走样）技术来进一步优化。 反走样 (Antialiasing) 我们用每一个像素的中心去检测是否在三角形内，然后把对应的像素涂上颜色，我们最后绘制出来的图案就是带有锯齿的图像。事实上我们不希望有锯齿，因此我们需要抗锯齿和反走样。锯齿的学名叫走样（Aliasing），反走样就称为Antialiasing。 采样理论 前面有提到，采样是在图形学中广泛存在的一个做法。光栅化的过程其实就是在屏幕空间离散的点上进行是否在三角形内的采样。对于任何一个我们拍出来的照片，放大后就会发现很多格子，也就是像素，这也是我们表示图像的基本方法。一副照片其实就是所有到达感光元件的光学信息，通过把它离散成图像像素的过程，其实也是采样。采样不光可以发生在不同的位置，也可以发生在不同的时间，视频就是在时间中进行采样。因此采样是广泛存在的，同样，采样所产生的问题也是广泛存在的。 Sampling Artifacts in Computer Graphics 采样所产生的错误（或称瑕疵）称为Artifacts。采样产生的第一个问题就是锯齿问题，如图所示。 然后是摩尔纹问题。如果我们把下左图中的奇数行和奇数列的像素都去掉，然后再重新对在一起，再显示原图大小，就会出现摩尔纹效果。当我们拿手机去拍显示器的屏幕也会看到类似的效果，这些都是采样带来的问题。 接着是车轮效应，当我们顺时针旋转纸片，如图所示，会有些条纹显得像是在逆时针旋转。在生活中也经常看到类似的现象，比如高速行驶的汽车，其轮子看上去在反向旋转，这是因为人眼在时间中的采样跟不上运动速度所造成的。 因为采样所产生的问题有： 锯齿问题 (Jaggies) - sampling in space 摩尔纹问题 (Moire) - undersampling images 车轮效应 (Wagon wheel effect) - sampling in time Many more... 采样产生问题的本质其实是信号变化过快导致采样速度跟不上。因此我们要通过频率来分析它。 Antialiasing Idea: Blurring (Pre-Filtering) Before Sampling 如何进行反走样，答案是在采样之前先做模糊（或滤波）。之前我们采样会发现，有些点完全在三角形内，有些点完全在三角形外，因此这些点要么是红的要么是白的。 但是如果我们在拿到一个三角形后，先做一个模糊操作，把它变成一个模糊的三角形，然后再去用像素中心点采样，这样就可以解决锯齿问题，得到下图效果。 举个例子，上图中，左图为抗锯齿操作之前，右图为抗锯齿操作之后。但是，先采样再模糊是不行的。下图是效果对比：左图是先采样再模糊，右图是先模糊再走样。先采样再模糊的操作称为Blurred Aliasing，先模糊再采样的操作称为反走样 Antialiasing 那么，为什么采样速度跟不上信号变化的速度就会产生走样？为什么要先做采样再做模糊达不到发走样的效果呢？为了弄清楚这些事我们需要了解**频域（Frequency Domain）**方面的知识 Frequency Domain 我们来看最简单的波：sines and cosines。通过调整xxx前面的系数，例如2πx2\\pi x2πx和4πx4 \\pi x4πx，我们能得到不同的余弦波，它们的不同在于频率不同，我们定义cos2πfxcos2\\pi fxcos2πfx，其中fff为频率，因此cos4πxcos4\\pi xcos4πx的频率f=2f=2f=2，因此我们可以用频率fff来定义余弦波的变化有多快。同样道理，我们还可以定义它的周期f=1Tf=\\frac{1}{T}f=T1​，通俗地讲，所谓周期就是每隔多少xxx，函数会重复自己一次，例如cos2πxcos2\\pi xcos2πx每隔111会重复一次，cos4πxcos4\\pi xcos4πx每隔0.50.50.5会重复一次。因此周期是频率的倒数。 那么为什么我们要介绍周期和频率呢？因为傅里叶变换（Fourier Transform）会用到它们。微积分里面有一个概念是函数展开，其中一个就是傅里叶级数展开。 傅里叶级数展开就是说，对于任何一个周期函数，都可以把它写成一系列正弦和余弦函数的线性组合以及一个常数项。 这样我们就可以把一个函数描述成很多不同的正弦余弦项的和。这说明傅里叶级数展开和傅里叶变换是紧密相连的。给定任何一个函数，我都可以经过一个复杂的操作变成另外一个函数，也可以把变换后的函数通过逆变换变回原来的函数，这样的操作就称为傅里叶变换和逆傅里叶变换。 不同的函数都有着不同的频率，仔细上图傅里叶级数展开的中ω\\omegaω的系数分别为3t,5t,7t,...3t,5t,7t,...3t,5t,7t,...这些值都代表了不同的频率，也就是说通过傅里叶级数展开可以将任何一个周期性函数分解为不同的频率。所谓傅里叶变换其实就是把函数变成不同的频率的段，并且把这些不同频率的段显示出来。 举个例子，如上图所示，通过傅里叶变换我们得到了这555个不同的函数，我们知道这些函数都含有不同的频率，假设我们对这些函数进行一次相同间隔的采样我们会发现如下图所示结果 从f1f_1f1​到f5f_5f5​会发现，f5f_5f5​的采样效果非常差。这也就是说通过频率分析，我们可以体会到，对于一个函数来说，它本身有一定的频率，采样也有一定的频率，但是如果我们采样的频率很低而函数本身的频率很高，就会跟不上它的变化，就没有办法将原始的信号拟合出来。 再看一个例子，蓝色线是函数本身，黑色线为采样得到的函数。现在如果我们把黑色线看作是另一函数，我们发现，如果我们用同样的一个采样方法采样两种截然不同的信号，我们会得到完全相同的结果。即采样频率相差很多的蓝色的函数和黑色的函数所得到的结果完全相同。这一现象就称为“走样”。 Filtering 现在我们知道了走样的定义，也知道了傅里叶变换，那么就可以开始真正的分析一些函数倒底拥有怎样的频率。这里有一个重要的概念——滤波（Filtering），它就是把某个特定的频段去除。傅里叶变换可以帮助我们理解这个问题，它可以将函数从实域变到频域。如下图所示，左图是一个人的图像，也就是实域，我们将其作傅里叶变换，得到右图为频域。 那么右边这幅图应该如何理解呢？中心是左边这张图最低频的区域，四周为高频区域，也就是说从中心到四周，其频率会越来越高。其亮度表达了左图在该频域的信息量。这张图就说明左图大多数的信息都是集中在低频上的，高频的信息相对于低频会少很多，实际上自然中所有的图片基本都是这样的。有人可能会问为什么图片中有一个十字，水平一条线、竖直一条线很明显。简单地说，这是因为在分析一个信号时它是一个周期重复的信号，对于没有周期重复的信号的话就将无限多个图像叠放，因为很少有图像左边界和右边界完全一致，不一致的情况下，我们把这个图像的左侧放到它的右侧，就会产生一个极高的高频，傅里叶变换就会产生这两条线，为了分析，我们暂时忽略这两条线。也就是说，傅里叶变换能够让我们看到这个图像在各个不同的频率的样子，我们称之为频谱。 接着，如果我们使用滤波，去掉频域中心的内容，也就是去掉低频的内容，保留高频的区域，然后我们再使用逆傅里叶变换得到实域，就会得到如图所示结果。从图中我们可以看到，高频信息表示的其实就是图像内容上的边界。这样的滤波称为高通滤波（High-pass filter）。那么通常我们所说的边界其实就是指图像上的某一区域的周围发生了剧烈的变换，也就是所谓的边界，比如人的衣服和背景之间的色差，这样的信息就是高频信息，那么如果我们只显示高频的信息，得到的就是图片中各物体的边界。 再反过来，将高频信息全部去除，只保留低频信息，得到如下图所示结果。我们会发现得到了一张相对模糊的图，绝大部分细节被去除了。这里就是应用了低通滤波器（Low-pass filter）。边界被去除了就会变得模糊。 接下来，我们将高频和最低频信息都去除，保留中间频率的信息，得到下图所示结果。我们得到了不是很明显的边界特征，因为最明显的边界特征对应最高的频率。 更多关于这方面的信息可以去了解一门课，叫《数字图像处理》，图形学中就不再过多赘述了。这是一个经典操作，现在更多的操作是通过机器学习来完成的。 Convolution Theorem 我们说滤波其实就是去掉特定频率的信息，从另外一个角度上看，滤波又等于卷积，或者平均，Filtering = Convolution ( = Averaging)。平均的概念比较好理解，低通滤波器就是将图片模糊，也就是一种平均操作。然后卷积是什么呢？ 如上图所示，对于一系列信号数组，滤波器就好像一个窗口，它可以左右移动，窗口的大小是3个格子，这其实是要做一个**卷积（Convolution）**操作。这是在说，在移动这个窗口的时候，窗口三个数所对应的信号的三个数做一个点积操作，如下图所示，最后将结果写回这个格子。对所有格子做这样的操作，就会得到新的数组。注意这里的卷积定义不是数学上的定义，是图形学简化的定义。 卷积理论：实域上如果要对两个信号进行卷积，其实，对应到两个信号各自的频域上，就是两个信号的频域的乘积。在实域上如果是对两个信号进行卷积，那就是频域上对其信号的乘积。通过这个理论我们可以直接对图进行一个卷积操作，也可以将这幅图先用傅里叶变换，变换到频域上，再把卷积的滤波器变到频域上，两者相乘，得到频域上的结果，再把它逆傅里叶变换变回实域，这两个操作是一样的。 我们对图片进行一个平均或者是卷积操作，即对任何一个像素，取它周围3×33×33×3个像素的平均，那么得到的结果就是一个模糊的图像。我也可以使用傅里叶变换的方法来做这样的操作，最后通过逆傅里叶变换变回这幅图。实域卷积=频域乘积，频域卷积=实域乘积。 再举一个例子，下左图为实域，右图为频域，如果左图中的方形变大了，频域会如何变换呢？ 答案如下图所示，变小了。因为我们为了模糊一张图，用了一个3×33×33×3的卷积操作，如果我们换成一个21×2121×2121×21的方形做卷积，那么得到的结果会越来越模糊，也就更接近于低频。如果我们用一个很小的方形做卷积，那么对应频域的范围就会很大，也就能留下更高的频域，最后模糊的效果就会不明显。 从频率的角度看采样 下面我们再从频率的角度来看什么是采样。采样就是在重复频率或者说频域上的内容Sampling = Repeating Frequency Contents。 如上图所示，图aaa为某个连续的函数，其频域为图bbb，假设我们要采样这个函数，就要取一系列离散的点，留下这些离散点的函数值。也就是让函数aaa去乘另外一个函数，这个函数只在一些固定的位置有值，其他地方没有值，也就是图ccc，这样的函数称为冲击函数。我们令aaa函数乘ccc函数，得到的就是图eee函数，这就是采样。也就是给定一个原始函数aaa，我们乘上一个冲击函数就可以得到采样结果。在频域上，冲击函数的频域还是冲击函数，只不过间隔有所变化。我们记得实域的乘积对应到频域上是卷积，即bbb卷积ddd，最后的结果就是将bbb重复了很多次。那也就是说，采样就是在重复原始信号的频谱。 所以，如果我们采样地不够快，原始信号重复的间隔就会非常小，意味着采样之间的距离很大，采样的越稀疏，频谱就越密集，频谱越密集，采样就越稀疏。如果频域上重复的时候叠在一起了，就发生了走样现象。所谓走样在频率图像上发生了混叠。 图形学中的反走样 增加采样率是终极解决办法，直接更换硬件来提高分辨率，例如用视网膜显示器来看图形，高分辨率意味着采样率高，意味着频谱与频谱之间的搬移间隔大，就不容易出现频谱的混叠。但是这并不是反走样要做的事情，反走样不会增加分辨率。 反走样操作：先模糊再采样。通过前面的讲解，我们知道先模糊再采样这一操作是有意义的。模糊就是用低通滤波将高频信息拿掉，然后再采样。 为什么这样是正确的？我们再看一个例子，一个函数的频谱就是如图所示的梯形，稀疏的采样就会发生频谱的混叠，产生走样。如果我们先做一个模糊，也就是把高频信号砍掉，也就是去掉虚线方块之外的信息，剩下的就是下图的信号。然后再采样，就不会发生混叠了。 在实际的操作中，我们用什么样的方法来对图像进行模糊呢？用一个一定大小的低通滤波器来进行卷积。最简单的块就是一个像素，一个像素就是一个box filter。因此我们的解决方案为： 原本的函数f(x,y)f(x,y)f(x,y)，判定了一个三角形要么在三角形里要么在三角形外，这就是一个二值函数，我们用每一个像素对它进行一个卷积操作，也就是求一个平均，然后用平均值的中心采样（其实不需要采样，因为就一个像素对应一个box filter，就只有一个值） 对于任何一个像素，我们都能知道它可能被覆盖，我们对它的覆盖面积求平均，然后根据覆盖得多少赋值。 Antialiasing By Supersampling (MSAA) 那么我们如何把一个像素被三角形覆盖的区域算出来呢？实现并不容易，因此人们研究出一种近似算法，称为Antialiasing By Supersampling (MSAA)，它是一个反走样的近似，并不能严格意义地解决反走样的问题。 算法是将每一个像素划分成许多小像素，每一个小像素有一个中心，我们可以判断这些点在三角形内，再将这些点平均起来，如果点足够多就能得到比较好的结果。其实就是在一个像素内部增加采样点。例如一个像素里，如果444个采样点都不在三角形内，就判定它不在三角形内，如果有一个采样点在三角形内，覆盖率就为25%25\\%25%，如果三个点在，覆盖率就为75%75\\%75%。通过这样的方法就能实现抗锯齿的效果。 因此MSAA解决的是对信号的模糊的操作，而下一步采样的操作被隐含在这一步里了。MSAA不是靠提升分辨率和采样率来直接解决走样问题。 通过MSAA，我们得到了一个不错的反走样效果，但是为了引入MSAA，我们牺牲了什么？很显然，我们使用了更多的点来测试是否在三角形内，也就是增大了计算量。因为这些点只是为了检测三角形的覆盖而已，却要付出很多倍的计算量。为了减少计算量，在工业界上不会规则地划分成均匀的444个点，而会用一些分布不均的点来使很多样本得到复用。 FXAA和TAA 其他的具有里程碑意义的方法有：FXAA (Fast Approximate AA) 和TAA (Temporal AA)。FXAA是图像的后期处理方法，先把有锯齿的图得到，再通过一种操作把锯齿消掉。但是如果先得到锯齿的图，再做模糊操作是不对的，FXAA是通过图像匹配的方法找到边界然后替换成没有锯齿的边界，效率非常高。TAA方法是最近几年兴起的方法，它可以寻找上一帧的信息，用像素内的一个点没有做MSAA来感知，假如我们看到的是静止的场景，相邻两帧看到的信息一样，那么我们可以用不同位置上的点来感知覆盖信息，那么大家会发现在时间范围内，得到的边界会各不相同。TAA是复用上一帧得到的像素的值，相当于将MSAA的样本分布在时间上，这样就没有引入任何额外的操作，这是一个非常聪明的作法，对于运动物体，我们会在后面的实时光线追踪继续讲解。 超分辨率问题 如果我们有一张512×512512×512512×512的图，我们想将其放大至1024×10241024×10241024×1024，但是我们又不想看到锯齿，就也要解决样本不足的问题，因此超分辨率问题和反走样问题是非常相似的。有效的做法是DLSS（Deep Learning Super Sampling），利用深度学习来解决。 ","link":"https://albertlidesign.github.io/post/games101_2_rasterization/"},{"title":"Games101(1): Transformation","content":"课程链接：GAMES101-现代计算机图形学入门-闫令琪 课程讲师：闫令琪 本系列笔记为本人根据学习该门课程的笔记，仅分享出来供大家交流，希望大家多多支持GAMES相关讲座及课程，如涉及侵权请联系我删除：albertlidesign@gmail.com 二维变换 Scale 在图形学中Scale变换是非常简单的，如果你想把一个物体Scale至它的sss倍，那么只需要将这个物体上所有点的分量都乘以sss，写成矩阵形式就是 [x′y′]=[s00s][xy]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\end{matrix}\\right] = \\left[\\begin{matrix}s &amp; 0\\\\ 0&amp; s\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right][x′y′​]=[s0​0s​][xy​] Reflection 将一个物体的镜像也很简单，在二维中，这个物体想沿着哪个轴镜像就将另一个轴的分量乘以−1-1−1即可，即 [x′y′]=[−1001][xy]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\end{matrix}\\right] = \\left[\\begin{matrix}-1 &amp; 0\\\\ 0&amp; 1\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right][x′y′​]=[−10​01​][xy​] Shear 在做切变的时候要注意，假设xyxyxy平面上左下角过原点的边长为111的正方形上方的两个点沿xxx轴平移aaa个单位，那么我们会发现： (1) y=0y=0y=0上的点没有移动 (2) y=1y=1y=1上的点移动了aaa个单位，如点(0,1)(0,1)(0,1)移动至了(a,1)(a,1)(a,1) (3) 垂直方向没有移动 (4) 如果假设y=0.5y=0.5y=0.5上有一个点(0,0.5)(0,0.5)(0,0.5)，那么它应该移动至(a2,0.5)(\\frac{a}{2},0.5)(2a​,0.5) 那么也就是说，实际上所有的yyy都没变，而所有的xxx都变为x+ayx+ayx+ay 因此用矩阵表示如下： [x′y′]=[1a01][xy]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\end{matrix}\\right] = \\left[\\begin{matrix}1 &amp; a\\\\ 0&amp; 1\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right][x′y′​]=[10​a1​][xy​] Rotate 首先规定，任何时候我们说旋转都是默认绕着原点(0,0)(0,0)(0,0)进行旋转（绕其他点旋转可以看作是先将物体移动至原点，进行旋转操作后再移动回去），另外，如果不规定旋转方向，那么我们默认都是逆时针旋转。现有一个物体，假设让它旋转θ\\thetaθ度，那么可以根据三角函数来求出旋转后的对应点的坐标。例如点(1,0)(1,0)(1,0)将会旋转至点(cosθ,sinθ)(cos\\theta,sin\\theta)(cosθ,sinθ)，点(0,1)(0,1)(0,1)将会旋转至点(−sinθ,cosθ)(-sin\\theta,cos\\theta)(−sinθ,cosθ)。 这样我们就能很轻易地写成矩阵形式： Rθ=[cosθ−sinθsinθcosθ]R_\\theta = \\left[\\begin{matrix}cos\\theta &amp; -sin\\theta\\\\ sin\\theta&amp; cos\\theta\\end{matrix}\\right]Rθ​=[cosθsinθ​−sinθcosθ​] 那么如果向顺时针方向旋转该怎么表达呢？顺时针方向旋转其实就是旋转了−θ-\\theta−θ，将其代如旋转矩阵得到 R−θ=[cosθsinθ−sinθcosθ]R_{-\\theta} = \\left[\\begin{matrix}cos\\theta &amp; sin\\theta\\\\ -sin\\theta&amp; cos\\theta\\end{matrix}\\right]R−θ​=[cosθ−sinθ​sinθcosθ​] 这里我们发现刚好R−θ=RθTR_{-\\theta} = R_{\\theta}^TR−θ​=RθT​，并且，旋转θ\\thetaθ角和旋转−θ-\\theta−θ角正好是互逆的操作，因此还有R−θ=Rθ−1R_{-\\theta} = R_\\theta^{-1}R−θ​=Rθ−1​（逆变换的意义其实就是将矩阵变换的操作反过来，下文会继续提到），因此Rθ−1=RθTR_\\theta^{-1} = R_\\theta^TRθ−1​=RθT​。这是因为旋转矩阵是一个正交矩阵。 求变换矩阵的方法 在变换中，无非就是将点(x,y)→(x′,y′)(x,y)\\rightarrow(x&#x27;,y&#x27;)(x,y)→(x′,y′)，表示成矩阵形式就是 [x′y′]=[abcd][xy]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\end{matrix}\\right] = \\left[\\begin{matrix}a &amp; b\\\\ c&amp; d\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right][x′y′​]=[ac​bd​][xy​] 即 x′=Mxx&#x27; = Mxx′=Mx 我们所做的就是求a,b,c,da,b,c,da,b,c,d，那么既然一个变换矩阵会对这个物体的所有点都起作用，那么也一定对一些特殊点起作用，那么我们就可以利用几个简单的特殊点来进行问题的求解，例如上面旋转矩阵的例子中，有(1,0)→(cosθ,sinθ)(1,0)\\rightarrow(cos\\theta,sin\\theta)(1,0)→(cosθ,sinθ)和(0,1)→(sinθ,cosθ)(0,1)\\rightarrow(sin\\theta,cos\\theta)(0,1)→(sinθ,cosθ) 用矩阵表示就是 [cosθsinθ]=[abcd][10][−sinθcosθ]=[abcd][01]\\left[\\begin{matrix}cos\\theta\\\\sin\\theta\\end{matrix}\\right] = \\left[\\begin{matrix}a &amp; b\\\\ c&amp; d\\end{matrix}\\right]\\left[\\begin{matrix}1\\\\0\\end{matrix}\\right]\\\\\\left[\\begin{matrix}-sin\\theta\\\\cos\\theta\\end{matrix}\\right] = \\left[\\begin{matrix}a &amp; b\\\\ c&amp; d\\end{matrix}\\right]\\left[\\begin{matrix}0\\\\1\\end{matrix}\\right][cosθsinθ​]=[ac​bd​][10​][−sinθcosθ​]=[ac​bd​][01​] 通过第一个矩阵等式我们直接就能求出aaa和ccc，再代入第二个矩阵求出bbb和ddd即可，最终我们就能求得旋转矩阵为 Rθ=[cosθ−sinθsinθcosθ]R_\\theta = \\left[\\begin{matrix}cos\\theta &amp; -sin\\theta\\\\ sin\\theta&amp; cos\\theta\\end{matrix}\\right]Rθ​=[cosθsinθ​−sinθcosθ​] 这也启发了我们，如果想求一个变换矩阵，只需要将变换前后的矩阵列出来，再代入特殊点求解即可。 Translation 平移操作我们可以很简单地将一个点操作前后的坐标写出来，即 x′=x+txy′=y+tyx&#x27;=x+t_x \\\\ y&#x27;=y+t_yx′=x+tx​y′=y+ty​ 但是我们会发现，我们不能将其表达成两个矩阵相乘的形式，这个操作的矩阵形式为 [x′y′]=[abcd][xy]+[txty]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\end{matrix}\\right] = \\left[\\begin{matrix}a &amp; b\\\\ c&amp; d\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right]+\\left[\\begin{matrix}t_x\\\\t_y\\end{matrix}\\right][x′y′​]=[ac​bd​][xy​]+[tx​ty​​] 这样一来，这里的变换就不是线性变换了，它就只能是一种特殊的变换了。 齐次坐标 在发现这件事之后人们就开始思考，有没有一种方法能将它表达成线性变换？答案是有，这就引入了齐次坐标(Homogeneous Coordinates) 的概念。人们引入了一种新的形式来表示物体的坐标，他们在二维坐标后面又加了一个分量www，规定 Point(2D)=(x,y,1)TVector(2D)=(x,y,0)TPoint(2D) = (x,y,1)^T \\\\Vector(2D) = (x,y,0)^TPoint(2D)=(x,y,1)TVector(2D)=(x,y,0)T 这样，平移的变换就可以写成线性变换形式，即 [x′y′w′]=[10tx01ty001][xy1]=[x+txy+ty1]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\\\w&#x27;\\end{matrix}\\right] = \\left[\\begin{matrix}1 &amp; 0 &amp; t_x\\\\ 0&amp; 1 &amp; t_y \\\\ 0 &amp; 0 &amp;1\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\\\1\\end{matrix}\\right]=\\left[\\begin{matrix}x+t_x\\\\y+t_y\\\\1\\end{matrix}\\right]⎣⎡​x′y′w′​⎦⎤​=⎣⎡​100​010​tx​ty​1​⎦⎤​⎣⎡​xy1​⎦⎤​=⎣⎡​x+tx​y+ty​1​⎦⎤​ 为什么在二维点的后面增加了111而在二维向量的后面增加了000呢？其实是有意义的。因为我们知道，在空间里，两向量和必为一个新的向量，如果坐标最后是000，那么相加后最后还是000；如果空间中的点，如果一个点减一个点，这样就形成了一个向量（末点-初点），我们发现最后的坐标是111时相减得到000，变成了一个向量，刚好也满足；一个点加一个向量表示为一个点沿着一个方向移动，移动到了一个新的点上，最后得到的还是一个点，最后的分量还是111，也可验证。因此第三个分量的引入，在点上加一个111，在向量上加一个000，保证了这些操作最后的结果是对的。 vector+vector=vectorpoint−point=vectorpoint+vector=pointpoint+point=??vector + vector = vector\\\\ point-point=vector\\\\point+vector = point\\\\point+point =??vector+vector=vectorpoint−point=vectorpoint+vector=pointpoint+point=?? 那么最后一个，两点相加后，最后的分量是222，这是什么意思呢？人们也扩充了它的定义，即齐次坐标[xyw]\\left[\\begin{matrix}x\\\\y\\\\w\\end{matrix}\\right]⎣⎡​xyw​⎦⎤​表示的是二维点[x/wy/w1],w=0\\left[\\begin{matrix}x/w\\\\y/w\\\\1\\end{matrix}\\right],w\\not =0⎣⎡​x/wy/w1​⎦⎤​,w​=0。因此两点相加表达的是它们的中点。 在引入齐次坐标之前，我们做平移、旋转等操作可以起个名字叫做仿射变换(Affine Transformation)，仿射(Affine map) = linear map + translation，即 [x′y′]=[abcd][xy]+[txty]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\end{matrix}\\right] = \\left[\\begin{matrix}a &amp; b\\\\ c&amp; d\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\end{matrix}\\right]+\\left[\\begin{matrix}t_x\\\\t_y\\end{matrix}\\right][x′y′​]=[ac​bd​][xy​]+[tx​ty​​] 使用齐次坐标来表达就可以写成 [x′y′1]=[abtxcdty001][xy1]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\\\1\\end{matrix}\\right] = \\left[\\begin{matrix}a &amp; b &amp; t_x\\\\ c&amp; d &amp; t_y \\\\ 0 &amp; 0 &amp;1\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\\\1\\end{matrix}\\right]⎣⎡​x′y′1​⎦⎤​=⎣⎡​ac0​bd0​tx​ty​1​⎦⎤​⎣⎡​xy1​⎦⎤​ 下面利用齐次坐标来重新书写我们前面所学的各种变换 Scale:S(sx,sy)=[sx000sy0001]Scale: S(s_x,s_y) = \\left[\\begin{matrix}s_x &amp; 0 &amp; 0\\\\ 0&amp; s_y &amp; 0\\\\0 &amp; 0 &amp;1\\end{matrix}\\right]Scale:S(sx​,sy​)=⎣⎡​sx​00​0sy​0​001​⎦⎤​ Rotation:R(α)=[cosα−sinα0sinαcosα0001]Rotation: R(\\alpha) = \\left[\\begin{matrix}cos\\alpha &amp; -sin\\alpha &amp; 0\\\\ sin\\alpha&amp; cos\\alpha &amp; 0\\\\0 &amp; 0 &amp;1\\end{matrix}\\right]Rotation:R(α)=⎣⎡​cosαsinα0​−sinαcosα0​001​⎦⎤​ Translation:T(tx,ty)=[10tx01ty001]Translation: T(t_x,t_y) = \\left[\\begin{matrix}1 &amp; 0 &amp; t_x\\\\ 0&amp; 1 &amp; t_y\\\\0 &amp; 0 &amp;1\\end{matrix}\\right]Translation:T(tx​,ty​)=⎣⎡​100​010​tx​ty​1​⎦⎤​ 逆变换和组合变换 逆变换 逆变换的意义其实就是将矩阵变换的操作反过来，例如矩阵MMM通过旋转矩阵RRR得到M′M&#x27;M′，那么如果我们已知变换后的矩阵M′M&#x27;M′想求得MMM只需要左乘旋转矩阵RRR的逆R−1R^{-1}R−1，即 M′=RMM=R−1M′M&#x27; = RM \\\\ M = R^{-1}M&#x27;M′=RMM=R−1M′ 组合变换 复杂的变换可以通过多个变换组合来得到，其中，变换顺序至关重要，因为矩阵相乘不满足交换律。做一次变换可以理解为左乘一个矩阵，当然我们也可以将多次变换的矩阵的作用效果看作是一个矩阵的作用效果，因为矩阵相乘满足结合律。 那么如图所示，我们想求左侧矩阵变换到最右侧矩阵的效果，需要先旋转再平移，那么我们可以写成 M′=T(1,0)R45MM&#x27; = T_{(1,0)}R_{45}MM′=T(1,0)​R45​M 对于一个起点不在原点的物体做宣传操作该如何做呢？我们可以先把物体移动到原点，做旋转操作后再移动回原来的位置，即 M′=T(c)RαT(−c)MM&#x27; = T_{(c)}R_{\\alpha}T_{(-c)}MM′=T(c)​Rα​T(−c)​M 三维变换 了解了二维变换之后，三维就变得很简单了，首先我们依然引入齐次坐标，得到三维点和三维向量： Point(3D)=(x,y,z,1)TVector(3D)=(x,y,z,0)TPoint(3D) = (x,y,z,1)^T \\\\Vector(3D) = (x,y,z,0)^TPoint(3D)=(x,y,z,1)TVector(3D)=(x,y,z,0)T 并且一般来说，齐次坐标[xyzw],w=0\\left[\\begin{matrix}x\\\\y\\\\z\\\\w\\end{matrix}\\right],w\\not =0⎣⎢⎢⎡​xyzw​⎦⎥⎥⎤​,w​=0表示的是三维点[x/wy/wz/w]\\left[\\begin{matrix}x/w\\\\y/w\\\\z/w\\end{matrix}\\right]⎣⎡​x/wy/wz/w​⎦⎤​。 当然，我们也可以用4×44×44×4的矩阵表达仿射变换，即 [x′y′z′1]=[abctxdeftyghitz0001][xyz1]\\left[\\begin{matrix}x&#x27;\\\\y&#x27;\\\\z&#x27;\\\\1\\end{matrix}\\right] = \\left[\\begin{matrix}a &amp; b &amp; c &amp; t_x\\\\ d&amp; e &amp;f &amp; t_y \\\\ g&amp;h&amp;i&amp;t_z\\\\0&amp; 0 &amp; 0 &amp;1\\end{matrix}\\right]\\left[\\begin{matrix}x\\\\y\\\\z\\\\1\\end{matrix}\\right]⎣⎢⎢⎡​x′y′z′1​⎦⎥⎥⎤​=⎣⎢⎢⎡​adg0​beh0​cfi0​tx​ty​tz​1​⎦⎥⎥⎤​⎣⎢⎢⎡​xyz1​⎦⎥⎥⎤​ 对应的三维中的各种变换表达如下： Scale:S(sx,sy,sz)=[sx0000sy0000sz00001]Scale: S(s_x,s_y,s_z) = \\left[\\begin{matrix}s_x &amp; 0 &amp; 0 &amp; 0\\\\ 0&amp; s_y &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; s_z &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]Scale:S(sx​,sy​,sz​)=⎣⎢⎢⎡​sx​000​0sy​00​00sz​0​0001​⎦⎥⎥⎤​ Translation:T(tx,ty,tz)=[100tx010ty001tz0001]Translation: T(t_x,t_y,t_z) = \\left[\\begin{matrix}1 &amp; 0 &amp; 0 &amp; t_x\\\\ 0&amp; 1 &amp; 0 &amp; t_y\\\\0 &amp; 0 &amp; 1 &amp; t_z \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]Translation:T(tx​,ty​,tz​)=⎣⎢⎢⎡​1000​0100​0010​tx​ty​tz​1​⎦⎥⎥⎤​ Rotation:Rx(α)=[10000cosα−sinα00sinαcosα00001]Rotation: R_x(\\alpha) = \\left[\\begin{matrix}1 &amp; 0 &amp; 0 &amp; 0\\\\ 0&amp; cos\\alpha &amp; -sin\\alpha &amp; 0\\\\ 0&amp; sin\\alpha&amp; cos\\alpha &amp; 0\\\\0 &amp;0&amp; 0 &amp;1\\end{matrix}\\right]Rotation:Rx​(α)=⎣⎢⎢⎡​1000​0cosαsinα0​0−sinαcosα0​0001​⎦⎥⎥⎤​ Rotation:Ry(α)=[cosα0sinα00100−sinα0cosα00001]Rotation: R_y(\\alpha) = \\left[\\begin{matrix}cos\\alpha &amp; 0 &amp; sin\\alpha &amp; 0\\\\ 0&amp; 1 &amp; 0 &amp; 0\\\\ -sin\\alpha &amp; 0 &amp; cos\\alpha &amp; 0\\\\0 &amp;0&amp; 0 &amp;1\\end{matrix}\\right]Rotation:Ry​(α)=⎣⎢⎢⎡​cosα0−sinα0​0100​sinα0cosα0​0001​⎦⎥⎥⎤​ Rotation:Rz(α)=[cosα−sinα00sinαcosα0000100001]Rotation: R_z(\\alpha) = \\left[\\begin{matrix}cos\\alpha &amp; -sin\\alpha &amp; 0 &amp; 0\\\\ sin\\alpha&amp; cos\\alpha &amp; 0 &amp; 0\\\\ 0&amp; 0&amp; 1 &amp; 0\\\\0 &amp;0&amp; 0 &amp;1\\end{matrix}\\right]Rotation:Rz​(α)=⎣⎢⎢⎡​cosαsinα00​−sinαcosα00​0010​0001​⎦⎥⎥⎤​ 注意，绕yyy轴旋转时，左下角变为−sinα-sin\\alpha−sinα，右上角变为sinαsin\\alphasinα，这是因为轴的顺序问题造成的，我们说轴的顺序是x→y→zx\\rightarrow y\\rightarrow zx→y→z，有x×y=zx×y =zx×y=z，y×z=xy×z=xy×z=x，但是根据右手定则，得到y=z×xy = z×xy=z×x，而不是x×zx×zx×z，因此这里是反的。 对于一般性旋转我们可以将其转化成三个轴的旋转，即 Rxyz(α,β,γ)=Rx(α)Ry(β)Rz(γ)R_{xyz}(\\alpha,\\beta,\\gamma) = R_x(\\alpha)R_y(\\beta)R_z(\\gamma)Rxyz​(α,β,γ)=Rx​(α)Ry​(β)Rz​(γ) 如果想绕着任意一个点旋转，我们可以先将物体沿着这个点平移至原点，做旋转操作后再移回该点。 如果想绕任意轴旋转，我们需要Rodrigues' Rotation Formula，给定角度α\\alphaα和轴nnn，我们有 R(n,α)=cos(α)I+(1−cos(α))nnT+sin(α)[0−nznynz0−nx−nynx0]R(n,\\alpha) = cos(\\alpha)I +(1-cos(\\alpha))nn^T+sin(\\alpha)\\left[\\begin{matrix}0 &amp; -n_z &amp; n_y\\\\ n_z &amp; 0 &amp; -n_x\\\\ -n_y &amp; n_x &amp;0\\end{matrix}\\right]R(n,α)=cos(α)I+(1−cos(α))nnT+sin(α)⎣⎡​0nz​−ny​​−nz​0nx​​ny​−nx​0​⎦⎤​ 观测变换 (Viewing Transformation) 现实生活中，拍照片我们需要如下步骤： (1) 找一个好的场地，集合所有人 (模型变换 Model Transformation) (2) 找一个好的角度，放置相机 (视图变换 View Transformation) (3) 拍照 (投影变换 Projection Transformation) 1. 定义相机 相机的位置 e⃗\\vec ee 拍摄方向 (look-at / gaze direction) g^\\hat gg^​ 向上方向 (up direction) t^\\hat tt^ （垂直于拍摄方向） 2.视图变换(View Transformation) 在现实生活中，假如在摄影棚里拍照，相同的人，相同的相机，相同的相对摆放位置，不管在哪一个摄影棚，拍出来的效果是一样的。也就是说，如果相机和所有物体（包括前景背景）都一起移动时，拍出来的照片一定是一样的。更抽象地说，当我们移动物体和移动相机没有相对运动时，拍出来的照片是一样的。那么我们可以将相机永远放在原点这个固定的位置上，物体都可以移动，相机永远不动，并且相机的向上方向为YYY方向，看向−Z-Z−Z。 通过变换将相机放到标准位置上 首先，相机原本在位置e⃗\\vec ee，向g^\\hat gg^​看，并且向上方向为t^\\hat tt^，现在要把它变成固定在原点，向−Z-Z−Z方向看，并且up方向为YYY。那么我们可以先将相机从e⃗\\vec ee移到原点，然后再把观原点察方向g^\\hat gg^​旋转到−Z-Z−Z上，再把向上方向t^\\hat tt^旋转到YYY，写成矩阵表达为Mview=RviewTviewM_{view} = R_{view}T_{view}Mview​=Rview​Tview​ 其中，将相机从e⃗\\vec ee移到原点很容易写出，将向量e⃗\\vec ee的三个分量各减去他们本身即可，为 Tview=[100−xe010−ye001−ze0001]T_{view} = \\left[\\begin{matrix}1 &amp; 0 &amp; 0 &amp; -x_e\\\\ 0&amp; 1 &amp; 0 &amp; -y_e\\\\0 &amp; 0 &amp; 1 &amp; -z_e \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]Tview​=⎣⎢⎢⎡​1000​0100​0010​−xe​−ye​−ze​1​⎦⎥⎥⎤​ 但是如何把观原点察方向g^\\hat gg^​旋转到−Z-Z−Z上，再把向上方向t^\\hat tt^旋转到YYY呢？这件事并不容易做，但是反过来，将XXX旋转到(g^×t^)(\\hat g×\\hat t)(g^​×t^)，将YYY旋转到t^\\hat tt^，将ZZZ旋转到−g^-\\hat g−g^​很容易实现，它和我们需要做的操作是一个互逆的操作，因此我们只需要求这一操作的矩阵的逆即可。 Rview−1=[xg^×t^xt^x−g^0yg^×t^yt^y−g^0zg^×t^zt^z−g^00001]Rview=[xg^×t^yg^×t^zg^×t^0xt^yt^zt^0x−g^y−g^z−g^00001]R_{view}^{-1} = \\left[\\begin{matrix}x_{\\hat g×\\hat t} &amp; x_{\\hat t} &amp; x_{-\\hat g} &amp; 0\\\\ y_{\\hat g×\\hat t}&amp; y_{\\hat t} &amp; y_{-\\hat g} &amp; 0\\\\z_{\\hat g×\\hat t} &amp; z_{\\hat t} &amp; z_{-\\hat g} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]\\quad R_{view} = \\left[\\begin{matrix}x_{\\hat g×\\hat t} &amp; y_{\\hat g×\\hat t}&amp; z_{\\hat g×\\hat t} &amp; 0 \\\\ x_{\\hat t} &amp; y_{\\hat t} &amp; z_{\\hat t}&amp; 0\\\\ x_{-\\hat g} &amp; y_{-\\hat g} &amp; z_{-\\hat g} &amp; 0\\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]Rview−1​=⎣⎢⎢⎡​xg^​×t^​yg^​×t^​zg^​×t^​0​xt^​yt^​zt^​0​x−g^​​y−g^​​z−g^​​0​0001​⎦⎥⎥⎤​Rview​=⎣⎢⎢⎡​xg^​×t^​xt^​x−g^​​0​yg^​×t^​yt^​y−g^​​0​zg^​×t^​zt^​z−g^​​0​0001​⎦⎥⎥⎤​ 需要注意的是，为了保证相对结果不变，我们要将场景中的所有物体都做这样的变换。 3. 投影变换(Projection Transformation) 投影包含两种投影方式：正交投影 (Orthographic Projection) 和透视投影 (Perspective Projection) 正交投影 (Orthographic Projection) 正交投影很简单，不管物体的远近，我们只需将它“挤”到某个平面上即可。投影到XYXYXY平面的操作步骤如下： 先将相机放到标准位置上（原点，看向−Z-Z−Z，向上为YYY） 移除掉物体的ZZZ坐标 平移、缩放将结果映射到[−1,1]2[-1,1]^2[−1,1]2 (约定俗称的方法，方便后续计算) 上述方法是一个简单的理解方式，但在图形学中，还有更方便的一种操作： 首先定义空间中的立方体[l,r]×[b,t]×[f,n][l,r]×[b,t]×[f,n][l,r]×[b,t]×[f,n]，只需定义立方体的左右在XXX轴上的值，下上在YYY轴上的值和前后在ZZZ轴上的值（由于右手坐标系，远的值小于近），一共666个数 将这个方体映射到标准立方体[−1,1]3[-1,1]^3[−1,1]3，将立方体的中心移动到原点，在将模型缩放至[−1,1]3[-1,1]^3[−1,1]3 实现方法：首先通过平移，将立方体中心移动至原点，然后再缩放，以长宽高都缩放至2为例，则 Mortho=[2r−l00002t−b00002n−f00001][100−r+l2010−t+b2001−n+f20001]M_{ortho} = \\left[\\begin{matrix}\\frac{2}{r-l} &amp; 0 &amp; 0 &amp; 0\\\\ 0&amp; \\frac{2}{t-b} &amp; 0 &amp; 0\\\\0 &amp; 0 &amp; \\frac{2}{n-f} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix}1 &amp; 0 &amp; 0 &amp; -\\frac{r+l}{2}\\\\ 0&amp; 1 &amp; 0 &amp; -\\frac{t+b}{2}\\\\0 &amp; 0 &amp; 1 &amp; -\\frac{n+f}{2} \\\\ 0 &amp; 0 &amp; 0 &amp; 1\\end{matrix}\\right]Mortho​=⎣⎢⎢⎡​r−l2​000​0t−b2​00​00n−f2​0​0001​⎦⎥⎥⎤​⎣⎢⎢⎡​1000​0100​0010​−2r+l​−2t+b​−2n+f​1​⎦⎥⎥⎤​ 注意： 由于我们定义相机看向−Z-Z−Z，所以近&gt;远，这也是为什么在OpenGL中使用左手系，但左手系意味着X×Y=ZX×Y \\not =ZX×Y​=Z 也可以用变换坐标系的方式来理解，道理是一样的，但是不直观、不好理解。 透视投影 (Perspective Projection) 透视投影是应用最广泛的投影，满足近大远小的性质，带来的视觉效果是平行线不再平行，相交于一点。 回顾一下我们之前关于齐次坐标的定义： (x,y,z,1),(kx,ky,kz,k=0),(xz,yz,z2,z=0)(x,y,z,1),(kx,ky,kz,k\\not =0),(xz,yz,z^2,z \\not =0)(x,y,z,1),(kx,ky,kz,k​=0),(xz,yz,z2,z​=0)都表示三维空间中的一个相同点(x,y,z)(x,y,z)(x,y,z)，因为zzz也属于任何一个数kkk 举个例子：(1,0,0,1)(1,0,0,1)(1,0,0,1)和(2,0,0,2)(2,0,0,2)(2,0,0,2)都表示点(1,0,0)(1,0,0)(1,0,0) 透视投影是从一个点（相机）开始，往外延伸出的一个四棱锥，我们定义近平面nnn和远平面fff，称为Frustum，和正交投影的区别在于远平面fff相对更大，这也是透视投影和正交投影的主要区别。 因此我们只需要在正交投影之前增加一步，将远平面fff先挤压至与近平面nnn相同的尺寸。也就是说透视投影的过程为两个步骤：先将远平面挤压至近平面的尺寸，再进行正交投影(Mpersp→ortho(4×4)M_{persp\\rightarrow ortho}^{(4×4)}Mpersp→ortho(4×4)​）。在这一过程中，我们规定： 近平面nnn永远不变 远平面fff上的点的zzz值不会变（因为是在平面内挤压，zzz值不变） 远平面的中心点挤压前后不变 从侧面看Frustum会发现远近平面与相机有着相似三角形的关系，从yyy坐标来看就有y′=nzyy&#x27; = \\frac{n}{z}yy′=zn​y，因此写成矩阵形式就有 [xyz1]⇒[nx/zny/zunknown1]==[nxnyunknownz](同乘z后仍表示同一个点)\\left[\\begin{matrix}x\\\\y\\\\z\\\\1\\end{matrix}\\right] \\Rightarrow \\left[\\begin{matrix}nx/z\\\\ny/z\\\\unknown\\\\1\\end{matrix}\\right] == \\left[\\begin{matrix}nx\\\\ny\\\\unknown\\\\z\\end{matrix}\\right](同乘z后仍表示同一个点)⎣⎢⎢⎡​xyz1​⎦⎥⎥⎤​⇒⎣⎢⎢⎡​nx/zny/zunknown1​⎦⎥⎥⎤​==⎣⎢⎢⎡​nxnyunknownz​⎦⎥⎥⎤​(同乘z后仍表示同一个点) 那么我们从 Mpersp→ortho(4×4)[xyz1]=[nxnyunknownz]M_{persp\\rightarrow ortho}^{(4×4)} \\left[\\begin{matrix}x\\\\y\\\\z\\\\1\\end{matrix}\\right] = \\left[\\begin{matrix}nx\\\\ny\\\\unknown\\\\z\\end{matrix}\\right]Mpersp→ortho(4×4)​⎣⎢⎢⎡​xyz1​⎦⎥⎥⎤​=⎣⎢⎢⎡​nxnyunknownz​⎦⎥⎥⎤​ 可知 Mpersp→ortho(4×4)=[n0000n00？？？？0010]M_{persp\\rightarrow ortho}^{(4×4)} = \\left[\\begin{matrix}n&amp; 0 &amp; 0 &amp; 0\\\\0 &amp; n &amp; 0 &amp; 0\\\\？&amp; ？ &amp; ？ &amp; ？\\\\0&amp; 0 &amp; 1 &amp; 0\\end{matrix}\\right]Mpersp→ortho(4×4)​=⎣⎢⎢⎡​n0？0​0n？0​00？1​00？0​⎦⎥⎥⎤​ 为了求第三行，我们需要另外两个条件：任意在近平面上的点都保持不变，任意在远平面上的点的zzz坐标保持不变。 （1）在近平面上，点坐标的zzz值其实是nnn，代入 Mpersp→ortho(4×4)[xyz1]=[nxnyunknownz]M_{persp\\rightarrow ortho}^{(4×4)} \\left[\\begin{matrix}x\\\\y\\\\z\\\\1\\end{matrix}\\right] = \\left[\\begin{matrix}nx\\\\ny\\\\unknown\\\\z\\end{matrix}\\right]Mpersp→ortho(4×4)​⎣⎢⎢⎡​xyz1​⎦⎥⎥⎤​=⎣⎢⎢⎡​nxnyunknownz​⎦⎥⎥⎤​ 有 [xyn1]⇒[xyn1]==[nxnyn2n]\\left[\\begin{matrix}x\\\\y\\\\n\\\\1\\end{matrix}\\right] \\Rightarrow \\left[\\begin{matrix}x\\\\y\\\\n\\\\1\\end{matrix}\\right]== \\left[\\begin{matrix}nx\\\\ny\\\\n^2\\\\n\\end{matrix}\\right]⎣⎢⎢⎡​xyn1​⎦⎥⎥⎤​⇒⎣⎢⎢⎡​xyn1​⎦⎥⎥⎤​==⎣⎢⎢⎡​nxnyn2n​⎦⎥⎥⎤​ 第三行为n2n^2n2，那么我们可以求出Mpersp→ortho(4×4)M_{persp\\rightarrow ortho}^{(4×4)}Mpersp→ortho(4×4)​的第三行的前两项为000，即 [00AB][xyn1]=n2\\left[\\begin{matrix}0&amp; 0&amp;A&amp;B\\end{matrix}\\right] \\left[\\begin{matrix}x\\\\y\\\\n\\\\1\\end{matrix}\\right] = n^2[0​0​A​B​]⎣⎢⎢⎡​xyn1​⎦⎥⎥⎤​=n2 现在还剩下两个未知数AAA和BBB，接着我们再使用第二个条件：任意在远平面上的点的zzz坐标保持不变。 （2）在远平面上，点坐标的zzz值是fff，代入 [00f1]⇒[00f1]==[00f2f]（同乘f后仍表示同一个点）\\left[\\begin{matrix}0\\\\0\\\\f\\\\1\\end{matrix}\\right]\\Rightarrow \\left[\\begin{matrix}0\\\\0\\\\f\\\\1\\end{matrix}\\right] == \\left[\\begin{matrix}0\\\\0\\\\f^2\\\\f\\end{matrix}\\right]（同乘f后仍表示同一个点）⎣⎢⎢⎡​00f1​⎦⎥⎥⎤​⇒⎣⎢⎢⎡​00f1​⎦⎥⎥⎤​==⎣⎢⎢⎡​00f2f​⎦⎥⎥⎤​（同乘f后仍表示同一个点） 根据近平面我们得到的结果，将其展开得 [00AB][xyn1]=n2⇒An+B=n2\\left[\\begin{matrix}0&amp; 0&amp;A&amp;B\\end{matrix}\\right] \\left[\\begin{matrix}x\\\\y\\\\n\\\\1\\end{matrix}\\right] = n^2 \\Rightarrow An+B=n^2[0​0​A​B​]⎣⎢⎢⎡​xyn1​⎦⎥⎥⎤​=n2⇒An+B=n2 根据远平面我们得到的结果，将其展开得 [00f1]⇒[00f1]==[00f2f]⇒Af+B=f2\\left[\\begin{matrix}0\\\\0\\\\f\\\\1\\end{matrix}\\right]\\Rightarrow \\left[\\begin{matrix}0\\\\0\\\\f\\\\1\\end{matrix}\\right] == \\left[\\begin{matrix}0\\\\0\\\\f^2\\\\f\\end{matrix}\\right] \\Rightarrow Af+B=f^2⎣⎢⎢⎡​00f1​⎦⎥⎥⎤​⇒⎣⎢⎢⎡​00f1​⎦⎥⎥⎤​==⎣⎢⎢⎡​00f2f​⎦⎥⎥⎤​⇒Af+B=f2 于是将两个展开式联立 $An+B=n^2\\ Af+B=f^2 $ 得 A=n+fB=−nfA = n+f\\\\B=-nfA=n+fB=−nf 因此 Mpersp→ortho(4×4)=[n0000n0000n+f−nf0010]M_{persp\\rightarrow ortho}^{(4×4)} = \\left[\\begin{matrix}n&amp; 0 &amp; 0 &amp; 0\\\\0 &amp; n &amp; 0 &amp; 0\\\\ 0 &amp; 0 &amp; n+f &amp; -nf\\\\0&amp; 0 &amp; 1 &amp; 0\\end{matrix}\\right]Mpersp→ortho(4×4)​=⎣⎢⎢⎡​n000​0n00​00n+f1​00−nf0​⎦⎥⎥⎤​ 透视投影的矩阵变换为 Mpersp=MorthoMpersp→orthoM_{persp} = M_{ortho}M_{persp\\rightarrow ortho}Mpersp​=Mortho​Mpersp→ortho​ 视口变换 之前已经把如何将透视投影转化成正交投影说明白了，透视投影转化成正交投影需要保证近和远两个平面都是不变的，大小上远平面要变成和近平面一样大。在表示立方体上我们需要 左右前后远近(l,r,b,t,f,nl,r,b,t,f,nl,r,b,t,f,n) 666个值来表示立方体，既然远和近在正交投影和透视投影中都是一样的，就不用管它了，我们可以将Frustum变成一个长方体，问题在于我们如何定义这个Frustum？ 如图所示，我们从摄像机出发看向某一个区域，如果假设看到的就是这个近的平面，那么我们可以定义一个宽度和高度，就好像我们在看一个显示器一样，我们需要定义一个宽高比 (aspect ratio)，如4:3,16:94:3,16:94:3,16:9。我们还需要定义另外一个概念，称为field-of-view，即能看到的角度的范围。假如我们在看一个屏幕，我们可以分别从相机出发与屏幕顶边的中点和底边的中点连出两条红线，它们所形成的夹角就是垂直可视角度 (field-of-view Y, fovY)。因此，**定义一个视锥需要定义一个宽高比和垂直可视角度。**有些游戏里有水平可视角度，这个可以通过长宽比和垂直可视角度推出。 有了这两个概念，我们就可以将它们和之前定义的空间中的长方体转化成同一个概念。如图所示，从侧面来看这个视锥体可以看到一个三角形，如果我们取垂直可视角度的tangenttangenttangent，可以发现tanfovY2=t∣n∣tan\\frac{fovY}{2} = \\frac{t}{|n|}tan2fovY​=∣n∣t​，也就是说，如果我们知道这个近平面的距离，就能知道这个屏幕一半的高度是多少，屏幕的最高点对应的yyy值就是ttt，最低点对应的yyy值就是−t-t−t，也就是说，如果定义一个空间中的长方体，b=−tb=-tb=−t，根据宽高比aspect=rtaspect = \\frac{r}{t}aspect=tr​即可求出水平方向上两边中点的坐标。 作业0 /* 给定一个点 P=(2,1), 将该点绕原点先逆时针旋转 45◦，再平移 (1,2), 计算出 变换后点的坐标（要求用齐次坐标进行计算）。 */ #include &lt;iostream&gt; #include&lt;cmath&gt; #include &lt;Eigen/Dense&gt; #define _USE_MATH_DEFINES using namespace Eigen; int main() { // define a 2d point (2,1) Vector3f vec = Vector3f(2, 1, 1); std::cout &lt;&lt; &quot;The point is: \\n&quot; &lt;&lt; vec &lt;&lt; std::endl; // build a rotation matrix Matrix3f R45(3, 3); float theta = 45.0f / 180.0f * M_PI; R45 &lt;&lt; cos(theta), -sin(theta), 0, sin(theta), cos(theta), 0, 0, 0, 1; std::cout &lt;&lt; &quot;Rotation Matrix: \\n&quot; &lt;&lt; R45 &lt;&lt; std::endl; // build a translation matrix Matrix3f T12(3, 3); T12 &lt;&lt; 1, 0, 1, 0, 1, 2, 0, 0, 1; std::cout &lt;&lt; &quot;Translation Matrix: \\n&quot; &lt;&lt; T12 &lt;&lt; std::endl; // compute the result Vector3f result = T12 * R45 * vec; std::cout &lt;&lt; &quot;The result is: \\n&quot; &lt;&lt; result &lt;&lt; std::endl; } 作业1 /* 填写一个旋转矩阵和一个透视投影矩阵。给定三维下三个点 v0(2.0, 0.0, -2.0), v1(0.0, 2.0, -2.0), v2( 2.0, 0.0, 2.0), 你需要将这三个点的坐 标变换为屏幕坐标并在屏幕上绘制出对应的线框三角形 (在代码框架中，我们已 经提供了 draw_triangle 函数，所以你只需要去构建变换矩阵即可)。简而言之， 我们需要进行模型、视图、投影、视口等变换来将三角形显示在屏幕上。在提供 的代码框架中，我们留下了模型变换和投影变换的部分给你去完成。 get_model_matrix(float rotation_angle): 逐个元素地构建模型变换矩阵并返回该矩阵。 在此函数中，你只需要实现三维中绕 z 轴旋转的变换矩阵，而不用处理平移与缩放。 get_projection_matrix(float eye_fov, float aspect_ratio, float zNear, float zFar): 使用给定的参数逐个元素地构建透视投影矩阵并返回该矩阵。 [Optional] main(): 自行补充你所需的其他操作。 */ #include &lt;iostream&gt; #include&lt;cmath&gt; #include &lt;Eigen/Dense&gt; #define _USE_MATH_DEFINES using namespace Eigen; Matrix4f get_model_matrix(float rotation_angle); Matrix4f get_rotation(Vector3f axis, float angle); // build a rotation matrix about any axis through orgin Matrix4f get_projection_matrix(float eye_fov, float aspect_ratio, float zNear, float zFar); Matrix4f get_model_matrix(float rotation_angle) { // build a matrix rotating about the z-axis Matrix4f RZ(4, 4); float theta = rotation_angle / 180.0f * M_PI; RZ &lt;&lt; cos(theta), -sin(theta), 0.0f, 0.0f, sin(theta), cos(theta), 0.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f, 0.0f, 0.0f, 0.0f, 1.0f; return RZ; } // build a rotation matrix about any axis through orgin Matrix4f get_rotation(Vector3f axis, float angle) { Matrix3f N(3, 3); float theta = angle / 180.0f * M_PI; N &lt;&lt; 0.0f, -axis.z(), axis.y(), axis.z(), 0.0f, -axis.x(), -axis.y(), axis.x(), 0.0f; Matrix3f R = cos(theta) * Matrix3f::Identity(3,3) + (1 - cos(theta)) * axis * axis.transpose() + sin(theta) * N; Matrix4f RN(4, 4); RN &lt;&lt; R(0, 0), R(0, 1), R(0, 2), 0.0f, R(1, 0), R(1, 1), R(1, 2), 0.0f, R(2, 0), R(2, 1), R(2, 2), 0.0f, 0.0f, 0.0f, 0.0f, 1.0f; return RN; } Matrix4f get_projection_matrix(float eye_fov, float aspect_ratio, float zNear, float zFar) { float fovY = eye_fov / 180.0f * M_PI; float t = tan(fovY / 2) * abs(zNear); float r = aspect_ratio * t; float l = -r; float b = -t; float n = zNear; float f = zFar; // build a matrix of orthographic projection Matrix4f orthoA(4, 4); orthoA &lt;&lt; 2.0f / (r-l), 0.0f, 0.0f, 0.0f, 0.0f, 2.0f / (t-b), 0.0f, 0.0f, 0.0f, 0.0f, 2.0f / (n - f), 0.0f, 0.0f, 0.0f, 0.0f, 1.0f; Matrix4f orthoB(4, 4); orthoB &lt;&lt; 1.0f, 0.0f, 0.0f, -(r + l) / 2.0f, 0.0f, 1.0f, 0.0f, -(t + b) / 2.0f, 0.0f, 0.0f, 1.0f, -(n + f) / 2.0f, 0.0f, 0.0f, 0.0f, 1.0f; Matrix4f ortho = orthoA * orthoB; // build a matrix from perspective to orthographic Matrix4f pto(4, 4); pto &lt;&lt; n, 0.0f, 0.0f, 0.0f, 0.0f, n, 0.0f, 0.0f, 0.0f, 0.0f, n + f, -(n * f), 0.0f, 0.0f, 1.0f, 0.0f; // compute the projection matrix Matrix4f proj = ortho * pto; return proj; } ","link":"https://albertlidesign.github.io/post/games101_1_transformation/"},{"title":"数值分析 第二章 线性方程组的直接方法(3)","content":"第二章 线性方程组的直接方法(3) 这一讲，我们来学习平方根法。平方根法也是一种求解线性方程组的三角分解法。之前我们已经讲过了求解三角分解的Doolittle三角分解法，那么平方根法有什么区别呢？ 2.5 平方根法 平方根法是用来适用于系数矩阵AAA是对称正定矩阵的，如果AAA为对称正定矩阵，则有唯一分解A=LUA=LUA=LU，且ukk&gt;0u_{kk}&gt;0ukk​&gt;0。那么我们可以将矩阵UUU继续分解，分解为一个对角矩阵和一个单位上三角矩阵的乘积： [u11u12...u1nu22...u2n⋱⋮unn]=[u11u22⋱unn][1u12u11...u1nu111...u2nu22⋱⋮1]=DM\\left[\\begin{matrix} u_{11} &amp; u_{12} &amp; ...&amp; u_{1n} \\\\ &amp; u_{22} &amp; ... &amp; u_{2n} \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; u_{nn}\\end{matrix}\\right]= \\left[\\begin{matrix} u_{11} &amp; &amp; &amp; \\\\ &amp; u_{22} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; u_{nn}\\end{matrix}\\right]\\left[\\begin{matrix} 1 &amp; \\frac{u_{12}}{u_{11}} &amp; ...&amp; \\frac{u_{1n}}{u_{11}} \\\\ &amp; 1 &amp; ... &amp; \\frac{u_{2n}}{u_{22}} \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; 1\\end{matrix}\\right] = DM⎣⎢⎢⎢⎡​u11​​u12​u22​​......⋱​u1n​u2n​⋮unn​​⎦⎥⎥⎥⎤​=⎣⎢⎢⎡​u11​​u22​​⋱​unn​​⎦⎥⎥⎤​⎣⎢⎢⎢⎡​1​u11​u12​​1​......⋱​u11​u1n​​u22​u2n​​⋮1​⎦⎥⎥⎥⎤​=DM 则有：A=LDMA=LDMA=LDM 又因为AAA是对称矩阵，那么我们又可以推出(LDM)T=MTDLT=LDM(LDM)^T = M^TDL^T = LDM(LDM)T=MTDLT=LDM，所以M=LTM=L^TM=LT，所以又可以进一步写成A=LDM=LDLTA=LDM=LDL^TA=LDM=LDLT。 因为ukk&gt;0u_{kk}&gt;0ukk​&gt;0，那么我们将对角矩阵DDD再进一步分解： 令D=[u11u22⋱unn][u11u22⋱unn]=D12D12D = \\left[\\begin{matrix} \\sqrt{u_{11}} &amp; &amp; &amp; \\\\ &amp; \\sqrt{u_{22}} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\sqrt{u_{nn}}\\end{matrix}\\right]\\left[\\begin{matrix} \\sqrt{u_{11}} &amp; &amp; &amp; \\\\ &amp; \\sqrt{u_{22}} &amp; &amp; \\\\ &amp; &amp; \\ddots &amp; \\\\ &amp; &amp; &amp; \\sqrt{u_{nn}}\\end{matrix}\\right] = D^{\\frac{1}{2}}D^{\\frac{1}{2}}D=⎣⎢⎢⎡​u11​​​u22​​​⋱​unn​​​⎦⎥⎥⎤​⎣⎢⎢⎡​u11​​​u22​​​⋱​unn​​​⎦⎥⎥⎤​=D21​D21​ 则有A=LD12D12LT=(LD12)(LD12)T=GGTA=LD^{\\frac{1}{2}}D^{\\frac{1}{2}}L^T = (LD^{\\frac{1}{2}})(LD^{\\frac{1}{2}})^T=GG^TA=LD21​D21​LT=(LD21​)(LD21​)T=GGT，其中，G=LD12G=LD^{\\frac{1}{2}}G=LD21​ 因此，正定对称矩阵AAA经过一系列分解，得到A=GGTA=GG^TA=GGT，称为对称正定矩阵的Cholesky分解。 那么，原来的Ax=bAx=bAx=b就转换为了Gy=bGy=bGy=b，GTx=yG^Tx=yGTx=y，这就是平方根法。 计算方法 若记G=(gij)G = (g_{ij})G=(gij​)，则有：对k=1,2,...,nk=1,2,...,nk=1,2,...,n {gkk=(akk−∑m=1k−1gkm2)12gik=(aik−∑m=1k−1gimgkm)÷gkk,i=k+1,...,n\\begin{cases}g_{kk} = (a_{kk} - \\sum^{k-1}_{m=1}g^2_{km})^{\\frac{1}{2}} \\\\ g_{ik} = (a_{ik} - \\sum^{k-1}_{m=1}g_{im}g_{km})\\div g_{kk}, i =k+1,...,n\\end{cases}{gkk​=(akk​−∑m=1k−1​gkm2​)21​gik​=(aik​−∑m=1k−1​gim​gkm​)÷gkk​,i=k+1,...,n​ 求解顺序：先求GGG的都一列，再求第二列，以此类推。 解三角方程Gy=bGy=bGy=b，GTx=yG^Tx=yGTx=y，可得： {yk=(bk−∑m=1k−1gkmym)÷gkkxk=(yk−∑m=k+1ngmkxm)÷gkk\\begin{cases} y_k = (b_k - \\sum^{k-1}_{m=1}g_{km}y_m)\\div g_{kk} \\\\ x_k = (y_k - \\sum^{n}_{m=k+1}g_{mk}x_m)\\div g_{kk} \\end{cases}{yk​=(bk​−∑m=1k−1​gkm​ym​)÷gkk​xk​=(yk​−∑m=k+1n​gmk​xm​)÷gkk​​ 例：解线性方程组： {4x1+2x2+4x3=42x1+10x2−x3=174x1−x2+6x3=0\\begin{cases} 4x_1 + 2x_2+4x_3 = 4 \\\\ 2x_1+10x_2-x_3=17 \\\\ 4x_1-x_2+6x_3=0 \\end{cases}⎩⎪⎨⎪⎧​4x1​+2x2​+4x3​=42x1​+10x2​−x3​=174x1​−x2​+6x3​=0​ 通过观察可知，这是一个正定对称矩阵，因此可以使用Cholesky分解。 解： 我们表示正定对称的系数矩阵AAA和GGG都可以只写出下三角元素 [42104−16]→[2132−11]\\left[\\begin{matrix}4 &amp; &amp; \\\\ 2 &amp; 10 &amp; \\\\ 4 &amp; -1 &amp; 6 \\end{matrix}\\right] \\rightarrow \\left[\\begin{matrix}2 &amp; &amp; \\\\ 1 &amp; 3 &amp; \\\\ 2 &amp; -1 &amp; 1 \\end{matrix}\\right]⎣⎡​424​10−1​6​⎦⎤​→⎣⎡​212​3−1​1​⎦⎤​ 所以： A=[2132−11][2123−11]=GGTA = \\left[\\begin{matrix}2 &amp; &amp; \\\\ 1 &amp; 3 &amp; \\\\ 2 &amp; -1 &amp; 1 \\end{matrix}\\right]\\left[\\begin{matrix}2 &amp; 1 &amp; 2\\\\ &amp; 3 &amp; -1 \\\\ &amp; &amp; 1 \\end{matrix}\\right] = GG^TA=⎣⎡​212​3−1​1​⎦⎤​⎣⎡​2​13​2−11​⎦⎤​=GGT 接下来求解三角方程，解方程Gy=bGy=bGy=b： [2132−11][y1y2y3]=[4170]\\left[\\begin{matrix}2 &amp; &amp; \\\\ 1 &amp; 3 &amp; \\\\ 2 &amp; -1 &amp; 1 \\end{matrix}\\right] \\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ y_3 \\end{matrix}\\right] = \\left[\\begin{matrix} 4 \\\\ 17 \\\\ 0 \\end{matrix}\\right]⎣⎡​212​3−1​1​⎦⎤​⎣⎡​y1​y2​y3​​⎦⎤​=⎣⎡​4170​⎦⎤​，得[y1y2y3]=[251]\\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ y_3 \\end{matrix}\\right] = \\left[\\begin{matrix} 2 \\\\ 5 \\\\ 1 \\end{matrix}\\right]⎣⎡​y1​y2​y3​​⎦⎤​=⎣⎡​251​⎦⎤​ 再解GTx=yG^Tx=yGTx=y： [2123−11][x1x2x3]=[251]\\left[\\begin{matrix}2 &amp; 1 &amp; 2\\\\ &amp; 3 &amp; -1 \\\\ &amp; &amp; 1 \\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix} 2 \\\\ 5 \\\\ 1 \\end{matrix}\\right]⎣⎡​2​13​2−11​⎦⎤​⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎡​251​⎦⎤​，得[x1x2x3]=[−121]\\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix} -1 \\\\ 2 \\\\ 1 \\end{matrix}\\right]⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎡​−121​⎦⎤​ 平方根法是求对称正定系数线性方程组的三角分解法，对称正定矩阵的Cholesky分解的计算量和存储量均约为一般矩阵的LULULU分解的一般。且Cholesky分解具有数值稳定性。 ","link":"https://albertlidesign.github.io/post/NumericalAnalysis4/"},{"title":"数值分析 第二章 线性方程组的直接方法(2)","content":"第二章 线性方程组的直接方法(2) 上一讲我们介绍了Gauss消去法的局限性，为了避免其局限性，提高计算的数值稳定性，在消元过程中采用选择主元的方法。常采用的是列主元消去法和全主元消去法。 2.2 列主元Gauss消去法 给定线性方程组Ax=bAx=bAx=b ，记A(1)=A,b(1)=bA^{(1)} = A,b^{(1)} = bA(1)=A,b(1)=b，列主元Gauss消去法的具体过程如下： （1）首先在增广矩阵B(1)=(A(1),b(1))B^{(1)} = (A^{(1)}, b^{(1)})B(1)=(A(1),b(1))的第一列元素中，取： 在第一列中取绝对值最大的元素为主元素，然后将该行与第一行进行互换，即 ∣ak1(1)∣=max1≤i≤n∣ai1(1)∣，rk⇔r1|a_{k1}^{(1)}| = max_{1\\leq i\\leq n}|a_{i1}^{(1)}| ， r_k \\Leftrightarrow r_1∣ak1(1)​∣=max1≤i≤n​∣ai1(1)​∣，rk​⇔r1​ 然后进行第一步消元得增广矩阵B(2)=(A(2),b(2))B^{(2)} = (A^{(2)}, b^{(2)})B(2)=(A(2),b(2))。 （2）再在矩阵B(2)=(A(2),b(2))B^{(2)} = (A^{(2)}, b^{(2)})B(2)=(A(2),b(2))的第二列元素中，取： ∣ak2(2)∣=max2≤i≤n∣ai2(2)∣，rk⇔r2|a_{k2}^{(2)}| = max_{2\\leq i\\leq n}|a_{i2}^{(2)}| ， r_k \\Leftrightarrow r_2∣ak2(2)​∣=max2≤i≤n​∣ai2(2)​∣，rk​⇔r2​ 然后进行第二步消元得增广矩阵B(3)=(A(3),b(3))B^{(3)} = (A^{(3)}, b^{(3)})B(3)=(A(3),b(3))。 （3）按此方法继续进行下去，经过n−1n-1n−1步选主元和消元运算，得到增广矩阵B(n)=(A(n),b(n))B^{(n)} = (A^{(n)},b^{(n)})B(n)=(A(n),b(n))。则方程组A(n)x=b(n)A^{(n)}x = b^{(n)}A(n)x=b(n)是与原方程等价的上三角形方程组，可进行回代求解。 列主元Gauss消去法可以很好地提高数值稳定性，并且每次选择一列中绝对值最大的数来作为主元，那么它可以执行完消元的充要条件是， 只要∣A∣=0|A| \\not = 0∣A∣​=0，列主元Gauss消去法就可以顺利进行。 例2：采用十进制四位浮点计算，分别用顺序Gauss消去法和列主元Gauss消去法求解线性方程组： {0.012x1+0.01x2+0.167x3=0.6781x1+0.8334x2+5.91x3=12.13200x1+1200x2+4.2x3=981\\begin{cases} 0.012x_1+0.01x_2+0.167x_3 = 0.6781 \\\\ x_1 + 0.8334x_2 + 5.91x_3 = 12.1 \\\\ 3200x_1 + 1200x_2 + 4.2x_3 = 981 \\end{cases}⎩⎪⎨⎪⎧​0.012x1​+0.01x2​+0.167x3​=0.6781x1​+0.8334x2​+5.91x3​=12.13200x1​+1200x2​+4.2x3​=981​ 我们观察发现，这个线性方程组的数量级差距很大，因此在加减和乘除运算中就很容易出现问题，可能会产生“大数吃小数”的问题。 方程组具有四位有效数字的精确解为：x1∗=17.46，x2∗=−45.76,x3∗=5.546x_1^* = 17.46，x_2^* = -45.76, x_3^* = 5.546x1∗​=17.46，x2∗​=−45.76,x3∗​=5.546 解：1. 用顺序Gauss消去法求解，消元过程为： [0.01200.01000.1670.67811.0000.83345.91012.10320012004.200981.0]\\left[\\begin{matrix} 0.0120 &amp; 0.0100 &amp; 0.167 &amp; 0.6781 \\\\ 1.000 &amp; 0.8334 &amp; 5.910 &amp; 12.10 \\\\ 3200 &amp; 1200 &amp; 4.200 &amp; 981.0 \\end{matrix}\\right]⎣⎡​0.01201.0003200​0.01000.83341200​0.1675.9104.200​0.678112.10981.0​⎦⎤​ =[0.01200.01000.1670.678100.1000×10−3−8.010−44.410−1467−4453×10−1798×102]= \\left[\\begin{matrix} 0.0120 &amp; 0.0100 &amp; 0.167 &amp; 0.6781 \\\\ 0 &amp; 0.1000×10^{-3} &amp; -8.010 &amp; -44.41 \\\\ 0 &amp; -1467 &amp; -4453×10 &amp; -1798×10^2 \\end{matrix}\\right]=⎣⎡​0.012000​0.01000.1000×10−3−1467​0.167−8.010−4453×10​0.6781−44.41−1798×102​⎦⎤​ =[0.01200.01000.1670.678100.1000×10−3−8.010−44.4100−1175×105−6517×105]= \\left[\\begin{matrix} 0.0120 &amp; 0.0100 &amp; 0.167 &amp; 0.6781 \\\\ 0 &amp; 0.1000×10^{-3} &amp; -8.010 &amp; -44.41 \\\\ 0 &amp; 0 &amp; -1175×10^5 &amp; -6517×10^5 \\end{matrix}\\right]=⎣⎡​0.012000​0.01000.1000×10−30​0.167−8.010−1175×105​0.6781−44.41−6517×105​⎦⎤​ 回代得x3=5.546,x2=100.0,x1=−104.0x_3 = 5.546, x_2 = 100.0, x_1 = -104.0x3​=5.546,x2​=100.0,x1​=−104.0 我们前面说过，顺序Gauss消去法是直接方法，不存在截断误差，那么导致误差巨大的原因就来自于舍入误差，产生舍入误差大的原因在于主元非常小。 使用列主元Gauss消去法求解，消元过程为： [0.01200.01000.1670.67811.0000.83345.91012.10320012004.200981.0]\\left[\\begin{matrix} 0.0120 &amp; 0.0100 &amp; 0.167 &amp; 0.6781 \\\\ 1.000 &amp; 0.8334 &amp; 5.910 &amp; 12.10 \\\\ 3200 &amp; 1200 &amp; 4.200 &amp; 981.0 \\end{matrix}\\right]⎣⎡​0.01201.0003200​0.01000.83341200​0.1675.9104.200​0.678112.10981.0​⎦⎤​ 选主元，将第一行和第三行互换，得：[320012004.200981.01.0000.83345.91012.100.01200.01000.1670.6781]\\left[\\begin{matrix} 3200 &amp; 1200 &amp; 4.200 &amp; 981.0\\\\ 1.000 &amp; 0.8334 &amp; 5.910 &amp; 12.10 \\\\ 0.0120 &amp; 0.0100 &amp; 0.167 &amp; 0.6781 \\end{matrix}\\right]⎣⎡​32001.0000.0120​12000.83340.0100​4.2005.9100.167​981.012.100.6781​⎦⎤​ 消元得：[320012004.200981.000.45845.90911.7900.55×10−20.1670.6744]\\left[\\begin{matrix} 3200 &amp; 1200 &amp; 4.200 &amp; 981.0\\\\ 0 &amp; 0.4584 &amp; 5.909 &amp; 11.79 \\\\ 0 &amp; 0.55×10^{-2} &amp; 0.167 &amp; 0.6744 \\end{matrix}\\right]⎣⎡​320000​12000.45840.55×10−2​4.2005.9090.167​981.011.790.6744​⎦⎤​ [320012004.200981.000.45845.90911.79000.09610.5329]\\left[\\begin{matrix} 3200 &amp; 1200 &amp; 4.200 &amp; 981.0\\\\ 0 &amp; 0.4584 &amp; 5.909 &amp; 11.79 \\\\ 0 &amp; 0 &amp; 0.0961 &amp; 0.5329 \\end{matrix}\\right]⎣⎡​320000​12000.45840​4.2005.9090.0961​981.011.790.5329​⎦⎤​ 回代得：x3=5.545,x2=−45.77,x1=17.46x_3 = 5.545, x_2 = -45.77, x_1 = 17.46x3​=5.545,x2​=−45.77,x1​=17.46 因此我们看到，列主元Gauss消去法确实能够提高方法的数值稳定性。 小结：列主元Gauss消去法是在每一步消元前，在主元所在的一列选取绝对值最大的元素作为主元素。 而还有一种方法叫做全主元Gauss消去法，是在每一步消元前，在所有元素中选取绝对值最大的元素作为主元素。但由于运算量大增，实际应用中并不常用。 2.3 矩阵三角分解法 我们在消去时都是采用增广矩阵来进行消去，目的是为了保持方程的等价性。本节我们仅对系数矩阵来进行矩阵运算。每对矩阵进行一次消元，实际上就是实施了一次初等行变换。那么从左乘初等矩阵的角度来看是怎样的呢？ 对矩阵A(1)=[a11(1)a12(1)a13(1)...a1n(1)a21(1)a22(1)a23(1)...a2n(1)a31(1)a32(1)a33(1)...a3n(1)⋮⋮⋮⋱⋮an1(1)an2(1)an3(1)...ann(1)]A^{(1)} = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)}&amp;... &amp; a_{1n}^{(1)} \\\\ a_{21}^{(1)} &amp; a_{22}^{(1)} &amp; a_{23}^{(1)}&amp; ... &amp; a_{2n}^{(1)} \\\\a_{31}^{(1)} &amp; a_{32}^{(1)} &amp;a_{33}^{(1)} &amp; ... &amp; a_{3n}^{(1)}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ a_{n1}^{(1)} &amp; a_{n2}^{(1)} &amp; a_{n3}^{(1)} &amp; ... &amp; a_{nn}^{(1)}\\end{matrix}\\right]A(1)=⎣⎢⎢⎢⎢⎢⎢⎡​a11(1)​a21(1)​a31(1)​⋮an1(1)​​a12(1)​a22(1)​a32(1)​⋮an2(1)​​a13(1)​a23(1)​a33(1)​⋮an3(1)​​.........⋱...​a1n(1)​a2n(1)​a3n(1)​⋮ann(1)​​⎦⎥⎥⎥⎥⎥⎥⎤​ 第一步：若a11(1)=0a_{11}^{(1)} \\not = 0a11(1)​​=0，令li1=ai1(1)a11(1),(i=2,3,...,n)l_{i1} = \\frac{a_{i1}^{(1)}}{a_{11}^{(1)}}, (i=2,3,...,n)li1​=a11(1)​ai1(1)​​,(i=2,3,...,n)，记： L1=[1−l211−l311⋮⋱−ln11]L_1 = \\left[\\begin{matrix} 1 &amp; &amp; &amp; &amp; \\\\ -l_{21} &amp; 1 &amp; &amp; &amp; \\\\ -l_{31} &amp; &amp; 1 &amp; &amp; \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; \\\\ -l_{n1} &amp; &amp; &amp; &amp; 1\\end{matrix}\\right]L1​=⎣⎢⎢⎢⎢⎢⎡​1−l21​−l31​⋮−ln1​​1​1​⋱​1​⎦⎥⎥⎥⎥⎥⎤​ 像L1L_1L1​这样的矩阵称为单位下三角矩阵，其主对角线上元素都是1，上三角部分都是0。接下来用L1L_1L1​左乘A(1)A^{(1)}A(1)得： A(2)=L1A(1)=[a11(1)a12(1)a13(1)...a1n(1)0a22(2)a23(2)...a2n(2)0a32(2)a33(2)...a3n(2)⋮⋮⋮⋱⋮0an2(2)an3(2)...ann(2)]A^{(2)} = L_1A^{(1)} = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)}&amp;... &amp; a_{1n}^{(1)} \\\\ 0 &amp; a_{22}^{(2)} &amp; a_{23}^{(2)}&amp; ... &amp; a_{2n}^{(2)} \\\\ 0 &amp; a_{32}^{(2)} &amp;a_{33}^{(2)} &amp; ... &amp; a_{3n}^{(2)}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\0 &amp; a_{n2}^{(2)} &amp; a_{n3}^{(2)} &amp; ... &amp; a_{nn}^{(2)}\\end{matrix}\\right]A(2)=L1​A(1)=⎣⎢⎢⎢⎢⎢⎢⎡​a11(1)​00⋮0​a12(1)​a22(2)​a32(2)​⋮an2(2)​​a13(1)​a23(2)​a33(2)​⋮an3(2)​​.........⋱...​a1n(1)​a2n(2)​a3n(2)​⋮ann(2)​​⎦⎥⎥⎥⎥⎥⎥⎤​ 第二步：若a22(2)=0a_{22}^{(2)} \\not = 0a22(2)​​=0，令li2=ai2(2)a22(2),(i=3,4,...,n)l_{i2} = \\frac{a_{i2}^{(2)}}{a_{22}^{(2)}}, (i=3,4,...,n)li2​=a22(2)​ai2(2)​​,(i=3,4,...,n)，记： L2=[11−l321⋮⋱−ln21]L_2 = \\left[\\begin{matrix} 1 &amp; &amp; &amp; &amp; \\\\ &amp; 1 &amp; &amp; &amp; \\\\ &amp; -l_{32} &amp; 1 &amp; &amp; \\\\ &amp; \\vdots &amp; &amp; \\ddots &amp; \\\\ &amp; -l_{n2} &amp; &amp; &amp; 1\\end{matrix}\\right]L2​=⎣⎢⎢⎢⎢⎢⎡​1​1−l32​⋮−ln2​​1​⋱​1​⎦⎥⎥⎥⎥⎥⎤​ 矩阵L2L_2L2​仍然是单位下三角矩阵，我们用L2L_2L2​去左乘矩阵A(2)A^{(2)}A(2)，得： A(3)=L2A(2)=[a11(1)a12(1)a13(1)...a1n(1)0a22(2)a23(2)...a2n(2)00a33(2)...a3n(2)⋮⋮⋮⋱⋮00an3(2)...ann(2)]A^{(3)} = L_2A^{(2)} = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)}&amp;... &amp; a_{1n}^{(1)} \\\\ 0 &amp; a_{22}^{(2)} &amp; a_{23}^{(2)}&amp; ... &amp; a_{2n}^{(2)} \\\\ 0 &amp; 0 &amp;a_{33}^{(2)} &amp; ... &amp; a_{3n}^{(2)}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\0 &amp; 0 &amp; a_{n3}^{(2)} &amp; ... &amp; a_{nn}^{(2)}\\end{matrix}\\right]A(3)=L2​A(2)=⎣⎢⎢⎢⎢⎢⎢⎡​a11(1)​00⋮0​a12(1)​a22(2)​0⋮0​a13(1)​a23(2)​a33(2)​⋮an3(2)​​.........⋱...​a1n(1)​a2n(2)​a3n(2)​⋮ann(2)​​⎦⎥⎥⎥⎥⎥⎥⎤​ 如此进行下去，第n−1n-1n−1步得到： A(n)=Ln−1A(n−1)=[a11(1)a12(1)a13(1)...a1n(1)a22(2)a23(2)...a2n(2)a33(2)...a3n(2)⋱⋮ann(2)]A^{(n)} = L_{n-1}A^{(n-1)} = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)}&amp;... &amp; a_{1n}^{(1)} \\\\ &amp; a_{22}^{(2)} &amp; a_{23}^{(2)}&amp; ... &amp; a_{2n}^{(2)} \\\\ &amp; &amp;a_{33}^{(2)} &amp; ... &amp; a_{3n}^{(2)}\\\\ &amp; &amp; &amp; \\ddots &amp; \\vdots\\\\ &amp; &amp; &amp; &amp; a_{nn}^{(2)}\\end{matrix}\\right]A(n)=Ln−1​A(n−1)=⎣⎢⎢⎢⎢⎢⎢⎡​a11(1)​​a12(1)​a22(2)​​a13(1)​a23(2)​a33(2)​​.........⋱​a1n(1)​a2n(2)​a3n(2)​⋮ann(2)​​⎦⎥⎥⎥⎥⎥⎥⎤​ 整理整个过程得： A(n)=Ln−1A(n−1)=Ln−1Ln−2A(n−2)=...=Ln−1Ln−2...L2L1A(1)A^{(n)} = L_{n-1}A^{(n-1)} = L_{n-1}L_{n-2}A^{(n-2)} = ... = L_{n-1}L_{n-2}...L_2L_1A^{(1)}A(n)=Ln−1​A(n−1)=Ln−1​Ln−2​A(n−2)=...=Ln−1​Ln−2​...L2​L1​A(1) 其中每一个LkL_kLk​都是一个单位下三角矩阵，而且都是可以求解出来的，说明它们都是可逆矩阵，所以有： A=A(1)=L1−1L2−1...Ln−1−1A(n)=LUA = A^{(1)} = L_{1}^{-1}L_{2}^{-1}...L_{n-1}^-1A^{(n)} = LUA=A(1)=L1−1​L2−1​...Ln−1−​1A(n)=LU 这就是AAA的LU三角分解。 A=LU=[1000l21100l31l3210l41l42l431][u11u12u13u140u22u23u2400u33u34000u44]A = LU = \\left[\\begin{matrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ l_{21} &amp; 1 &amp; 0 &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; 1 &amp; 0 \\\\ l_{41} &amp; l_{42} &amp; l_{43} &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix} u_{11} &amp; u_{12} &amp; u_{13} &amp; u_{14} \\\\ 0 &amp; u_{22} &amp; u_{23} &amp; u_{24} \\\\ 0 &amp; 0 &amp; u_{33} &amp; u_{34} \\\\ 0 &amp; 0 &amp; 0 &amp; u_{44}\\end{matrix}\\right] A=LU=⎣⎢⎢⎡​1l21​l31​l41​​01l32​l42​​001l43​​0001​⎦⎥⎥⎤​⎣⎢⎢⎡​u11​000​u12​u22​00​u13​u23​u33​0​u14​u24​u34​u44​​⎦⎥⎥⎤​ 2.4 直接三角分解法 定理1：若nnn阶方阵AAA的各阶顺序主子式不为0，则存在唯一单位下三角矩阵LLL和上三角矩阵UUU使A=LUA=LUA=LU。 证明：只证唯一性，设有两种分解A=LU=L′U′A=LU=L&#x27;U&#x27;A=LU=L′U′ 则有L′−1L=U′U−1=IL&#x27;^{-1}L = U&#x27;U^{-1} = IL′−1L=U′U−1=I（若下三角矩阵=上三角矩阵，则等号两边都为单位矩阵） 所以L=L′,U=U′L=L&#x27;,U=U&#x27;L=L′,U=U′。 于是Ax=bAx=bAx=b可以写成LUx=bLUx=bLUx=b，令Ux=yUx=yUx=y，得： {Ly=bUx=y\\begin{cases} Ly=b\\\\Ux=y \\end{cases}{Ly=bUx=y​ 大家知道，矩阵乘向量得到的是向量，那么Ly=bLy=bLy=b，Ux=yUx=yUx=y。那么原来一个线性方程组的求解，经过三角分解后变成两个三角形方程组的求解，而三角形线性方程组的求解，相对于普通线性方程组的求解要容易的很多。我们刚刚给出的三角分解和顺序Gauss消元法是一样的。 下面介绍三角分解的Doolittle分解方法，设： [a11a12...a1na21a22...a2n⋮⋮⋱⋮an1an2...ann]=[1l211l31l321⋮⋮⋮⋱ln1ln2ln3...1][u11u12...u1nu22...u2n⋱⋮unn]\\left[\\begin{matrix} a_{11} &amp; a_{12} &amp; ... &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; ... &amp; a_{2n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; ... &amp; a_{nn} \\end{matrix}\\right] = \\left[\\begin{matrix} 1 &amp; &amp; &amp; &amp;\\\\ l_{21} &amp; 1 &amp; &amp; &amp; \\\\ l_{31} &amp; l_{32} &amp; 1 &amp; &amp; \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\\\ l_{n1} &amp; l_{n2} &amp; l_{n3}&amp; ... &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix} u_{11} &amp; u_{12} &amp; ...&amp; u_{1n} \\\\ &amp; u_{22} &amp; ... &amp; u_{2n} \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; u_{nn}\\end{matrix}\\right] ⎣⎢⎢⎢⎡​a11​a21​⋮an1​​a12​a22​⋮an2​​......⋱...​a1n​a2n​⋮ann​​⎦⎥⎥⎥⎤​=⎣⎢⎢⎢⎢⎢⎡​1l21​l31​⋮ln1​​1l32​⋮ln2​​1⋮ln3​​⋱...​1​⎦⎥⎥⎥⎥⎥⎤​⎣⎢⎢⎢⎡​u11​​u12​u22​​......⋱​u1n​u2n​⋮unn​​⎦⎥⎥⎥⎤​ 为了求得两个三角矩阵的分量元素，我们要利用两个三角矩阵的特殊形式和矩阵乘法的运算规则。 则{u1j=a1j,j=1,2,...,nli1=ai1÷u11,i=2,3,...,nakj=lk1u1j+lk2u2j+...+lkk−1uk−1j+ukjukj=akj−∑m=1k−1lkmumj,j=k,k+1,...,nlik=(aik−∑m=1k−1limumk)÷ukk,i=k+1,k+2,...,n\\begin{cases} u_{1j} = a_{1j}, j=1,2,...,n \\\\ l_{i1} = a_{i1}\\div u_{11}, i=2,3,...,n \\\\ a_{kj} = l_{k1}u_{1j}+l_{k2}u_{2j}+...+l_{kk-1}u_{k-1j}+u_{kj} \\\\ u_{kj} = a_{kj}-\\sum ^{k-1}_{m=1}l_{km}u_{mj}, j=k,k+1,...,n\\\\ l_{ik} = (a_{ik} -\\sum ^{k-1}_{m=1}l_{im}u_{mk})\\div u_{kk}, i=k+1,k+2,...,n \\end{cases}⎩⎪⎪⎪⎪⎪⎪⎨⎪⎪⎪⎪⎪⎪⎧​u1j​=a1j​,j=1,2,...,nli1​=ai1​÷u11​,i=2,3,...,nakj​=lk1​u1j​+lk2​u2j​+...+lkk−1​uk−1j​+ukj​ukj​=akj​−∑m=1k−1​lkm​umj​,j=k,k+1,...,nlik​=(aik​−∑m=1k−1​lim​umk​)÷ukk​,i=k+1,k+2,...,n​ 求解顺序：先求UUU的第一行，再求LLL的第一列，再求UUU的第二行，再求LLL的第二列，以此类推。 由{Ly=bUx=y\\begin{cases} Ly=b\\\\Ux=y \\end{cases}{Ly=bUx=y​，得 [1l211l31l321⋮⋮⋮⋱ln1ln2ln3...1][y1y2⋮yn]=[b1b2⋮bn]\\left[\\begin{matrix} 1 &amp; &amp; &amp; &amp;\\\\ l_{21} &amp; 1 &amp; &amp; &amp; \\\\ l_{31} &amp; l_{32} &amp; 1 &amp; &amp; \\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\\\ l_{n1} &amp; l_{n2} &amp; l_{n3}&amp; ... &amp; 1\\end{matrix}\\right] \\left[\\begin{matrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{matrix}\\right] = \\left[\\begin{matrix}b_1\\\\b_2\\\\\\vdots\\\\b_n\\end{matrix}\\right]⎣⎢⎢⎢⎢⎢⎡​1l21​l31​⋮ln1​​1l32​⋮ln2​​1⋮ln3​​⋱...​1​⎦⎥⎥⎥⎥⎥⎤​⎣⎢⎢⎢⎡​y1​y2​⋮yn​​⎦⎥⎥⎥⎤​=⎣⎢⎢⎢⎡​b1​b2​⋮bn​​⎦⎥⎥⎥⎤​ [u11u12...u1nu22...u2n⋱⋮unn][x1x2⋮xn]=[y1y2⋮yn]\\left[\\begin{matrix} u_{11} &amp; u_{12} &amp; ...&amp; u_{1n} \\\\ &amp; u_{22} &amp; ... &amp; u_{2n} \\\\ &amp; &amp; \\ddots &amp; \\vdots \\\\ &amp; &amp; &amp; u_{nn}\\end{matrix}\\right] \\left[\\begin{matrix}x_1\\\\x_2\\\\\\vdots\\\\x_n\\end{matrix}\\right] = \\left[\\begin{matrix}y_1\\\\y_2\\\\\\vdots\\\\y_n\\end{matrix}\\right]⎣⎢⎢⎢⎡​u11​​u12​u22​​......⋱​u1n​u2n​⋮unn​​⎦⎥⎥⎥⎤​⎣⎢⎢⎢⎡​x1​x2​⋮xn​​⎦⎥⎥⎥⎤​=⎣⎢⎢⎢⎡​y1​y2​⋮yn​​⎦⎥⎥⎥⎤​ 可得： {y1=b1yk=bk−∑i=1k−1lkiyi,k=2,3,...,nxn=yn÷unnxi=(yi−∑j=i+1nuijxj)÷uii,i=n−1,n−2,...,1\\begin{cases} y_1=b_1 \\\\ y_{k} = b_{k} - \\sum^{k-1}_{i=1} l_{ki}y_i, k=2,3,...,n \\\\ x_n = y_n \\div u_{nn} \\\\ x_{i} = (y_i - \\sum ^n_{j=i+1}u_{ij}x_j)\\div u_{ii}, i=n-1,n-2,...,1 \\end{cases}⎩⎪⎪⎪⎨⎪⎪⎪⎧​y1​=b1​yk​=bk​−∑i=1k−1​lki​yi​,k=2,3,...,nxn​=yn​÷unn​xi​=(yi​−∑j=i+1n​uij​xj​)÷uii​,i=n−1,n−2,...,1​ 这就是求解方程组Ax=bAx=bAx=b的Doolittle三角分解法。 应用 例1：利用三角分解方法求解线性方程组 {x1+2x2−3x3=12x1−x2+3x3=53x1−2x2+2x3=1\\begin{cases} x_1+2x_2-3x_3 = 1 \\\\ 2x_1 - x_2 + 3x_3 = 5 \\\\ 3x_1 - 2x_2 + 2x_3 = 1 \\end{cases}⎩⎪⎨⎪⎧​x1​+2x2​−3x3​=12x1​−x2​+3x3​=53x1​−2x2​+2x3​=1​ 解： (1) 将系数矩阵AAA书写出来为： A=[12−32−133−22]A = \\left[\\begin{matrix} 1 &amp; 2 &amp; -3 \\\\ 2 &amp; -1 &amp; 3 \\\\ 3 &amp; -2 &amp; 2 \\end{matrix}\\right]A=⎣⎡​123​2−1−2​−332​⎦⎤​ (2) 将系数矩阵AAA分解为LULULU形式，分解后UUU的第一行和AAA的第一行一致，LLL的第一列和AAA的第一列一致，我们只需要计算其他位置的元素即可，有： A=[12−32−133−22]⇒[12−32−59385−175]A = \\left[\\begin{matrix} 1 &amp; 2 &amp; -3 \\\\ 2 &amp; -1 &amp; 3 \\\\ 3 &amp; -2 &amp; 2 \\end{matrix}\\right] \\Rightarrow \\left[\\begin{matrix} 1 &amp; 2 &amp; -3 \\\\ 2 &amp; -5 &amp; 9 \\\\ 3 &amp; \\frac{8}{5} &amp; -\\frac{17}{5} \\end{matrix}\\right]A=⎣⎡​123​2−1−2​−332​⎦⎤​⇒⎣⎡​123​2−558​​−39−517​​⎦⎤​ 所以，我们就可以写出LLL和UUU了 A=[12−32−133−22]=[1213851][12−3−59−175]=LUA = \\left[\\begin{matrix} 1 &amp; 2 &amp; -3 \\\\ 2 &amp; -1 &amp; 3 \\\\ 3 &amp; -2 &amp; 2 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 &amp; &amp; \\\\ 2 &amp; 1 &amp; \\\\ 3 &amp; \\frac{8}{5} &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix} 1 &amp; 2 &amp; -3 \\\\ &amp; -5 &amp; 9 \\\\ &amp; &amp; -\\frac{17}{5} \\end{matrix}\\right] = LUA=⎣⎡​123​2−1−2​−332​⎦⎤​=⎣⎡​123​158​​1​⎦⎤​⎣⎡​1​2−5​−39−517​​⎦⎤​=LU (3) 接下来就是求解两个三角方程组了，先解Ly=bLy=bLy=b，有： [1213851][y1y2y3]=[151]\\left[\\begin{matrix} 1 &amp; &amp; \\\\ 2 &amp; 1 &amp; \\\\ 3 &amp; \\frac{8}{5} &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ y_3 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ 5 \\\\ 1 \\end{matrix}\\right]⎣⎡​123​158​​1​⎦⎤​⎣⎡​y1​y2​y3​​⎦⎤​=⎣⎡​151​⎦⎤​，得[y1y2y3]=[13−345]\\left[\\begin{matrix} y_1 \\\\ y_2 \\\\ y_3 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ 3 \\\\ -\\frac{34}{5} \\end{matrix}\\right]⎣⎡​y1​y2​y3​​⎦⎤​=⎣⎡​13−534​​⎦⎤​ (4) 再解Ux=yUx=yUx=y，有： [12−3−59−175][x1x2x3]=[13−345]\\left[\\begin{matrix} 1 &amp; 2 &amp; -3 \\\\ &amp; -5 &amp; 9 \\\\ &amp; &amp; -\\frac{17}{5} \\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ 3 \\\\ -\\frac{34}{5} \\end{matrix}\\right]⎣⎡​1​2−5​−39−517​​⎦⎤​⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎡​13−534​​⎦⎤​，得[x1x2x3]=[132]\\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ 3 \\\\ 2 \\end{matrix}\\right]⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎡​132​⎦⎤​ 解线性方程组Ax=bAx=bAx=b的Doolittle三角分解法的计算量约为13n3\\frac{1}{3}n^331​n3，与Gauss消去法基本相同。其优点在于求一系列同系数的线性方程组Ax=bk(k=1,2,..,m)Ax=b_k(k=1,2,..,m)Ax=bk​(k=1,2,..,m)时，可大大节省运算量。我们做三角分解仅仅是对系数矩阵AAA进行分解，而bkb_kbk​的变化体现在后面求解两个三角方程组，所以我们说，如果用Doolittle三角分解法在求同系数三角分解时，只需要做一次分解。 ","link":"https://albertlidesign.github.io/post/NumericalAnalysis3/"},{"title":"数值分析 第二章 线性方程组的直接方法(1)","content":"第二章 线性方程组的直接方法(1) 2.1 顺序高斯消去法 很多工程实际问题最终都可以归结为求解线性方程组。如： 用最小二乘法求实验数据的曲线拟合问题 工程中的三次样条函数的插值问题 经济运行中的投入产出问题 大地测量、机械与建筑结构的设计计算问题 这些问题都可以归结为求解线性方程组或者非线性方程组的数学问题。 本节讨论nnn元线性方程组的直接解法 {a11x1+a12x2+...+a1nxn=b1a21x1+a22x2+...+a2nxn=b2...............................an1x1+an2x2+...+annxn=bn\\begin{cases} a_{11}x_1 + a_{12}x_2+...+a_{1n}x_{n}=b_1 \\\\ a_{21}x_1 + a_{22}x_2+...+a_{2n}x_{n}=b_2 \\\\...............................\\\\ a_{n1}x_1 + a_{n2}x_2+...+a_{nn}x_{n}=b_n \\end{cases}⎩⎪⎪⎪⎨⎪⎪⎪⎧​a11​x1​+a12​x2​+...+a1n​xn​=b1​a21​x1​+a22​x2​+...+a2n​xn​=b2​...............................an1​x1​+an2​x2​+...+ann​xn​=bn​​ 矩阵形式为Ax=bAx=bAx=b，为 A=[a11a12a13a1na21a22...a2n............an1an2...ann],x=[x1x2...xn],b=[b1b2...bn]A = \\left[\\begin{matrix} a_{11} &amp; a_{12} &amp; a_{13} &amp; a_{1n} \\\\ a_{21} &amp; a_{22} &amp; ... &amp; a_{2n} \\\\ ... &amp; ... &amp; ... &amp; ... \\\\ a_{n1} &amp; a_{n2} &amp; ... &amp; a_{nn}\\end{matrix}\\right] , x=\\left[\\begin{matrix}x_1\\\\x_2\\\\...\\\\x_n \\end{matrix}\\right],b=\\left[\\begin{matrix}b_1\\\\b_2\\\\...\\\\b_n \\end{matrix}\\right]A=⎣⎢⎢⎡​a11​a21​...an1​​a12​a22​...an2​​a13​.........​a1n​a2n​...ann​​⎦⎥⎥⎤​,x=⎣⎢⎢⎡​x1​x2​...xn​​⎦⎥⎥⎤​,b=⎣⎢⎢⎡​b1​b2​...bn​​⎦⎥⎥⎤​ 根据线性代数知识我们知道，这个矩阵有唯一解的条件是det(A)=0det(A)\\not =0det(A)​=0 求解线性方程组的直接方法：如果不考虑计算过程当中的舍入误差，经过有限次算数运算就可以得到方程组的精确解。 但是我们知道，我们的研究方法都要拿到计算机上实现，舍入误差是不可避免地。上讲提到的Cramer法则就是一种直接方法，但是它并不实用。如果用Cramer法则去计算线性方程组的话，计算量非常大。 接下来介绍几种实用的直接方法。 Gauss消去法 它是一种规则化的加减消元法，它的基本思想是通过逐次消元计算，把一个一般的线性方程组的求解问题转化为等效的上三角方程组的求解问题。 简单示例 {2x1+4x2−2x3=2x1−3x2−3x3=−14x1+2x2+2x3=3\\begin{cases} 2x_1+4x_2-2x_3=2 \\\\ x_1-3x_2-3x_3=-1\\\\4x_1+2x_2+2x_3=3 \\end{cases}⎩⎪⎨⎪⎧​2x1​+4x2​−2x3​=2x1​−3x2​−3x3​=−14x1​+2x2​+2x3​=3​ 消去后面两式x1x_1x1​得 {2x1+4x2−2x3=2−5x2−2x3=−2−6x2+6x3=−1\\begin{cases} 2x_1+4x_2-2x_3=2 \\\\ -5x_2-2x_3=-2\\\\-6x_2+6x_3=-1 \\end{cases}⎩⎪⎨⎪⎧​2x1​+4x2​−2x3​=2−5x2​−2x3​=−2−6x2​+6x3​=−1​ 消去第三个方程中的x2x_2x2​，得 {2x1+4x2−2x3=2−5x2−2x3=−2425x3=75\\begin{cases} 2x_1+4x_2-2x_3=2 \\\\ -5x_2-2x_3=-2\\\\\\frac{42}{5}x_3=\\frac{7}{5} \\end{cases}⎩⎪⎨⎪⎧​2x1​+4x2​−2x3​=2−5x2​−2x3​=−2542​x3​=57​​ 这个时候方程已经变成一个上三角形式的方程组，所以我们通过回代求解出x1，x2，x3x_1，x_2，x_3x1​，x2​，x3​，得 x3=16,x2=13,x1=12x_3 = \\frac{1}{6}, x_2 = \\frac{1}{3},x_1 = \\frac{1}{2}x3​=61​,x2​=31​,x1​=21​ 这就是顺序高斯消元法的基本思想，先逐次消元，然后回代求解。 下面用矩阵形式来将刚刚的消元过程表示，需要注意的是，上面的消元过程是对增广矩阵来进行的，而不是系数矩阵。 (A,b)=[24−221−3−3−14223](A,b) = \\left[\\begin{array}{ccc|c} 2 &amp; 4 &amp; -2 &amp; 2 \\\\ 1 &amp; -3 &amp; -3 &amp; -1 \\\\ 4 &amp; 2 &amp; 2 &amp; 3\\end{array}\\right](A,b)=⎣⎡​214​4−32​−2−32​2−13​⎦⎤​ 消去后面两行中的第一列，r2−12r1,r3−2r1r_2 -\\frac{1}{2}r_1, r_3 -2r_1r2​−21​r1​,r3​−2r1​得 [24−220−5−2−20−66−1]\\left[\\begin{array}{ccc|c} 2 &amp; 4 &amp; -2 &amp; 2 \\\\ 0 &amp; -5 &amp; -2 &amp; -2 \\\\ 0 &amp; -6 &amp; 6 &amp; -1 \\end{array}\\right]⎣⎡​200​4−5−6​−2−26​2−2−1​⎦⎤​ 消去最后一行的第二列，r3−65r2r_3-\\frac{6}{5}r_2r3​−56​r2​得 [24−220−5−2−20042575]\\left[\\begin{array}{ccc|c} 2 &amp; 4 &amp; -2 &amp; 2 \\\\ 0 &amp; -5 &amp; -2 &amp; -2 \\\\ 0 &amp; 0 &amp; \\frac{42}{5} &amp; \\frac{7}{5} \\end{array}\\right]⎣⎡​200​4−50​−2−2542​​2−257​​⎦⎤​ 消元过程实际上就是对增广矩阵不断地实施初等行变换。 一般方法 接下来来介绍求解线性方程组(1)的顺序Gauss消去法，为了清楚起见，我们记A(1)=A,b(1)=b,aij1=aij,bi(1)=biA^{(1)} = A,b^{(1)} = b,a_{ij}^{1} = a_{ij}, b_{i}^{(1)} = b_{i}A(1)=A,b(1)=b,aij1​=aij​,bi(1)​=bi​ 则，线性方程组(1)的增广矩阵为： (A(1),b(1))=[a11(1)a12(1)a13(1)...a1n(1)b1(1)a21(1)a22(1)a23(1)...a2n(1)b2(1)a31(1)a32(1)a33(1)...a3n(1)b3(1)..................an1(1)an2(1)an3(1)...ann(1)bn(1)](A^{(1)},b^{(1)}) = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)}&amp;... &amp; a_{1n}^{(1)} &amp; b_1^{(1)} \\\\ a_{21}^{(1)} &amp; a_{22}^{(1)} &amp; a_{23}^{(1)}&amp; ... &amp; a_{2n}^{(1)} &amp; b_2^{(1)} \\\\a_{31}^{(1)} &amp; a_{32}^{(1)} &amp;a_{33}^{(1)} &amp; ... &amp; a_{3n}^{(1)} &amp; b_3^{(1)} \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ...\\\\ a_{n1}^{(1)} &amp; a_{n2}^{(1)} &amp; a_{n3}^{(1)} &amp; ... &amp; a_{nn}^{(1)} &amp; b_n^{(1)}\\end{matrix}\\right](A(1),b(1))=⎣⎢⎢⎢⎢⎢⎡​a11(1)​a21(1)​a31(1)​...an1(1)​​a12(1)​a22(1)​a32(1)​...an2(1)​​a13(1)​a23(1)​a33(1)​...an3(1)​​...............​a1n(1)​a2n(1)​a3n(1)​...ann(1)​​b1(1)​b2(1)​b3(1)​...bn(1)​​⎦⎥⎥⎥⎥⎥⎤​ 第一步：设a11(1)=0a_{11}^{(1)} \\not = 0a11(1)​​=0，依次用−li1=−ai1(1)a11(1),(i=2,3,...,n)-l_{i1} = -\\frac{a_{i1}^{(1)}}{a_{11}^{(1)}}, (i=2,3,...,n)−li1​=−a11(1)​ai1(1)​​,(i=2,3,...,n)乘矩阵的第1行加到第iii行，得到矩阵： (A(2),b(2))=[a11(1)a12(1)a13(1)...a1n(1)b1(1)0a22(2)a23(2)...a2n(2)b2(2)0a32(2)a33(2)...a3n(2)b3(2)...............0an2(2)an3(2)...ann(2)bn(2)](A^{(2)},b^{(2)}) = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)} &amp;...&amp; a_{1n}^{(1)} &amp; b_1^{(1)} \\\\ 0 &amp; a_{22}^{(2)} &amp; a_{23}^{(2)} &amp; ... &amp; a_{2n}^{(2)} &amp; b_2^{(2)} \\\\0 &amp; a_{32}^{(2)}&amp; a_{33}^{(2)} &amp; ... &amp; a_{3n}^{(2)} &amp; b_3^{(2)} \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ...\\\\ 0 &amp; a_{n2}^{(2)} &amp;a_{n3}^{(2)} &amp; ... &amp; a_{nn}^{(2)} &amp; b_n^{(2)}\\end{matrix}\\right](A(2),b(2))=⎣⎢⎢⎢⎢⎢⎡​a11(1)​00...0​a12(1)​a22(2)​a32(2)​...an2(2)​​a13(1)​a23(2)​a33(2)​...an3(2)​​...............​a1n(1)​a2n(2)​a3n(2)​...ann(2)​​b1(1)​b2(2)​b3(2)​bn(2)​​⎦⎥⎥⎥⎥⎥⎤​ 其中：aij(2)=aij(1)−li1a1j(1),i,j=2,3,...,nbi(2)=bi(1)−li1b1(1),i=2,3,...,na_{ij}^{(2)} = a_{ij}^{(1)} - l_{i1}a_{1j}^{(1)}, i,j=2,3,...,n \\\\ b_i^{(2)} = b_{i}^{(1)} - l_{i1}b_{1}^{(1)}, i=2,3,...,naij(2)​=aij(1)​−li1​a1j(1)​,i,j=2,3,...,nbi(2)​=bi(1)​−li1​b1(1)​,i=2,3,...,n 第二步：设a22(2)=0a_{22}^{(2)} \\not = 0a22(2)​​=0，依次用−li2=−ai2(2)a22(2),(i=3,4,...,n)-l_{i2} = -\\frac{a_{i2}^{(2)}}{a_{22}^{(2)}}, (i=3,4,...,n)−li2​=−a22(2)​ai2(2)​​,(i=3,4,...,n)乘矩阵的第2行加到第iii行，得到矩阵： (A(3),b(3))=[a11(1)a12(1)a13(1)...a1n(1)b1(1)0a22(2)a23(2)...a2n(2)b2(2)00a33(3)...a3n(3)b3(3)..................00an3(3)...ann(3)bn(3)](A^{(3)},b^{(3)}) = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)} &amp;... &amp; a_{1n}^{(1)} &amp; b_1^{(1)} \\\\ 0 &amp; a_{22}^{(2)} &amp; a_{23}^{(2)} &amp; ... &amp; a_{2n}^{(2)} &amp; b_2^{(2)} \\\\0 &amp; 0 &amp; a_{33}^{(3)} &amp;... &amp; a_{3n}^{(3)} &amp; b_3^{(3)} \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ...\\\\ 0 &amp; 0 &amp; a_{n3}^{(3)}&amp; ... &amp; a_{nn}^{(3)} &amp; b_n^{(3)}\\end{matrix}\\right](A(3),b(3))=⎣⎢⎢⎢⎢⎢⎡​a11(1)​00...0​a12(1)​a22(2)​0...0​a13(1)​a23(2)​a33(3)​...an3(3)​​...............​a1n(1)​a2n(2)​a3n(3)​...ann(3)​​b1(1)​b2(2)​b3(3)​...bn(3)​​⎦⎥⎥⎥⎥⎥⎤​ 其中：aij(3)=aij(2)−li2a2j(2),i,j=3,4,...,nbi(3)=bi(2)−li2b2(2),i=3,4,...,na_{ij}^{(3)} = a_{ij}^{(2)} - l_{i2}a_{2j}^{(2)}, i,j=3,4,...,n \\\\ b_i^{(3)} = b_{i}^{(2)} - l_{i2}b_{2}^{(2)}, i=3,4,...,naij(3)​=aij(2)​−li2​a2j(2)​,i,j=3,4,...,nbi(3)​=bi(2)​−li2​b2(2)​,i=3,4,...,n 如此继续消元下去，第n−1n-1n−1步结束后得到矩阵： (A(n),b(n))=[a11(1)a12(1)a13(1)...a1n(1)b1(1)0a22(2)a23(2)...a2n(2)b2(2)00a33(3)...a3n(3)b3(3)..................000...ann(n)bn(n)](A^{(n)},b^{(n)}) = \\left[\\begin{matrix} a_{11}^{(1)} &amp; a_{12}^{(1)} &amp; a_{13}^{(1)} &amp;... &amp; a_{1n}^{(1)} &amp; b_1^{(1)} \\\\ 0 &amp; a_{22}^{(2)} &amp; a_{23}^{(2)} &amp; ... &amp; a_{2n}^{(2)} &amp; b_2^{(2)} \\\\0 &amp; 0 &amp; a_{33}^{(3)} &amp;... &amp; a_{3n}^{(3)} &amp; b_3^{(3)} \\\\ ... &amp; ... &amp; ... &amp; ... &amp; ... &amp; ...\\\\ 0 &amp; 0 &amp; 0 &amp; ... &amp; a_{nn}^{(n)} &amp; b_n^{(n)}\\end{matrix}\\right](A(n),b(n))=⎣⎢⎢⎢⎢⎢⎡​a11(1)​00...0​a12(1)​a22(2)​0...0​a13(1)​a23(2)​a33(3)​...0​...............​a1n(1)​a2n(2)​a3n(3)​...ann(n)​​b1(1)​b2(2)​b3(3)​...bn(n)​​⎦⎥⎥⎥⎥⎥⎤​ 观察这个矩阵，第一行矩阵从头到尾均没有发生变化，第二行元素只发生一次变化，第三行发生了两次变化，这里我们发现，系数矩阵AAA已经变成了上三角矩阵UUU，说明消元过程已经结束了。 对应的方程变成： {a11(1)x1+a12(1)x2+...+a1n(1)xn=b1(1)a22(2)x2+...+a2n(2)xn=b2(2)...............................ann(n)xn=bn(n)\\begin{cases} a_{11}^{(1)}x_1 + a_{12}^{(1)}x_2+...+a_{1n}^{(1)}x_{n}=b_1^{(1)} \\\\ a_{22}^{(2)}x_2+...+a_{2n}^{(2)}x_{n}=b_2^{(2)} \\\\...............................\\\\ a_{nn}^{(n)}x_{n}=b_n^{(n)} \\end{cases}⎩⎪⎪⎪⎨⎪⎪⎪⎧​a11(1)​x1​+a12(1)​x2​+...+a1n(1)​xn​=b1(1)​a22(2)​x2​+...+a2n(2)​xn​=b2(2)​...............................ann(n)​xn​=bn(n)​​ 对方程组进行回代，就可求出方程组的解。 整个消元和回代的过程加在一起就是我们所说的顺序Gauss消去法，那么大家看到了，在整个过程中，方法是精确的，也就是没有截断误差，也就是说顺序Gauss消去法确实是一种直接方法。那么它对比于Cramer法则，它的计算量是如何呢？ 算法效率 顺序Gauss消去法求解nnn元线性方程组的乘除运算量是： n2−1+(n−1)2−1+...+22−1+1+2+...+n=∑k=1n(k2−1)+∑k=1nk=n(n+1)(2n+1)6−n+n(n+1)2=13(n3+3n2−n)n^2-1+(n-1)^2-1+...+2^2-1+1+2+...+n \\\\ = \\sum^n_{k=1}(k^2-1) + \\sum^n_{k=1}k \\\\ = \\frac{n(n+1)(2n+1)}{6}-n+\\frac{n(n+1)}{2}\\\\ = \\frac{1}{3}(n^3+3n^2-n)n2−1+(n−1)2−1+...+22−1+1+2+...+n=∑k=1n​(k2−1)+∑k=1n​k=6n(n+1)(2n+1)​−n+2n(n+1)​=31​(n3+3n2−n) 例如，假设n=20n=20n=20，则乘除次数为306030603060次，Cramer法则为9.7×10209.7×10^{20}9.7×1020 因此，顺序Gauss消去法确实大大减少了运算量，因此它是一种比较实用的直接方法，简称为Gauss消去法。 局限性 （1）主元为0造成的局限性 需要注意的是，在消元之前，我们假设了akkka_{kk}^{k}akkk​不能为0，如果为0则无法进行消元，我们将akkk(k=1,2,...,n)a_{kk}^{k}(k=1,2,...,n)akkk​(k=1,2,...,n)称为主元素。这里的kkk需要真正算到这一步的时候才能判断出kkk是否为0，如果我们在某一步的时候才判断出k=0k=0k=0了，那么前面的步骤就全部白算了。因此我们希望有一种判断方法，能在开始消元之前就判断出整个Gauss消去法中，主元素能否为0。结论为： 主元素都不为0的充要条件是：矩阵AAA的各阶顺序主子式都不为0 矩阵AAA为系数矩阵，因此我们可以根据判断矩阵AAA的各阶顺序主子式来判断，因此这也是Gauss消去法的局限所在。 （2）消去过程中产生“大数吃小数”问题 顺序高斯消去法除了当主元素为0时可能造成的无法消元以外还有其他局限性吗？我们来看一个例子 例1：解线性方程组（用十进制四位浮点计算） {0.000100x1+1.00x2=1.001.00x1+1.00x2=2.00\\begin{cases} 0.000100x_1+1.00x_2=1.00 \\\\ 1.00x_1+1.00x_2=2.00 \\end{cases}{0.000100x1​+1.00x2​=1.001.00x1​+1.00x2​=2.00​ （用Cramer法则可得精确解x1∗=1.00010,x2∗=0.99990x_1^* = 1.00010, x_2^* = 0.99990x1∗​=1.00010,x2∗​=0.99990） 解：用顺序Gauss消去法，消元得 {0.000100x1+1.00x2=1.00−10000x2=−10000\\begin{cases} 0.000100x_1+1.00x_2=1.00 \\\\ -10000x_2=-10000 \\end{cases}{0.000100x1​+1.00x2​=1.00−10000x2​=−10000​ 这就产生了“大数吃小数”的问题，这样求解得x2=1.00,x1=0.00x_2 = 1.00, x_1 = 0.00x2​=1.00,x1​=0.00。我们发现这个误差非常非常大。由于顺序Gauss消去法没有截断误差，因此这是由舍入误差造成的。那么这个“大数”是从哪来的呢？我们发现在第一次进行消元的时候，求两行比例系数的过程中，我们用了绝对值很小的数做了除数，以至于误差被放大了。 我们将方程组交换两行，改写成 {1.00x1+1.00x2=2.000.000100x1+1.00x2=1.00\\begin{cases} 1.00x_1+1.00x_2=2.00 \\\\ 0.000100x_1+1.00x_2=1.00\\end{cases}{1.00x1​+1.00x2​=2.000.000100x1​+1.00x2​=1.00​ 用顺序Gauss消去法，消元得： {1.00x1+1.00x2=2.001.00x2=1.00\\begin{cases} 1.00x_1+1.00x_2=2.00 \\\\ 1.00x_2=1.00\\end{cases}{1.00x1​+1.00x2​=2.001.00x2​=1.00​ 回代得解x2=1.00,x1=1.00x_2 = 1.00, x_1 = 1.00x2​=1.00,x1​=1.00，这个近似解是可以接受的，误差也在可接受的范围。 通过这个例子大家看到了，在顺序Gauss消去法当中，尽管主元素不为0，可以保证Gauss消去法从头进行到尾，但是如果在消元的过程中，主元素很小的话，同样会产生最终解的舍入误差非常大以至于不能用的情况。这就是顺序高斯消去法第二个局限性，同时我们也找到了改变这个局限性的方法，就是互换两个方程。 ","link":"https://albertlidesign.github.io/post/NumericalAnalysis2/"},{"title":"数值分析 第一章 绪论","content":"误差的来源和分类 按误差来源分类 模型误差 数学模型通常是由实际问题抽象得到的，一般带有误差，这种误差称为模型误差，一般来说是不可避免的。 观测误差 数学模型中包含的一些物理参数通常是通过观测和实验得到的，难免带有误差，这种误差称为观测误差。也是不可避免的。 截断误差 求解数学模型所用的数值方法通常是一种近似方法，这种因方法产生的误差称为截断误差或方法误差。 例如：利用ln(x+1)ln(x+1)ln(x+1)的Taylor公式: ln(x+1)=x−12x2+13x3−14x4+...+(−1)n+11nxn+...ln(x+1) = x-\\frac{1}{2}x^2 + \\frac{1}{3}x^3 - \\frac{1}{4}x^4 +...+(-1)^{n+1}\\frac{1}{n}x^n+...ln(x+1)=x−21​x2+31​x3−41​x4+...+(−1)n+1n1​xn+... 这是一个级数的形式，我们在计算机中无法计算无穷多项 实际计算时只能截取有限项代数和计算，如取前5项有： ln(2)≈1−12+13−14+15ln(2) \\approx 1-\\frac{1}{2}+\\frac{1}{3}-\\frac{1}{4}+\\frac{1}{5}ln(2)≈1−21​+31​−41​+51​ 这里产生的误差为截断误差，记作R5R_5R5​ R5=−16+17−18+19−...R_5 = -\\frac{1}{6} + \\frac{1}{7}-\\frac{1}{8}+\\frac{1}{9}-...R5​=−61​+71​−81​+91​−... 舍入误差 由于计算机只能对有限位数进行运算，在运算中像eee、2\\sqrt{2}2​、13\\frac{1}{3}31​等都要按舍入原则保留有限位，这时产生的误差称为舍入误差或计算误差。 截断误差或者说方法误差是由我们主观采用近似方法而产生的误差。舍入误差是由于计算机只能对有限位数进行运算而产生的误差，是不可避免的。 在数值分析中，我们总假定数学模型是准确的，因为不考虑模型误差和观测误差，主要研究截断误差和舍入误差对计算结果的影响。 绝对误差和相对误差 绝对误差 设xxx是精确值x∗x^*x∗的一个近似值，记 e=x∗−xe = x^*-xe=x∗−x 称eee为近似值xxx的绝对误差，简称误差。 在本课程中，xxx为近似值，x∗x^*x∗为精确值。一般来说，x∗x^*x∗仅用于理论分析。 如果ϵ\\epsilonϵ满足： ∣e∣⩽ϵ|e| \\leqslant \\epsilon∣e∣⩽ϵ 则称ϵ\\epsilonϵ为近似值xxx的绝对误差限，简称为误差限。 精确值x∗x^*x∗、近似值xxx和误差限ϵ\\epsilonϵ之间满足： x−ϵ⩽x∗⩽x+ϵx-\\epsilon \\leqslant x^* \\leqslant x+\\epsilonx−ϵ⩽x∗⩽x+ϵ 简写为 x∗=x±ϵx^* = x \\pm \\epsilonx∗=x±ϵ 注意，这里的===不是精确的 绝对误差有时并不能很好地反应近似程度的好坏，如： x∗=10,ϵx=1,y∗=10000,ϵy=5x^*=10, \\epsilon _x = 1, y^* = 10000, \\epsilon _y = 5x∗=10,ϵx​=1,y∗=10000,ϵy​=5 虽然ϵy\\epsilon _yϵy​是ϵx\\epsilon _xϵx​的5倍，但在10000内差5显然比10内差1好。 相对误差 记： er=ex∗=x∗−xx∗e_r = \\frac{e}{x^*} = \\frac{x^*-x}{x^*}er​=x∗e​=x∗x∗−x​ 称ere_rer​为近似值xxx的相对误差。 同样地，由于x∗x^*x∗未知，实际使用时总是将xxx的相对误差取为： er=ex=x∗−xxe_r = \\frac{e}{x} = \\frac{x^*-x}{x}er​=xe​=xx∗−x​ ϵr=ϵ∣x∣\\epsilon _r = \\frac{\\epsilon}{|x|}ϵr​=∣x∣ϵ​称为近似值xxx的相对误差限。∣er∣⩽ϵr|e_r| \\leqslant \\epsilon _r∣er​∣⩽ϵr​ 例题 例1：设x=1.24x=1.24x=1.24是由精确值x∗x^*x∗经过四舍五入得到的近似值，求xxx的绝对误差限和相对误差限。 解：由已知可得1.235⩽x∗&lt;1.2451.235 \\leqslant x^* &lt;1.2451.235⩽x∗&lt;1.245，我们说这个误差是截断误差，是由四舍五入这个方法得到的近似值。 所以ϵ=0.005,ϵr=0.005÷1.24≈0.4%\\epsilon = 0.005, \\epsilon _r = 0.005 \\div 1.24 \\approx 0.4\\%ϵ=0.005,ϵr​=0.005÷1.24≈0.4% 一般地，凡是由精确值经过四舍五入得到的近似值，其绝对误差限等于该近似值末尾的半个单位。 有效数字 有效数字的定义 问题：下列数作为π\\piπ的近似值，它们的近似效果都一样吗？每个近似值当中的数字都起作用吗？ 3.14,3.141,3.153.14, 3.141, 3.153.14,3.141,3.15 定义1：设数xxx是数x∗x^*x∗的近似值，如果xxx的绝对误差限是它的某一数位的半个单位，并且从xxx左起第一个非零数字到该数为共有nnn位，则称这nnn个数字为xxx的有效数字，也称用xxx近似x∗x^*x∗时具有nnn位有效数字。 数xxx总可以写成如下形式: x=±0.a1a2...ak×10mx = \\pm 0.a_1a_2...a_k×10^mx=±0.a1​a2​...ak​×10m 其中mmm为整数，aia_iai​是0到9中的一个数字，a1=0a_1 \\not = 0a1​​=0。 我们称这种形式为标准浮点数，任何一个数都可以转化为标准浮点数。 有效数字和绝对误差限之间的关系 xxx作为x∗x^*x∗的近似值，具有nnn位(n⩽kn \\leqslant kn⩽k)有效数字当且仅当： ∣x∗−x∣⩽12×10m−n|x^* - x| \\leqslant \\frac{1}{2} × 10^{m-n}∣x∗−x∣⩽21​×10m−n 由此可见，近似值的有效数字越多，其绝对误差越小。 例1：为了使x∗=2x^* = \\sqrt{2}x∗=2​的近似值的绝对误差小于10−510^{-5}10−5，问应取几位有效数字？ 根据∣x∗−x∣⩽12×10m−n|x^* - x| \\leqslant \\frac{1}{2} × 10^{m-n}∣x∗−x∣⩽21​×10m−n 解：由于2=1.4...\\sqrt{2} = 1.4...2​=1.4...，则近似值xxx可以写为: x=±0.a1a2...ak×10x = \\pm 0.a_1a_2...a_k×10x=±0.a1​a2​...ak​×10，a1=1=0a_1 =1 \\not = 0a1​=1​=0，式中的m=1m=1m=1。 令∣2−x∣⩽12×101−n⩽10−5|\\sqrt{2} -x|\\leqslant \\frac{1}{2} ×10^{1-n} \\leqslant 10^{-5}∣2​−x∣⩽21​×101−n⩽10−5 故取n=6n=6n=6，即取6位有效数字。此时x=1.41421x=1.41421x=1.41421。 注意：精确值的有效数字可认为有无穷多位。 3.14,3.141,3.153.14, 3.141, 3.153.14,3.141,3.15 求它们的近似程度实际上就是求它们的绝对误差限是多少，每个近似值当中的数字都起作用吗？实际上就是求有效数字的位数。 有效数字和相对误差限的关系 若xxx有nnn位有效数字，则其相对误差限为： ϵr⩽12a1×10−n+1\\epsilon _r \\leqslant \\frac{1}{2a_1}×10^{-n+1}ϵr​⩽2a1​1​×10−n+1 反之，若xxx的相对误差限： ϵr⩽12(a1+1)×10−n+1\\epsilon _r \\leqslant \\frac{1}{2(a_1 +1)} ×10^{-n+1}ϵr​⩽2(a1​+1)1​×10−n+1，则xxx至少有nnn位有效数字。 因此，知道有效数字可以求出相对误差限，知道相对误差限也可以求出有效数字。 数值计算的若干原则 为了减少舍入误差的影响，设计算法时应遵循如下原则。 1. 避免两个相近的数相减 如果两个相近的数相减，它得到的差会很小。从数值分析的角度上来说，它的有效数字的位数会大大减少。对于近似值，我们要尽可能多地保留有效数字的位数，因此要避免两个相近的数相减。 在数值计算中，如果遇到两个相近的数相减运算，可以考虑改变一下算法以避免两数相减。例如 当x1≈x2x_1 \\approx x_2x1​≈x2​时，要求计算logx1−logx2log{x_1} - log{x_2}logx1​−logx2​，此时可以用对数的运算法则， 用logx1x2log{\\frac{x_1}{x_2}}logx2​x1​​来替换。 当x≈0x \\approx 0x≈0时，要求计算1−cosx1-cosx1−cosx，此时可以用三角函数的和差化积公式来计算，用2sin2x22sin^2\\frac{x}{2}2sin22x​。 当x&gt;&gt;1x&gt;&gt;1x&gt;&gt;1时，要求计算x+1−x\\sqrt{x+1}-\\sqrt{x}x+1​−x​，此时可以用1x+1+x\\frac{1}{\\sqrt{x+1}+\\sqrt{x}}x+1​+x​1​。 例1：求方程x2−64x+1=0x^2 -64x+1=0x2−64x+1=0的两个根，使他们至少具有四位有效数字。（1023≈31.984\\sqrt{1023} \\approx 31.9841023​≈31.984） 解：由求根公式有x1=32+1023≈63.984x_1 = 32+\\sqrt{1023}\\approx 63.984x1​=32+1023​≈63.984，具有5位有效数字，满足要求 若由x2=32−1023≈0.016x_2 = 32-\\sqrt{1023} \\approx 0.016x2​=32−1023​≈0.016，仅有两位有效数字。 这里我们需要改变算法，根据两个根之间的关系得x2=1x1≈0.01563x_2 = \\frac{1}{x_1} \\approx 0.01563x2​=x1​1​≈0.01563，则具有四位有效数字。 对两个相近的数相减，若找不到适当的方法代替，只能在计算机上采用双倍字长计算，以提高精度。 2. 防止大数“吃掉”小数 因为在计算机上只能采用有限位数计算，若参加运算的数量级差很大，在它们的加、减运算中，绝对值很小的数往往被绝对值较大的数“吃掉”，造成的计算结果失真。 例如，用八位十进制的浮点计算A=26358713+0.8+0.2A = 26358713+0.8+0.2A=26358713+0.8+0.2 按照加法浮点运算的对阶规则，应有： A=0.26358713×108+0.000000008×108+0.000000002×108A = 0.26358713×10^8 + 0.000000008×10^8 + 0.000000002×10^8A=0.26358713×108+0.000000008×108+0.000000002×108 由于采用八位数运算，于是A=26358713A = 26358713A=26358713 若改变计算顺序，计算0.2+0.8+263587130.2+0.8+263587130.2+0.8+26358713，则有 A=0.00000001×108+0.26358713×108=26358714A = 0.00000001 ×10^8 + 0.26358713×10^8 = 26358714A=0.00000001×108+0.26358713×108=26358714 可见，在求和或差的过程中应采用由小到大的运算过程。 3. 绝对值太小的数不宜作除数 由于除数很小，将导致商的绝对误差很大，有可能出现“溢出”的现象。 4. 注意简化计算程序，减少计算次数 首先，若算法计算量太大，实际计算无法完成。 例如用线性代数中的Cramer法则求nnn元线性方程组Ax=bAx=bAx=b的解， 需要计算n+1n+1n+1个nnn阶行列式，而每个nnn阶行列式按定义D=∑P1P2...Pn(−1)ta1P1a2P2...anPnD =\\sum_{P_1P_2...P_n}(-1)^t a_{1P1}a_{2P2}...a_{nPn}D=∑P1​P2​...Pn​​(−1)ta1P1​a2P2​...anPn​需要计算(n−1)n!(n-1)n!(n−1)n!次乘法，则Cramer法则至少需要(n2−1)n!(n^2-1)n!(n2−1)n!次乘法，当n=20n=20n=20时，有(202−1)20!≈9.7×1020(20^2-1)20! \\approx 9.7×10^{20}(202−1)20!≈9.7×1020次乘法运算。 其次，即使是可行算法，则计算量越大积累的误差也越大，因此计算量越小越好。 因为舍入误差是不可避免地，它会随着计算量传播和积累。 例如计算nnn次多项式： Pn(x)=anxn+an−1xn−1+...+a1x+a0P_n(x) = a_nx^n+a_{n-1}x^{n-1}+...+a_1x+a0Pn​(x)=an​xn+an−1​xn−1+...+a1​x+a0 若直接逐项计算，大约需要乘法运算次数为n(n+1)2\\frac{n(n+1)}{2}2n(n+1)​次。 若将多项式改写为： Pn(x)=(...((anx+an−1)x+an−2)x+...)x+a0P_n(x) = (...((a_nx+a_{n-1})x +a_{n-2})x+...)x+a_0Pn​(x)=(...((an​x+an−1​)x+an−2​)x+...)x+a0​ 则只需nnn次乘法和nnn次加法运算。 5. 选用数值稳定性好的算法 一种数值算法，如果其计算舍入误差积累是可控制的，则称其为数值稳定的，反之称之为不稳定的。 例如积分： In=∫01xnex−1dxI_n = \\int _0^1 x^ne^{x-1}dxIn​=∫01​xnex−1dx 利用分部积分法可得计算InI_nIn​的递推公式： In=1−nIn−1,n=1,2,...I_n = 1-nI_{n-1},n=1,2,...In​=1−nIn−1​,n=1,2,... 我们还需要一个初始迭代值I0I_0I0​，由于n=0n=0n=0时： I0=∫01ex−1dx=1−e−1=0.632120558...I_0 = \\int_0^1 e^{x-1}dx = 1-e^{-1} = 0.632120558...I0​=∫01​ex−1dx=1−e−1=0.632120558... 也就是说，在取I0I_0I0​时会有一个舍入误差，若取四位有效数字则递推可得： 对任何nnn都应有In&gt;0I_n&gt;0In​&gt;0，但计算结果显示I8&lt;0I_8&lt;0I8​&lt;0，可见，虽然I0I_0I0​的近似误差不超过0.5×10−40.5×10^{-4}0.5×10−4，但随着计算步骤的增加，误差明显增大。这说明这里的递推公式是数值不稳定的。 这个算法本身是没有截断误差的，而在取I0I_0I0​时产生舍入误差。 根据原因，我们可以修改算法为 In−1=1n(1−In),n=k,k−1,...,2,1I_{n-1} = \\frac{1}{n}(1-I_n),n=k,k-1,...,2,1In−1​=n1​(1−In​),n=k,k−1,...,2,1 类似地，可得： Ik−Ik∗=(−1)n−kk!n!(In−In∗),k=n,n−1,...,1,0I_k - I_k^* = (-1)^{n-k}\\frac{k!}{n!}(I_n-I_n^*), k=n,n-1,...,1,0Ik​−Ik∗​=(−1)n−kn!k!​(In​−In∗​),k=n,n−1,...,1,0 可见，近似误差Ik−Ik∗I_k-I_k^*Ik​−Ik∗​是可控的，因此算法是数值稳定的。 ","link":"https://albertlidesign.github.io/post/NumericalAnalysis1/"},{"title":"ALG_Metaballs","content":"Introduction ALG_Metaballs is an plug-in to generate 3d metaballs for Grasshopper using Marching Cubes algorithm on GPU. The development of this project is an important learning experience for me. I hope it can be a parallel programming reference case which can help GPU programming beginners and Grasshopper developers. At present, its computational performance can still be optimized. I will continue to improve it in my future work. Algorithm Compute bounding box from input points. Generation of a voxel-based grid inside the box. Classify voxels: Mark all the active voxels to get an active voxels array and calculate the number of vertices in each voxel by looking up vertices table. It is executed using one thread per voxel. Exclusive sum scan: Get the total number of active voxels and the total number of resulting vertices using exclusive scan algorithm. They are obtained by the sum of the last value of the exclusive scan and the last value of input array. Compact voxels: This compacts the active voxels array to get rid of empty voxels. This allows us to execute Isosurface Extraction on only the active voxels. Isosurface extraction: Calculate the position of the points in the active voxel and obtain all the result points by looking up triangle table. Generate a mesh model from result points. How metaballs work [10] Each metaball has a &quot;sphere of influence&quot;. When you merge two metaballs and they extend into each other’s sphere of influence, they react in a way similar to drops of water: the surface tension works to form a smooth bridge between them. This is useful for making organic &quot;blobby&quot; shapes which meld into each other. Metaballs can be thought of as spherical force fields whose surface is an implicit function defined at any point where the density of the force field equals a certain threshold. Because the density of the force field can be increased by the proximity of other metaball force fields, metaballs have the unique property that they change their shape to adapt and fuse with surrounding metaballs. This makes them very effective for modeling organic surfaces. For example, below we have a metaball. The surface of the metaball exists whenever the density of the metaball’s field reaches a certain threshold: When two or more metaball force fields are combined, as in the illustration below, the resulting density of the force fields is added, and the surface extends to include that area where the force fields intersect and create density values with a value of one. Reference [1] Lorensen W E, Cline H E. Marching cubes: A high resolution 3D surface construction algorithm. ACM SIGGRAPH Computer Graphics. 1987;21(4) [2] The algorithm and lookup tables by Paul Bourke httppaulbourke.netgeometrypolygonise：http://paulbourke.net/geometry/polygonise/ [3] Triquet, F., Meseure, P., &amp; Chaillou, C. (2001). Fast polygonization of implicit surfaces. [4] McPheeters, G. W. C., &amp; Wyvill, B. (1986). Data structure for soft objects. The Visual Computer, 2(4), 227-234. [5] https://www.sidefx.com/docs/houdini/nodes/sop/metaball.html ","link":"https://albertlidesign.github.io/post/alg_metaballs/"},{"title":"理论力学 第五节 平面任意力系","content":"平面任意力系 平面任意力系的简化 平面任意力系 力系中各力（偶）的作用线（面）处于同一平面且任意分布时，这样的力系称为平面任意力系。 武汉长江大桥，上下两个桥面的主桁架结构，它所受到的力可以近似地认为是两个平面任意力系。 哈尔滨松花江铁路桥，桥中两个主桁架所受到地内力和外力，都可以认为在这个桁架所在的平面内，构成了两个平面任意力系。 那么如何来对平面任意力系来进行简化呢？ 力的平移定理 可以把作用在刚体上的点A的力FFF平行移动到任一点B，但必须同时附加一个力偶，这个附加力偶的矩等于运来的力FFF对新作用点B的矩。MB=MB(F)=FdM_B = M_B(F) = FdMB​=MB​(F)=Fd 证明：在刚体上A点作用有一个力FFF，利用加减平衡力系原理，我们可以在B点加上一个F′F&#x27;F′和F′′F&#x27;&#x27;F′′，它们与FFF的大小相等，方向平行，这样就与力FFF构成了一个力偶，利用力偶的等效定理，可以用一个力偶的符号来表示，这样就相当于，将作用在A点的力平移到了B点，并且作用效果保持不变。 力的平移定理提供了将力在刚体内等效移动的方法。是任意力系简化的基础，在静力学中占有重要地位。 力的平移定理的应用实例 打乒乓球时，高手容易打出弧线球，这是因为在击球过程中，拍面的法线方向与击球法向存在很大的角度，更多的是依靠拍面与球的摩擦力来使其移动，利用力的平移定理，将摩擦力平移到乒乓球的球心，为了保持力的作用效果不变，需要加上一个力偶，大小等于摩擦力对球心的矩，这样使球沿着击球方向运动，而力偶使球产生旋转。 平面任意力系向作用面内一点简化 · 主矢和主矩 如图所示平面任意力系，将F1F_1F1​平移到O点得到F1′F_1&#x27;F1′​，大小不变，同时需要附加一个力偶M1=MO(F1)M_1=M_O(F_1)M1​=MO​(F1​)，大小等于F−1F-1F−1对O点的矩。 依次将所有力如此处理，得到了一个平面汇交力系，而M1M_1M1​到MnM_nMn​构成了平面力偶系。 也就是说，平面汇交力系和一个平面力偶系可以等效地代替一个平面任意力系。而平面汇交力系可简化为一个合力FRF_RFR​，而平面力偶系也可以进行合成，合成为一个合力偶。即 FR′=∑Fi′=∑FiF_R&#x27; = \\sum F_i&#x27; = \\sum F_iFR′​=∑Fi′​=∑Fi​ MO=∑Mi=∑MO(Fi)M_O = \\sum M_i = \\sum M_O(F_i)MO​=∑Mi​=∑MO​(Fi​) 我们不再把这个力叫做平面任意力系的合力，而是称之为平面任意力系向一点简化的主矢，即FR′=∑FiF_R&#x27; = \\sum F_iFR′​=∑Fi​。类似地，合力偶称为平面任意力系向一点简化的主矩，即MO=∑MO(Fi)M_O = \\sum M_O(F_i)MO​=∑MO​(Fi​)。 主矢和主矩是平面任意力系向一点简化的特征量，它们可以等效替换一个任意力系。 主矢在力系所在平面内，大小与简化中心无关。 主矩作用面也在力系所在平面内，大小一般与简化中心有关。 小结：平面任意力系向作用面内任一点O简化，可得一个力和一个力偶。力为力系的主矢，大小与简化中心无关，但作用线通过简化中心；力偶为力系对O点的主矩，作用点任意，但大小一般与简化中心有关。 主矢的计算方法一般和计算平面汇交力系的方法相同，采用解析法 FRx′=∑Fix′=∑Fix=∑FxF_Rx&#x27; = \\sum F_{ix}&#x27; = \\sum F_{ix} = \\sum F_xFR​x′=∑Fix′​=∑Fix​=∑Fx​ FRy′=∑Fiy′=∑Fiy=∑FyF_Ry&#x27; = \\sum F_{iy}&#x27; = \\sum F_{iy} = \\sum F_yFR​y′=∑Fiy′​=∑Fiy​=∑Fy​ 主矢大小：FR′=(∑Fix)2+(∑Fiy)2F_R&#x27; = \\sqrt{(\\sum F_{ix})^2 + (\\sum F_{iy})^2}FR′​=(∑Fix​)2+(∑Fiy​)2​ 主矢方向：cos(F⃗R′,i)=∑FixFR′cos(F⃗R′,j)=∑FiyFR′cos(\\vec F_R&#x27;,i) = \\frac{\\sum F_{ix}}{F_R&#x27;} \\quad cos(\\vec F_R&#x27;,j) = \\frac{\\sum F_{iy}}{F_R&#x27;}cos(FR′​,i)=FR′​∑Fix​​cos(FR′​,j)=FR′​∑Fiy​​ 作用点：一般令其作用于简化中心上 主矩：MO=∑MO(Fi)M_O = \\sum M_O(F_i)MO​=∑MO​(Fi​) 平面任意力系向一点简化实例：固定端约束 埋入地下的电线杆，伸出楼面的阳台，正在车削的工件，舵机的按钮，钉在墙壁上挂东西的钉子。当他们受到来自同一个平面上的主动力时，其约束力就是一个典型的平面任意力系。这样的约束就称为平面固定端约束，它既限制了物体的移动，又限制了物体的转动。可以利用平面任意力系的简化理论，对这类约束问题进行简化求解。 为了求解这类问题，首先需要将约束端的受力进行简化，约束端的受力是个复杂并且未知的平面任意力系。不管这个力系如何复杂，我们总可以向一点，比如A点进行简化，得到一个力FAF_AFA​和一个力偶MAM_AMA​，可以用两个分量FAxF_{Ax}FAx​和FAyF_{Ay}FAy​表示，这样一来，这个复杂的任意力系可以用三个未知量来等效地替换。这样就可以很方便地求解。因此可以看来，平面固定端约束比光滑铰链约束多了一个限制的力偶。因此两个约束有本质的区别。 平面任意力系向一点简化的结果分析 第一种情况：一个平面任意力系向一点简化得到主矢FR′F_R&#x27;FR′​和主矩MOM_OMO​，当主矢FR′=0F_R&#x27;=0FR′​=0而MO=0M_O \\not ={0}MO​​=0表明简化的结果是个合力偶。如果不向O点简化，而是向平面内任意一点O1O_1O1​简化，显然主矢仍然为000，因为主矢的大小与简化中心位置无关，所以向O1O_1O1​点简化，结果仍然是一个力偶，那么主矩是多少呢？因为主矢为000所以平移后主矩的结果仍然是MOM_OMO​，这表明在这种情况下，该力系向任意一点简化，所得到的力系仍然是相同的力系，也就是说这其实是是一个力偶系，与简化中心位置无关。所以平面力偶系是平面任意力系的特殊情形。 前面提到过，平面任意力系向一点简化，主矢的大小和简化中心无关，而主矩的大小和简化中心有关，但是在平面力偶系的情况下，主矩大小与简化中心无关。 第二种情况：一个平面任意力系向一点简化得到主矢FR′F_R&#x27;FR′​和主矩MOM_OMO​，当主矢FR′=0F_R&#x27; \\not =0FR′​​=0而MO=0M_O ={0}MO​=0表明简化的结果是个合力，作用线过简化中心。也就是说作用在O点的力FR′F_R&#x27;FR′​与这个力系等效，那么这个合力是否一定要作用在O点？不是的，作用在刚体上的力具有平移性，也就是FR′F_R&#x27;FR′​可以沿着它的作用线平移，不改变作用效果，仅仅要求它的作用线通过简化中心，作用点只要存在于刚体上即可。 第三种情况：一个平面任意力系向一点简化得到主矢FR′F_R&#x27;FR′​和主矩MOM_OMO​，当主矢FR′=0F_R&#x27; \\not =0FR′​​=0而MO=0M_O \\not ={0}MO​​=0，利用力偶的等效定理，我们可以用FRF_RFR​和FR′′F_R&#x27;&#x27;FR′′​，其中FRF_RFR​和FR′′F_R&#x27;&#x27;FR′′​的大小与主矢FR′F_R&#x27;FR′​相等，方向平行，力偶臂d=MOFR′d = \\frac{M_O}{F_R&#x27;}d=FR′​MO​​，这样就将主矢和主矩等效替换成了三个力，其中FR′F_R&#x27;FR′​和FR′′F_R&#x27;&#x27;FR′′​构成了一个平衡力系，由加减平衡力系原理，因此可以将这个平衡力系拿掉，就变成了单个的力FRF_RFR​与这个力系等效，因此简化的结果仍然是一个合力，只不过作用线至简化中心有一定的距离，距离大小为MO∣FR′∣\\frac{M_O}{|F_R&#x27;|}∣FR′​∣MO​​。 其实主矩MO=FRdM_O = F_RdMO​=FR​d，FR=FR′=∑FiF_R = F_R&#x27; = \\sum F_iFR​=FR′​=∑Fi​，这阐述的就是合力矩定理，合力对某点的矩，等于所有分力对这一点矩的和。MO(FR)=MO=∑MO(Fi)M_O(F_R) = M_O = \\sum M_O(F_i)MO​(FR​)=MO​=∑MO​(Fi​)。 也就是说该情况简化为了作用线不过简化中心的合力。 第四种情况：一个平面任意力系向一点简化得到主矢FR′F_R&#x27;FR′​和主矩MOM_OMO​，当主矢FR′=0F_R&#x27; =0FR′​=0而MO=0M_O ={0}MO​=0，这表明了不需要任何力或者是力偶来与这个力系等效，表明这个力系自身就是平衡的，既然平衡的话，与简化中心的位置无关。 因此，平面任意力系简化的最后结果只能是合力、合力偶、平衡三种情况。 总结 FR′=0,MO=0F_R&#x27;=0, M_O \\not ={0}FR′​=0,MO​​=0 FR′=0,MO=0F_R&#x27; \\not =0, M_O = 0FR′​​=0,MO​=0 FR′=0,MO=0F_R&#x27; \\not =0, M_O\\not ={0}FR′​​=0,MO​​=0 FR′=0,MO=0F_R&#x27;=0, M_O = 0FR′​=0,MO​=0 结果 合力偶 合力 合力 平衡 特点 该力系向任意一点简化，所得到的力系仍然是相同的力系（力偶系） 作用线过简化中心，也就是FR′F_R&#x27;FR′​可以沿着它的作用线平移，不改变作用效果 作用线不过简化中心的合力，作用线至简化中心有一定的距离，距离大小为MOabs(FR′)\\frac{M_O}{abs(F_R&#x27;)}abs(FR′​)MO​​ 表明了不需要任何力或者是力偶来与这个力系等效，这个力系自身就是平衡的 简化中心 与简化中心无关 作用线过简化中心 作用线距简化中心有一定距离 与简化中心无关 解： 建如图所示坐标系，向O点简化。 通过前面的分析我们知道，并不是一个任意力系向任意一点简化都能得到一个合力，只是在某些特殊的位置，结果才可能是一个合力。目前我们还不知道具体是哪一点，但是为了简单，我们可以首先将这个力系向O点简化，得到向这一点转化的主矢和主矩。 力系中F2F_2F2​比较特殊，首先我们确定F2F_2F2​与xxx轴的夹角θ\\thetaθ，由几何关系可知 θ=∠ACB=arctanABBC=16.7°\\theta = \\angle ACB = arctan\\frac{AB}{BC} = 16.7°θ=∠ACB=arctanBCAB​=16.7° 主矢的计算方法，先分别计算主矢在xxx轴和yyy轴上的投影 FRx′=∑Fix=F1−F2cosθ=232.9kNF_{Rx}&#x27; = \\sum F_{ix} = F_1 - F_2cos\\theta = 232.9kNFRx′​=∑Fix​=F1​−F2​cosθ=232.9kN FRy′=∑Fiy=−P1−P2−F2sinθ=−670.1kNF_{Ry}&#x27; = \\sum F_{iy} = -P_1 - P_2 - F_2sin\\theta = -670.1kNFRy′​=∑Fiy​=−P1​−P2​−F2​sinθ=−670.1kN 主矢FR′F_R&#x27;FR′​的大小为：FR′=(∑Fix)2+(∑Fiy)2=709.4kNF_R&#x27; = \\sqrt{(\\sum F_{ix})^2 + (\\sum F_{iy})^2} = 709.4kNFR′​=(∑Fix​)2+(∑Fiy​)2​=709.4kN 主矢的方向余弦分别算得： cos(F⃗R′,i⃗)=∑FixFR′=0.3283cos(\\vec F_R&#x27;, \\vec i) = \\frac{\\sum F_ix}{F_R&#x27;} = 0.3283cos(FR′​,i)=FR′​∑Fi​x​=0.3283 cos(F⃗R′,j⃗)=∑FiyFR′=−0.9446cos(\\vec F_R&#x27;, \\vec j) = \\frac{\\sum F_iy}{F_R&#x27;} = -0.9446cos(FR′​,j​)=FR′​∑Fi​y​=−0.9446 得到主矢与xxx轴正向夹角为α=70.84°\\alpha = 70.84°α=70.84°，主矢与y轴负向的夹角为β=19.16°\\beta = 19.16°β=19.16° 注意：在求力矩的时候，就要先看一个力绕作用点是顺时针还是逆时针，顺时针就是负的，逆时针就是正的。 主矩的大小 MO=∑MO(Fˉ)=−3F1−1.5P1−3.9P2=−2355kN⋅mM_O = \\sum M_O(\\bar F) = -3F_1-1.5P_1-3.9P_2 = -2355kN·mMO​=∑MO​(Fˉ)=−3F1​−1.5P1​−3.9P2​=−2355kN⋅m 主矩为负值，表明主矩是顺时针的，这样力系向O点简化的主矢和主矩都求出来了。 2. 求合力及作用线位置 向O点简化得到的主矢和主矩可以进一步简化得到一个合力。这个合力的大小、方向与主矢是相同的，合力的作用线到O点的距离为 d=∣MO∣FR′=2355709.4=3.3197md = \\frac{|M_O|}{F_R&#x27;} = \\frac{2355}{709.4} = 3.3197 md=FR′​∣MO​∣​=709.42355​=3.3197m 那么合力的作用线到O点的距离xxx由几何关系很容易得到 x=dcos(90°−70.84°)=3.514mx = \\frac{d}{cos(90°-70.84°)} = 3.514 mx=cos(90°−70.84°)d​=3.514m 3. 求合力作用线方程 合力显然可以沿着它的作用线上下移动，不管作用在哪个点，它的效果是一样的，它的力到O点的矩总是保持不变的，等于力系向O点简化的主矩，因此可由这个关系列出合力作用线方程 MO=∑MO(Fˉ)=x⋅FRy−y⋅FRx=x⋅FRy′−y⋅FRx′M_O = \\sum M_O(\\bar F) = x·F_{Ry} - y·F_{Rx} = x·F{Ry}&#x27;-y·F{Rx}&#x27;MO​=∑MO​(Fˉ)=x⋅FRy​−y⋅FRx​=x⋅FRy′−y⋅FRx′ 即−2355=x(−670.1)−y(232.9)-2355 = x(-670.1) - y(232.9)−2355=x(−670.1)−y(232.9) 有670.1x+232.9y−2355=0670.1x+232.9y-2355=0670.1x+232.9y−2355=0 平面任意力系的平衡条件和平衡方程 如果刚体在一个平面任意力系的作用下平衡的话，平衡的条件是什么呢？ 平面任意力系的平衡方程 由上一节平面任意力系的简化结果我们知道，平面任意力系的简化结果是一个主矢和一个主矩，但主矢和主矩同时为0时，这个力系平衡，并且与简化中心无关。也就是说力系平衡的话，向任意一点简化，都是相同的结果。 因此平面任意力系平衡的充要条件：力系的主矢和对任意点的主矩都等于零。 ","link":"https://albertlidesign.github.io/post/TheoreticalMechanics5/"},{"title":"理论力学 第四节 平面汇交力系和平面力偶系","content":"第四节 平面汇交力系和平面力偶系 平面汇交力系合成与平衡的几何法 所谓几何法就是几何作图的方法，所以这种方法也成为图解法。在学习这个方法之前首先要了解汇交力系 汇交力系 各力的作用线都汇交于一点的力系。可分为空间汇交力系和平面汇交力系。 合力 多个力的作用线汇交于一点，如果能用一个力来等效替换，此力称为合力。简言之，如果一个力与某一个力系等效，则称此力为该力系的合力。 那么怎么来求汇交力系的合力呢？ 两个共点力的合成——力三角形规则 我们知道使用力的平行四边形定则可以求出合力的大小和方向，那么如何用力三角形规则来求呢？ 力是一个矢量，如果人为规定，向量的起点为力首，重点为力尾的话，那么这个线段就有了首尾之分。 将第一个力矢量画出来以后，将第二个力矢量平移出来，将第二个力的力首与第一个力的力尾首尾相连，然后从第一个力的力首指向第二个力的力首，就是合力的矢量FRF_RFR​。规则里，第一个力和第二个力并不存在顺序问题。完全可以先画出第二个力，再画第一个力。 注意：力三角形规则求出的是合力的大小与方向，作用点仍在交汇点。 多个汇交力的合成——力多边形规则 假设在物体上有一个汇交力系，从F1F_1F1​到FNF_NFN​，它们的作用线汇交于O点，现在我们用平移定理，将所有的力都平移到O点，而不改变这个力系的作用效果。这样一来，这个汇交力系就变成了共点力系，对这个汇交力系的合成就是对共点力系的合成。 利用力的三角形规则 首先得到FR1=F1+F2F_{R1} = F_1+F_2FR1​=F1​+F2​; 接着计算FR2=F1+F2+F3=FR1+F3F_{R2} = F_1+F_2+F_3 = F_{R1} + F3FR2​=F1​+F2​+F3​=FR1​+F3 ....... 依次进行下去，就可以得到所有力的合力，把所有中间过程省略掉，就可以发现，合力的矢量其实就是从第一个力的力首指向最后一个力的力尾的连线。FR=∑i=1n=∑FiF_R = \\sum_{i=1}^n = \\sum F_iFR​=∑i=1n​=∑Fi​ 这样求汇交力系的方法就被称为力多边形规则，这样一个多边形就被称为力多边形。 如果将这个多边形其中的任意几个力交换顺序，最后得到的合力不变。也就是说，对于一个汇交力多边形来说，它的力多边形不唯一。 注意：由力多边形规则求出的是合力的大小与方向，作用点仍在交汇点。 汇交力系平衡的几何条件 我们知道一个汇交力系可以与一个力等效，这个力就是汇交力系的合力，所以汇交力系的平衡条件就是，这个力的合力矢量为0，即FR=∑Fi=0F_R = \\sum F_i =0FR​=∑Fi​=0。 也就是说，汇交力系平衡的充要条件是：该力系的力多边形是自行封闭的（汇交力系平衡的几何条件）。 假设力系的力多边形不自行封闭的话，那么第一个力的力首和最后一个力的力尾间是有间隙的，那么合力即为从第一个力的力首指向最后一个力的力尾，FR=0F_R \\not ={} 0FR​​=0。如果没有间隙，那么合力FR=0F_R =0FR​=0。 注意：力多边形规则求合力和平衡的几何条件适用于任意汇交力系（平面及空间）。 求解实际问题 CD杆是一个二力杆，以AB杆为研究对象，画受力图 利用汇交力系，利用平衡的几何条件，画封闭的力三角形 最后我们可以根据比例尺量出或根据三角函数计算出FAF_AFA​和FCF_CFC​的大小，得FA=22.4N，FC=28.3NF_A = 22.4N ， F_C = 28.3NFA​=22.4N，FC​=28.3N。 因此我们发现，利用几何法来分析起问题来，很直观也很简单，但是计算不是很方便，因此在实际问题中，这种方法应用不是很多。 平面汇交力系合成与平衡的解析法 力在直角坐标系上的投影和力沿轴的分解 假设有一个平面力FFF，大小、方向、作用点确定，现在我们建立这样一个平面直角坐标系，与坐标轴xxx的夹角为θ\\thetaθ，将FFF向xxx轴做投影得到FxF_xFx​，我们知道Fx=F⋅cosθF_x = F · cos\\thetaFx​=F⋅cosθ。类似地，我们可以得到它在yyy轴的投影，得到Fy=F⋅sinθF_y = F·sin\\thetaFy​=F⋅sinθ。这就是力矢量在直角坐标系的投影。 同时也由力的平行四边形法则，还可以将力FFF写成两个分力的矢量和，即F=Fx+Fy=Fxi+FyjF = F_x+F_y = F_xi+F_yjF=Fx​+Fy​=Fx​i+Fy​j，这就是力沿直角坐标轴的分解。 注意：力在坐标轴上的投影一般不等于力沿着该坐标轴的分解，但是直角坐标系下二者是相等的。所以一般的静力学问题大多选择在直角坐标系下进行分析。 平面汇交力系合成的解析法 我们利用合矢量的投影定理来得到合力的投影定理。我们知道合矢量的投影定理说的是有限个矢量的和，在坐标轴上的投影等于所有矢量各自在这个轴上的投影的和。 FR=∑FiF_R=\\sum F_iFR​=∑Fi​ 也就是说 FRx=∑FixF_{Rx} = \\sum F_{ix}FRx​=∑Fix​ FRy=∑FiyF_{Ry} = \\sum F_{iy}FRy​=∑Fiy​ 合力的大小为：FR=FRx2+FRy2F_R = \\sqrt {F_{Rx}^2+F_{Ry}^2}FR​=FRx2​+FRy2​​ 合力方向的余弦值分别为：cos(FR,i)=∑FixFRcos(FR,j)=∑FiyFRcos(F_R,i) = \\frac{\\sum F_{ix}}{F_R} \\quad cos(F_R, j) = \\frac{\\sum F_{iy}}{F_R}cos(FR​,i)=FR​∑Fix​​cos(FR​,j)=FR​∑Fiy​​ 作用点为力的交汇点 平面汇交力系平衡的解析条件（平衡方程） 那么如果一个平面汇交力系平衡的话，它所满足的解析条件是什么？ 我们前面学过，一个平面汇交力系平衡的几何条件是力多边形自行封闭。 我们知道平面汇交力系与它的合力是等效的，这个力系平衡的话也就意味着它们的合力为000，也就是说 FR=∑Fi=0F_R = \\sum F_i = 0FR​=∑Fi​=0 也就意味着这个合力在两个坐标系上的分量为000，即 FRx=∑Fix=0F_{Rx} = \\sum F_{ix} = 0FRx​=∑Fix​=0 FRy=∑Fiy=0F_{Ry} = \\sum F_{iy} = 0FRy​=∑Fiy​=0 一个平衡力系可以列出两个独立的平衡方程，可以求解两个独立的未知量。 求解实际问题 解： 取研究对象。AB、BC两杆都是二力杆。通常将二力杆看成是一种约束，其受力图可以不画。假设AB杆受拉力，BC杆受压力。取滑轮B为研究对象，画受力图。 我们知道滑轮B用绳索悬吊着重物P，因此可知F1=F2=PF_1=F_2 = PF1​=F2​=P 列平衡方程求解。选取合适的坐标系如图所示，列平衡方程为： ∑Fx=−FBA+F1⋅cos60°−F2⋅cos30°=0\\sum F_x =-F_{BA} + F_1·cos60°-F_2·cos30° =0∑Fx​=−FBA​+F1​⋅cos60°−F2​⋅cos30°=0 ∑Fy=FBC−F1⋅cos30°−F2⋅cos60°=0\\sum F_y =F_{BC} - F_1·cos30°-F_2·cos60° =0∑Fy​=FBC​−F1​⋅cos30°−F2​⋅cos60°=0 解得FBA=−7.321kN;FBC=27.32kNF_{BA} = -7.321kN; F_{BC} = 27.32kNFBA​=−7.321kN;FBC​=27.32kN FBAF_{BA}FBA​为负值，表示BA杆的受力方向与假设相反，即BA杆实际受压力；FBCF_BCFB​C为正值，表示BC杆的受力方向与假设相同，即BC杆实际也受压力。 平面力对点的矩和平面力偶 平面力对点的矩 我们知道力可以使点发生移动，也可以使点发生转动，那么对这个转动效果的度量就是力矩。力矩不仅与力的大小有关，还与力到作用点的距离有关。 作用在平面上的一个力FFF，如果将物体在OOO点固定，那么力可以使物体绕OOO点转动，这就是力FFF对OOO点的矩，在这里点OOO称为矩心，而力的作用线与点OOO所确定的平面，称为力矩的作用面，也就是说OOO与力矢量的首尾确定的平面称为力矩作用面，而OOO到力的作用线的垂直距离hhh称为力臂。 力FFF使物体绕OOO点的转动效果，完全由两个要素来决定： 大小：力FFF与力臂hhh的乘积F⋅hF·hF⋅h 转向：使物体绕OOO点转动方向 用数学公式表达： $M_o(F) = ±F⋅h\\pm F·h±F⋅h 所以平面内力对点的矩是一个代数量，它的绝对值等于力的大小与力臂的乘积，它的正负：力使物体绕矩心逆时针转向时为正，反之为负。常用单位N.m或kN.m 合力矩定理与力矩的解析表达式 我们知道汇交力系有合力，合力矩定理就是说，这个汇交力系的合力对平面内任意一点的矩等于各个分力对这一点的矩的代数和。它提供了计算多个力对一点矩的简单方法，只需要计算它们的合力对这一点的矩就可以了，即Mo(FR)=∑Mo(Fi)M_o(F_R) = \\sum M_o(F_i)Mo​(FR​)=∑Mo​(Fi​)。该结论适用于任何存在合力的力系。 假设平面内有一作用在A点的力FFF，计算它对坐标系原点OOO点的矩。 计算力FFF对OOO点的矩其实就是计算它在xxx轴和yyy轴的两个分量分别对OOO点的矩。 力矩的解析表达式： Mo(F)=Mo(Fy)−Mo(Fx)=x⋅F⋅sinθ−y⋅F⋅cosθM_o(F) = M_o(F_y) - M_o(F_x) = x·F·sin \\theta - y·F·cos \\thetaMo​(F)=Mo​(Fy​)−Mo​(Fx​)=x⋅F⋅sinθ−y⋅F⋅cosθ = xF_y - y_F_x 如果有多个汇交力，则： Mo(FR)=∑Mo(Fi)=∑(xi⋅Fiy−yi⋅Fix)M_o(F_R) = \\sum M_o(F_i) = \\sum (x_i·F_{iy} - y_i·F{ix})Mo​(FR​)=∑Mo​(Fi​)=∑(xi​⋅Fiy​−yi​⋅Fix) 力偶 除了力矩以外，还有一类特殊的力系能够使物体产生转动效果，这就是力偶。什么是力偶呢？拧水龙头的时候，两根手指作用在开关上的力，它们是大小相等、方向相反，但是又不在同一条直线上的力。开车的时候，双手抓住方向盘，也是大小相等、方向相反，但是不在同一条直线上的两个力。 由两个大小相等（等值）、方向相反（反向）、不共线（平行）力组成的力系称为力偶，记作(F,F′)(F,F&#x27;)(F,F′)。力偶能对物体产生纯转动的效果。 两个力的两个作用线可以确定一个平面，称作力偶的作用面 两个力之间的距离称为力偶臂 需要注意的是 力偶是力系但不是平衡力系 力偶没有合力，不能用一个力来等效替换，也不能用一个力来平衡 力偶和力一样，是力学中的一个基本要素，不能再简化 力偶矩 力偶矩是对力偶使物体转动效果的度量。 平面力偶矩是一个代数量，其绝对值等于力的大小与力偶臂的乘积，正负号表示力偶的转向：一般以逆时针转向为正，顺时针转向为负。单位N·m，大小也可以用三角形（平行四边形）的面积表示。 M=±F⋅d=2A△ABCM = \\pm F·d = 2A_{\\triangle ABC}M=±F⋅d=2A△ABC​ 大小：力与力偶臂的乘积 转向：作用面内的转动方向 同平面内力偶的等效定理 在同一个平面内的两个力偶，如果力偶矩相等，则两力偶彼此相等。 推论1 力偶可在其作用面内任意移转，而不改变它对刚体的作用。力偶对刚体的作用与力偶在其作用面内的位置无关 推论2 只要保持力偶矩的大小和力偶的转向不变，可以同时改变力偶中力的大小与力偶臂的长短，对刚体的作用效果不变。 力偶矩是平面力偶作用的唯一度量。也就是说，我们不关心它具体对应的力是什么样子的，而使用力偶矩的符号来表示力偶。 平面力偶系的合成和平衡条件 当有多个平面力偶，也就是说有一个平面力偶系得时候，能否像对汇交平面力系一样对平面力偶进行合成呢？如果能，结果会是什么呢? 平面力偶系得合成 已知：M1,M2,...,MnM_1,M_2,...,M_nM1​,M2​,...,Mn​。 我们可以将这n个力偶矩，根据同平面内力偶的等效定理，用具有相同力偶臂长度的一系列具体的力偶来表示。 力偶臂长度d如果确定的话，则力F1=M1dF_1 = \\frac{M_1}{d}F1​=dM1​​，类似地，F2=M2dF_2 = \\frac{M_2}{d}F2​=dM2​​，Fn=MndF_n = \\frac{M_n}{d}Fn​=dMn​​。这样一来这n个力偶就变成了两个共点力系，我们可以合成为两个合力FFF和F′F&#x27;F′，则有F=F1+F2+...+FnF = F_1+F_2+...+F_nF=F1​+F2​+...+Fn​和F′=F1′+F2′+...+Fn′F&#x27; = F_1&#x27; + F_2&#x27; +...+F_n&#x27;F′=F1′​+F2′​+...+Fn′​ 我们可以用MMM来表示，则M=Fd=∑Fid=∑MiM = Fd = \\sum F_id = \\sum M_iM=Fd=∑Fi​d=∑Mi​。 通过这样的分析过程我们可以看到，一个平面任意力偶系，可以合称为一个合力偶，合力偶的矩等于各力偶矩的代数和。 平面力偶系的平衡条件 平面力偶系平衡的充要条件：M=0M = 0M=0，或各力偶系的代数和等于零，∑Mi=0\\sum M_i = 0∑Mi​=0。 求解实际问题 解： 取圆轮为研究对象，分析受力。 圆轮上作用有一个主动力M1M_1M1​，然后分析约束力，在A点受到一个遥感约束力，这是一个光滑接触约束，因此FAF_AFA​是垂直于摇杆方向的，所以说它与OA呈夹角为θ\\thetaθ。O点的约束力是一个光滑铰链约束，本质上是一个约束力FOF_OFO​，那么它的方向指向哪呢？由前面我们讲到，力偶只能由力偶来平衡，因此FOF_OFO​必须与FAF_AFA​大小相等，方向相反，它们是相互平行的，构成了一组力偶，即(FA,FO)(F_A,F_O)(FA​,FO​)与MAM_AMA​是平衡的。 由平衡条件∑Mi=0\\sum M_i = 0∑Mi​=0，得 M1−FArsin⋅θ=0M1 -F_Arsin·\\theta =0M1−FA​rsin⋅θ=0 解得FO=FA=M1rsin⋅30°=8kNF_O = F_A = \\frac{M_1}{rsin·30°} = 8kNFO​=FA​=rsin⋅30°M1​​=8kN 铰链在O处的约束力也就计算出来了。 取遥感为研究对象，分析受力。 首先遥感上有一个主动力M2M_2M2​，A点受到了销子给它的作用力FA′F_A&#x27;FA′​，方向垂直于遥感。B点的约束力我们知道，B点为一个光滑铰链约束，所以B点是一个力，FBF_BFB​，由力偶只能由力偶来平衡我们知道，FBF_BFB​和FA′F_A&#x27;FA′​必须构成一个力偶才能够与M2M_2M2​来平衡，所以FBF_BFB​和FA′F_A&#x27;FA′​大小相等，方向相反。 由平衡条件∑Mi=0\\sum M_i = 0∑Mi​=0，得 −M2−FA′rsinθ=0-M2 -F_A&#x27;\\frac{r}{sin\\theta} =0−M2−FA′​sinθr​=0 解得M2=8kN⋅m,FO=FB=FA=8kNM_2 = 8kN·m, F_O = F_B = F_A = 8kNM2​=8kN⋅m,FO​=FB​=FA​=8kN 因此，力偶只能由力偶来平衡，这是做题中隐含的已知条件。 课后习题及解析 对于楔块来说，D点压力FD（垂直于斜面AC）和E点支持力FE的交点，在底面上的投影必须在底面AB上，这样地面支持力才能与这两个力满足三力平衡汇交定理。因为地面支持力，是由地面提供，必须位于地面AB上。如果超出AB，则三力无法汇交于一点，故不能达成平衡。 三个力的方向均已知，平衡的话需满足三力平衡汇交定理。 注意题目中没说不计重力要考虑重力，光滑接触面约束的约束力的方向是垂直于公切面。 ","link":"https://albertlidesign.github.io/post/TheoreticalMechanics4/"},{"title":"理论力学 第三节 物体的受力分析和受力图","content":"第三节 物体的受力分析和受力图 3.1 受力分析和受力图 受力分析 简单地说就是分析物体的受力情况，确定物体受到哪些力，各个力的作用点在哪？作用方向是什么？是一种定型的分析。 为什么要进行受力分析 只有先定性地给出物体地受力情况，才能定量地求解各力的大小，然后才能解决其他问题。没有受力分析，求解静力学问题将会无从下手。 受力图 把物体所受到的所有力（所有的主动力和约束反力）以一种简明的图形画出来，称为画物体的受力图。 取分离体 为了把结果清晰地显示出来，把要研究的那个物体从周围的物体中分离出来，单独画它的简图，这个步骤叫做取分离体，或者叫做取研究对象，画出来的这个简图称为分离体图。 画受力图步骤 应该说画受力图没有固定的方法，但是如果遵循下面画受力图的步骤，会使画受力图变得清晰简单些： 取所要研究物体为研究对象（分离体），画出其简图 画出所有主动力 按约束性质画出所有约束（被动）力，不要多画力，也不要少画力。 解： 画出简图 画出所有主动力 （1）自身的重力PPP，作用点作用在碾子的中心处，方向竖直向下。 （2）斜向上的拉力FFF，也是作用在碾子的中心处。 依据约束的特点，画出所有约束力 （1）B点的约束是一个光滑接触约束，它的约束力作用在接触点B，沿着公法线方向指向碾子中心。 （2）A点的约束也是一个光滑接触约束，它的约束力作用在接触点A，沿着公法线方向指向碾子中心。 注意：因为A处约束是固定铰链支座与地面固定在一起，所以约束力为两个正交分力。C点是通过光滑圆柱铰链与右边连接在一起，因此也是两个正交分力。 注意：在画BC拱时，C点是带着销钉的。B处约束是固定铰链支座与地面固定在一起，所以约束力为两个正交分力。C点是受到AC拱对销钉的作用力，它与AC拱对C点的受力是一对作用力和反作用力。 注意：在以整体为研究对象时，将AC拱和BC拱所受到的力分别画出，会发现在C点处是两组相互平衡的作用和反作用力，其实C点的受力在整体的结构中它是内力，此时不画。 所研究的系统内，物体与物体未分离处相互作用的力称为内力。 内力一律不画在受力图上，绝对禁止画在受力图上。 所以整体的受力图应该这么画： 画受力图是静力学求解中非常关键的一步，如果受力图画错，后面的计算都是没有意义的，除了多加练习以外没有其他方法。 3.2 二力构件 工程中有一种很常见也很重要的构件叫做二力构件，工程中画受力图的时候要特别注意这类构件的存在。我们结合这样一个例子来说明二力构件的结构特点。 解： 取CD杆为研究对象，画出简图 画出所有主动力 依据约束性质画出所有约束力 那么这么画对不对呢？有前面关于约束的特点我们指导，D点为光滑圆柱铰链约束，本质上是光滑接触约束，本质上是一个力，C点是固定铰链约束，本质上也是一个力。因此CD杆其实处于两个力的作用下。由二力平衡公理可知，刚体上作用有两个力，使刚体保持平衡的充要条件是：这两个力大小相等，方向相反，且作用在同一直线上。因此这两个力等值、反向、共线。因此CD杆的受力是这样的，CD杆就是二力构件 二力构件 只在两个力作用下平衡的构件，称为二力构件；若构件为直杆或弯杆，则成为二力杆。 注意：一个构件是否为二力构件仅与它的受力有关，而与它的形状无关。 二力杆（二力构件）的受力特点：两个力必定沿着两个力作用点的连线，且大小相等，方向相反。 对于直杆来说，两个力都是沿着杆的方向，大小相等方向相反，可能是压力也可能是拉力；对于弯杆来说，无论形状如何，两个力的方向必定在两个受力点之间的连线上，大小相等方向相反；对于任何形状的构件来说，只要在两个力的作用下平衡，两个力的方向都在两个受力点之间的连线上，大小相等方向相反。所以对于二力构件来说，它只有一个未知约束力，知道了一个，另一个也就知道了。 回到上述示例 2. 取AB梁为研究对象，画出简图 画出所有主动力 依据约束性质画出所有约束力 A点受到的是固定铰链支座约束，所以它的约束力可以表示为两个正交分力 D点处的受力其实是CD杆给D点处销钉的作用力，它和CD杆在D点处的受力是作用力与反作用力的关系。 CD杆的受力图也可以画成这样，上面CD杆的受力图是受压力，现在改成了受拉力 那么此时AB梁的受力图应该这样画，D点处也是受拉力 注意：通过这样一个例子我们可以看到，在画受力图的时候，不一定要给出真实的受力方向。因为在实际情况下，真实的受力方向有时很难判断，我们给出的都是假定的受力方向，真实的方向需要根据具体的计算结果来得到。 练习 请判断下列图形中，哪些是二力构件？并画出杆上D点的受力方向 详解图4： 这个结构包含有三根杆，首先可以看出DB杆是一个二力杆，因为它一端通过固定铰链支座与地面连接在一起，另外一端为光滑圆柱铰链约束，这两个约束本质上都是光滑接触约束，所以DB杆是一个二力杆。 类似地，AB杆也是一个二力杆。 再来看看AC杆，在A点是固定铰链支座约束，本质上是一个光滑接触约束，是一个力，那么C点的约束可以从两个方面来考虑，一方面，如果把杆和销钉分开的话，C点其实是受到来自销钉的作用力，这是一个光滑接触约束，本质上是一个力，另外一方面，如果把销钉和杆在一起考虑的话，C点的作用力其实是作用在销钉上的两段绳子的拉力，这是两个共点力，可以合成为一个力，所以也可以看作是一个力，所以AC杆也是一个二力杆。 3.3 画受力图练习 我们通过两个例子来练习画受力图，需要着重注意画受力图的技巧以及一些注意事项 首先我们来分析BC的受力，画出BC的受力图，取BC为研究对象，画出简图 画出BC拱的主动力 根据约束地性质画出所有约束力：B点为固定铰链支座约束，C点为光滑圆柱铰链约束，所以BC拱其实受到两个作用力，因此可以看作是一个二力构件，因此B点的约束力和C点的约束力应该是沿着两点的沿线方向，两个力大小相等方向相反。 我们再来看看AC拱的受力，取AC拱为研究对象，画出简图 画出AC拱的主动力，主动力只有一个集中力FFF。 根据约束地性质画出所有约束力：C点的约束力其实是销钉受到的来自BC拱的反作用力，它和BC拱在C点处受到的力是一对作用力和反作用力。A点的约束力按照约束的性质的话，它是一个固定铰链支座约束，应该画成两个正交的分力，但是它本质上是一个力，所以AC拱其实是受到三个力，达成平衡状态，我们根据三力平衡汇交定理知道，其中两个力FFF和FC′F&#x27;_CFC′​汇交于一点，那么第三个力也汇交与这一点，并且这三个力在同一个平面上，所以A点的作用方向其实是确定的。 取整体为研究对象，画出简图 画出所有主动力，主动力只有一个集中力FFF。 根据约束地性质画出所有约束力：约束力在B点的约束力方向是已知的，A点的约束力方向也是已知的，所以对整个整体的结构来说，它也是受到了三个力的作用来达到平衡，它也满足三力平衡汇交定理。 取BC杆为研究对象，销钉置于AC杆上，画出简图 画出所有主动力，因为不考虑杆的自重，并且销钉在AC杆上，所以没有主动力。 根据约束地性质画出所有约束力：B、C点都是光滑铰链约束，所以BC杆是一个二力杆。受力图如下 取AC杆为研究对象，画出简图 画出所有主动力，AC杆上有销钉，它受到主动力FFF。 依据约束性质画出所有约束力：C点受到的约束力是来自BC杆的反作用力，它和BC杆在C点处受到的力是一对作用力和反作用力，是大小相等方向相反的。通过分析C点的受力我们知道，AC杆其实也是一个二力杆，它应该也是沿着杆的方向。有的同学可能有疑问：C点的受力也是沿着杆的方向吗？是的，因为C点受到的两个力的合力是沿着杆的方向的。这个合力与A点受到的力FAF_AFA​是大小相等方向相反的，所以说AC杆仍然是二力杆。 取整体为研究对象，画处出简图 画出所有主动力，C点处销钉受到主动力FFF。 依据约束性质画出所有约束力。 如果取AC，BC杆为研究对象，且销钉与两杆分离的话，则： 3.4 力学模型和力学简图 理论力学所研究的问题都是从实际问题中抽象出来的模型，所面对的图形都是力学简图，如何实现从一个实际问题到力学简化模型的过程呢？涉及到力学建模，这一节介绍力学建模的过程。 力学建模 将实际问题抽象为力学模型的过程。对任何实际问题进行力学分析、计算时都要将实际问题抽象为力学模型，然后对力学模型进行分析、计算。任何力学计算实际上都是针对力学模型进行的。将实际问题转化为力学模型是进行力学计算所必须的、重要的、关键的一环，将直接影响计算过程和结果。 从这些例子可以看出，力学建模的过程也是一个简化的过程，不仅是对结构的简化，也是对载荷、约束等等多方面的简化，那么简化的原则是什么？其实这也是力学建模的原则 力学建模的原则 抓住关键、本质因素、忽略次要因素 多方面进行抽象化处理。 非均匀材料假设成均匀材料，比如说碾子的材制不可能完全均匀。 结构有变形但抽象成刚体，比如碾子在受力中会有变形，但是对于我们所研究的问题影响很小，我们认为它们都是刚体。 三维物体有时处理成二维，比如碾子的轴向几乎不受力，因此可以不考虑轴向的尺寸，把它处理成二维的圆盘。 把复杂形状简化为简单形状，比如碾子截面的形状不可能是标准的圆形，我们将它处理成光滑的圆盘，这对实际的影响不大。 载荷简化成集中力或分布力，比如碾子受到的拉力作用在中心处，它的作用面是一个面，但是因为面非常小，因此简化为集中力。类似地，如果力的作用范围是作用在一个面上，或者作用在一条线上，我们可以简化为分布载荷，如果差别不大的话，我们还可以进一步简化为均布载荷。 很多约束简化为理想约束，比如碾子在B点和A点都是存在摩擦力的，但是我们简化为光滑接触。 通过这样的简化，一个复杂的工程实际问题就会变得相对简单。但是所产生的误差又是在可接受的范围之内，如果不进行简化，那么大部分的工程实际问题都是无法求解的，所以这样的简化、建模的过程都是非常重要的。 这就是力学建模的概念和原则，将力学模型画出来就是力学简图。 力学简图 将力学模型用简单明了的图形来表示，这类图形称之为力学简图。 需要注意的是，很多实际结构，在简化后可能得到的是同一个力学模型，也就是说，同样一个力学简图可能对应多种实际结构。 比如这是力学结构中非常常见的一种结构叫做简支梁，梁的一端通过铰链固定在地面上，另外一端通过滚动支座可以在地面上自由地伸缩。它对应的实际结构可以是桥梁中的桥箱，也可能对应的是屋顶的一根大梁。 一个实际结构也可能简化成不同的力学模型。 一个拱桥可以简化成三铰拱桥结构，有时候简化为二铰拱桥结构。 课后习题及解析 如图所示结构中，不计各杆的自重，请问AB杆的受力，画出受力分析图 首先分析CD杆不带销钉，CD杆为二力杆。 接着分析AB杆，C处带销钉，画出简图后，画出主动力FFF，由于CD为二力杆，C处受到的约束力为CD杆在C处作用力的反作用力，A处为固定支座约束，只有一个约束力，因此AB杆仅受三个力达到平衡，根据三力平衡汇交定理即可求出三个力的方向。 如图所示结构中，不计各杆的自重，绘制整体结构的受力分析图 设AB杆和CD杆中间的约束为T。首先分析CD杆，CD杆的主动力为FFF，约束T可以视作是光滑接触面约束，约束力方向垂直于AB杆指向右上方，C约束为光滑圆柱铰链约束，因此根据三力平衡汇交定理可以求出C处的约束力方向。 接着分析AO杆，尽管C处的约束力可以求出，但是A点的受力方向无法确定，无法利用三力平衡汇交定理确定O点受力方向，因此O点处的受力需要用两个分量来表示。 同理，对于AB杆，由于A点受力方向也无法确定，因此B点受力方向也无法确定，应用两个分量来表示。 C、T、A处的受力均为内力，不需要绘出。 图示结构中，F1作用在D点的销钉上，画出CD的受力图 D点连着销钉时： 首先分析AB杆，A处为固定铰支座约束，B处为光滑圆柱铰链约束，各有一个约束力，因此AB杆为二力杆，受力方向沿着AB所在直线。 接着分析BCD杆，首先绘出主动力F1F_1F1​和ED杆的反作用力，因为ED杆在D点的受力方向未知，用两个正交分力表示。 C处为滚动支座约束，构件受到垂直于光滑面的约束力FNCF_{NC}FNC​，B处连接二力杆AB，因此B处受到的约束力为AB杆在B处作用力的反作用力FBF_BFB​。D点的F1、FDx、FDy这三个力的和力与FB和FNC满足三力平衡汇交定理。 D点不带销钉时： 首先分析AB杆，A处为固定铰支座约束，B处为光滑圆柱铰链约束，各有一个约束力，因此AB杆为二力杆，受力方向沿着AB所在直线。 接着分析BCD杆，D点不带销钉，因此不体现主动力F1F_1F1​。 C处为滚动支座约束，构件受到垂直于光滑面的约束力FNCF_{NC}FNC​，B处连接二力杆AB，因此B处受到的约束力为AB杆在B处作用力的反作用力FBF_BFB​。因此可由三力平衡汇交定理确定D处受到另一端杆件的反作用力FDF_DFD​。 图示结构中，绘制AB杆的受力图 不考虑CB杆的受力情况，单独考虑AB杆，不带销钉时，AB杆在AB两点的受力方向皆不可知，均用用两个正交分力表示。 带着销钉，不考虑CB杆的受力情况，单独考虑AB杆的话，AB杆在B两点的受力方向不可知，A点为滚动支座约束，受力为销钉所受的地面支持力FNAF_{NA}FNA​和二力杆AD的作用力FDAF_{DA}FDA​。 带着销钉，考虑CB杆的受力情况的话，CB杆在C点的受力可通过整体由三力平衡汇交定理得出，由此B点的受力在CB杆上通过三力平衡汇交定理得出。故AB杆在B点的受力方向可确定。A点受力为销钉所受的地面支持力FNAF_{NA}FNA​和二力杆AD的作用力FDAF_{DA}FDA​。 ","link":"https://albertlidesign.github.io/post/TheoreticalMechanics3/"},{"title":"理论力学 第二节 静力学公理及常见约束","content":"第二节 静力学公理及常见约束 2.1 静力学引言 建筑应用 分析事故原因 学习静力学的目的 直接解决工程实际问题； 为后续课程打基础，例如材料力学、结构力学、弹性力学、建筑力学、机械设计等。 解决工程应用中的实际问题 静力学中的几个基本概念 刚体：在力的作用下，其内部任意两点间的距离始终保持不变的物体。 例如用手推动桌子时，桌子自身结构会发生微小变形，但是变形量对于我们研究的问题过小，所以这个桌子可以被视为刚体。 一个物体能否被视为刚体，与物体本身的绝对刚度没有关系，与我们所研究的问题有关。 例如一个网球，如果我们研究它的飞行轨迹和落点，它可以被视为刚体。如果我们研究的是击球的瞬间网球所发生的变形，那么要分析内部的受力情况，那么此时它应该被视作变形体。 力：物体间相互的机械作用，作用效果使物体的机械运动状态发生改变。 力有三要素：大小、方向、作用点 力是矢量，本课程用F⃗\\vec FF或FFF来表示。 力系：由很多个力在一起组成的系统，或者说一群力。 平衡：物体（系）相对惯性参考系（如地面）静止或作匀速直线运动。 静力学的任务 对物体进行受力分析 对力系进行等效替换（或简化） 建立各种力系的平衡条件，并求解未知力。 2.2 静力学五个公理 公理1 力的平行四边形法则 作用在物体上同一点的两个力，可以合成为一个合力。 合力的作用点也在该点，合力的大小和方向，由这两个力为邻边构成的平行四边形的对角线确定 合力（合力的大小与方向）F⃗R=F⃗1+F⃗2\\vec F_R = \\vec F_1 + \\vec F_2FR​=F1​+F2​ 亦可用力三角形求得合力矢 公理2 二力平衡条件 作用在刚体上的两个力，使刚体保持平衡的充要条件是：这两个力的大小相等，方向相反，且作用在同一直线上，即 F⃗1=F⃗2\\vec F_1 = \\vec F_2F1​=F2​。 公理3 加减平衡力系原理 在已知力系上加上或减去任意的平衡力系，并不改变原力系对刚体的作用。 推理1 力的可传性 作用于刚体上某点的力，可以沿着它的作用推移到刚体内任意一点，并不改变该力对刚体的作用。 例如推桌子，用手在后面推和在前面拉桌子，如果力的大小相同，那么作用效果不变。 对于在刚体上的力是滑动矢量，它的力的三要素为大小、方向和作用线。 推理2 三力平衡汇交定理 作用于刚体上三个相互平衡的力，若其中两个力的作用线汇交于一点，则此三力必在同一平面内，且第三个力的作用线通过汇交点。 公理4 作用和反作用定律 作用力和反作用力总是同时存在，同时消失，等值、反向、共线，作用在相互作用的两个物体上。 在画受力图时要注意此公理的应用。 公理5 刚化原理（针对变形体） 变形体在某一力系作用下处于平衡，如将此变形体刚化为刚体，其平衡状态保持不变。（反之不一定成立） 该公理给出了变形体在理论力学体系中的适用条件。 2.3 约束和约束反力 自由物体 空间位置不受限制的研究对象称为自由物体（简称为自由体） 气球的空间位置不受任何限制 气球为自由体 自由体的真实运动取决于作用在该物体上的力，称之为主动力 非自由体 空间位置受到限制的研究对象称为非自由体（简称为非自由体） 气球被限制在套筒中上下运动 气球为非自由体 约束 非自由体的运动（位移）所受到的限制称为约束，或者说，对非自由体的位移起限制作用的物体，称之为约束。 对气球位移起限制作用的套筒称为气球的约束 约束力 非自由体的真实运动是两种力共同作用的结果 主动力（气球的浮力） 约束对该物体的作用力：约束力或约束反力（套筒对气球的限制） 两类约束力 限制小球在套筒中的运动：理想约束力 不起这种限制作用：非理想约束力，例如气球与套筒间的摩擦力 理想约束：只考虑理想约束力的约束 小结 约束：对物体的位移起限制作用的物体。 约束反力：约束对被约束物体的作用力。也可称为被动力。 主动力：使物体产生运动或运动趋势的力。重力、风力、载荷力等 约束反力的大小是待定的，方向与该约束所能阻碍的位移方向相反，作用点为接触处。 2.4 工程中常见约束及约束力方向的确定 具有光滑接触表面的约束（光滑接触约束） 这类约束是指约束与被约束物体之间，虽然相互接触但是彼此是独立的，并且接触的表面被认为是光滑的，它不能够限制被约束物体沿约束物体表面切线的位移，只能约束沿法线并进入约束内部的位移，这个约束仅仅通过被约束物体的正压力来产生作用。 比如说沿着光滑表面滑动的滑块，它受到的约束力都是沿着约束表面垂直于被约束物体的。用更准确的语言表述：光滑接触约束，它所提供的约束力垂直于两个接触面的公切面，沿着公切面的法线方向，指向被约束物体的。 但是对于有些物体而言，比如两个啮合的齿轮，如果忽略摩擦力的话，两个接触面是互为光滑的。在考虑二维的情况下，那么它的约束力是垂直于二者的公切面方向，指向被约束物体的。比如说对于右边的齿轮来说，它受到下面齿轮的约束力，指向它自己。 总结 光滑支撑接触对物体的约束力，作用在接触处；方向沿接触处的公法线并指向受力物体，故称为法向约束力，用FNF_NFN​表示。 具体约束实例：可以忽略摩擦力的一般常见的接触约束。 由柔软的绳索、胶带或链条等构成的约束（柔索类约束） 我们知道柔索只能承受拉力，不能承受压力，所以说柔索这类的约束，它所提供的约束力为张力，一般用FTF_TFT​来表示 以皮带轮为例，假象将皮带切开，分为左右两部分，对于两部分来说，它们都受到向上下皮带的拉力，这些拉力都是沿着皮带的方向并且背向皮带轮。 总结 柔索对物体的约束力沿着柔索背向被约束物体。 皮带（链条）对轮的约束力沿轮缘的切线方向，为拉力。 光滑铰链约束（径向轴承、圆柱铰链、固定铰链支座等） 铰链是指用来连接两个构件，并允许二者之间做相对转动的一种机械装置。如果忽略摩擦力的影响的话，这样的铰链称之为光滑铰链。工程中铰链通常分为三类，径向轴承、圆柱铰链和固定铰链支座。 径向轴承（向心轴承） 它是通过一个带有圆筒的轴承来对转轴进行约束的装置。 约束特点： 轴在轴承孔内，轴被视为约束体、轴承孔为约束。 可以用简图来表示：圆孔代表轴承，被约束的轴只能在圆孔中转动，不能在圆孔中做任何移动。 约束力： 当不计摩擦时，轴与孔在接触处为光滑接触约束——法向约束力。约束力作用在接触处，沿径向指向轴心。 当外界载荷变化时，接触顶会变，则约束力的大小与方向均有改变。 我们可用两个通过轴心的正交分力FxF_xFx​和FyF_yFy​来表示，注意这只是一种表示方法，实质还是一个力 光滑圆柱铰链 约束特点：由两个穿孔的构件和圆柱销钉组成，如剪刀 约束力 光滑圆柱铰链：亦为孔与轴的配合问题，与轴承一样，可用两个正交分力表示。 两组约束互为作用和反作用关系，即FCx=−FCx′F_{Cx} = -F&#x27;_{Cx}FCx​=−FCx′​，FCy=−FCy′F_{Cy} = -F&#x27;_{Cy}FCy​=−FCy′​ 但是要注意，这两个构件之间并不是直接发生作用的，它们是通过销钉来传递的，所以一般情况下，我们将销钉和某个构件放在一起，不单独分析销钉的受力，如果要分析销钉的受力，需要将它单独取出。 固定铰链支座（也称固定铰支座） 它的组成与光滑圆柱铰链类似，只不过将其中一个构件换成固定支座形式，另一个被约束的构件只能做定轴转动。 约束特点：由径向轴承约束与地面或机架固定而成。 约束力：与圆柱铰链相同 小结 这三种约束（径向轴承、光滑圆柱铰链、固定铰支座），其约束特性相同，均为轴与孔的配合问题，都可称作光滑圆柱铰链。约束力一般用两个正交分力表示，但本质上是一个力。 其他类型约束 滚动支座约束 滚动支座就是可以随意滚动的铰链支座，它相当于在固定铰链支座的基础上，在支座与地面间装上光滑的滚轴而形成的。 约束特点：在前述固定铰支座与光滑固定平面之间装有光滑滚轴而成。它仅仅限制构件在竖直方向的位移，而不限制构件绕 支点的转动和在水平方向上的位移。 约束力：构件受到垂直于光滑面的约束力，可能是压力也可能是拉力。 滚动支座在实际工程中有广泛的应用，比如在桥梁的一端必须设计成滚动支座，这是为了让桥梁自由地伸缩从而防止因热胀冷缩所带来的破坏。 球铰链 它是由一个球和球壳组成的，构件可以连接在球体上，也可以连接在球壳上。 约束特点：通过球与球壳将构件连接，构件可以绕球心相对转动，但不能有任何移动。 约束力：当忽略摩擦时，球与球壳亦是光滑约束问题。约束力通过接触点，并指向球心，是一个不能预先确定的空间力。可用三个正交分力表示，本质是一个力。 工程中球铰链的应用有很多，比如换挡器，各种球节点等都属于球铰链约束类型。 止推轴承 它是由轴承和一个止推伐，因此它比径向轴承多了一个轴向的位移限制。 约束特点：止推轴承比径向轴承多一个轴向的位移限制。 约束力：比径向轴承多一个轴向的约束力，亦可有三个正交分力FAxF_{Ax}FAx​ FAyF_{Ay}FAy​ FAzF_{Az}FAz​。 小结 工程中的约束多种多样，有的还很复杂，分析起来需要专门的知识和经验，有时需要适当的简化和抽象化。后续的学习中还会陆续介绍。 手臂的约束：肩膀与手臂的关节为球铰链约束，手臂肘处的约束为光滑圆柱铰链约束，手腕处为两个并联的球铰链约束，手指为光滑圆柱铰链约束。 课后习题及解析 光滑铰链、球铰链、止推轴承这几种工程中的常见约束，虽然约束力可以用两个或者三个正交分力表示，但约束力本质上都是一个力。 错误，把止推轴承换成径向轴承才正确。止推轴承比径向轴承、光滑铰链和球铰链多了一个轴向的约束力。 光滑圆柱铰链中，销钉所串起来的两个构件它们所受到的约束力是一对作用力和反作用力，如果表示成正交分量的形式，则是两对作用力和反作用力。 错误 光滑铰链约束类型中包含径向轴承、光滑圆柱铰链、固定铰支座、光滑球铰链等 错误，不包含光滑球铰链。 控制门或者窗户开关的合页（蝶铰链）可以视作是一个光滑圆柱铰链约束。 正确 链条约束与皮带约束类似，约束力只能沿着链条的方向背离被约束的物体。 正确 滚动支座约束中，构件受到垂直于光滑面的支持力，约束结构的特点决定了其约束力的方向只能是垂直于接触面指向被约束物体。 ","link":"https://albertlidesign.github.io/post/TheoreticalMechanics2/"},{"title":"Mesh is Art (1): Mesh","content":" #前言 我一直认为这样写学习笔记总结是最好的学习方式，一方面能系统地梳理自己的知识框架，另一方面能促进不断地思考，加深自己对一些概念的领悟。先介绍下自己，我是一名美术生，虽说高中数学成绩一直名列前茅，但大学后数学也就没怎么碰了。本科是环境设计，对参数化设计痴迷向往，但是随着技术的不断精进，渐渐意识到很多所谓的参数化设计其实是伪参数化，之后的很长一段时间都陷入一种苦恼，就是那种知道这是未来但是没有明确方向。 后来阅读了清华大学代数拓扑专家顾险峰教授的相关文章，意识到参数化设计真正的核心应该是“量化”和“映射”。然而要想实现对形体的**“量化”和“映射”，网格处理是最关键的技术，借助网格才能将数字转化成形体，才能更容易地建立设计中的“概念”与实际形体地映射关系**（Nurbs与网格相比并不是很好的选择，原因就不在这里提了）。在那之后便开启了我的网格技术学习之路。通过两年多的学习和理解，现在已经成为了一名助理研发工程师（网格算法方向）。但仍然无奈于没有系统地学过代数拓扑和微分几何，很多概念只是道理上有了一定的理解，无法从数学上去严格地证明（说白了就是因为不懂数学，心里没底），只能从逻辑上去推导着理解网格的相关知识。不管怎么说，这次重启网格学习笔记就是为了梳理一遍自己的最底层理解，去寻找一些看似约定俗成的事物的背后的原因。 有趣的是，在学习网格技术的过程中，随着我对网格技术的理解越来越深，在处理一些复杂的设计问题上，能很轻易地找到了各种简单、巧妙的方法。比如建筑上，基本所有的节点形式都可以表达成网格，各种看似复杂的节点或幕墙设计都能轻松地运用网格表达出来；各种复杂的结构，做有限元分析时能够很自然地划出均匀、高效的有限元单元；在做造型设计时，也有了很强的布线能力。工作中经常体会到“刚刚好就应该是那个样子”的感觉。我想说的是，网格思维真的是一种非常棒的设计思维、几何思维，网格不只是一种建模工具，Mesh is Art！ 网络上对于网格技术的信息非常非常少，虽然本系列文章也是不专业的，但我想对于设计师这个群体而言，认真读完这个系列文章之后应该能有所收获，至少能加深对一些软件的理解能力，在学习软件的路上走得更快一些吧。 #网格建模简介 大家都了解过CAD软件，例如Auto CAD、Rhinoceros、Maya、3D MAX等等都是工业界和设计界常用的软件。通过这些软件，我们可以在软件中的虚拟空间建立二维、三维模型，实现对脑海中所想像出的形体进行建模，这样无论是生产制造还是设计，我们都有了直观的参考，不需要再像以前一样通过理解图纸加上想像去费时费力地推测形体本身的样子。那么，计算机是如何建模表达这些三维模型的呢？ 学过了高中数学，我们知道可以通过构建坐标系去实现各种形体的绘制。有了坐标系，我们能在空间中绘制各种各样的曲线或者图形。然而，像曲线这样的连续几何体是不可能完全精准的表达成图形的（即便是计算机也不能），计算机的表达方法和我们高中画函数的方法其实如出一辙，就是在空间中插值取点的方法，离散地来表达形体。比如绘制一个二次函数的函数图像，我们需要通过均匀地插值取点，然后用直线段顺次连接，最终得到一条折线(Polyline)，当取无数个插值点时，就可以近似拟合成曲线了。 那么，根据上面的例子，我们发现，对于一条曲线的表达，通常是用线段去拟合的（每条线段是由两个点加一根线构成的，两点一定共线）把曲线用若干条线段的过程叫做离散化。以此类推，我们不难想到，曲面的表达是用三角面去拟合的（因为每个三角面是由三个点加三条线构成的，三点一定共面，三角形是最简单的平面图形）。线段和三角形都可以称为其所处维度的单纯形（代数拓扑）。一维单纯形就是线段；二维单纯形就是三角形；三维单纯形就是四面体。 有了单纯形，我们就可以用它来表达生活中能见到的所有形体（通过将形体离散成若干个单纯形的方式）。现在我们再说回到计算机表达，通过单纯形，计算机可以将任何形体用三角形或四面体表达（这里主要说二维和三维，一维的线可以看作三角形和四面体的子集），像这种用三角形或四面体表示形体的建模方式称为网格建模，说网格通常指的就是三角形或四面体（当然还有其他形式，这里先不做展开）及所构成的图形。 #网格的应用 在引言中，相信大家对网格有了基本的认识，对于网格在各行各业的应用，我们就直接来看图吧。 对于建筑设计师来说，理解了网格可以使用算法生形，提高建模的效率和建模质量，对接其他工程师可以降低沟通成本。风环境、光环境、热环境等各种可视化分析均与网格质量密切相关。 对于幕墙设计师来说，理解网格可以快速生成曲面的幕墙嵌板模型，可以利用细分算法变换嵌板类型，利用平板算法降低幕墙造价，利用网格几何特性减少嵌板数量及安装复杂度。 对于结构（机械）工程师来说，理解网格可以更好地使用有限元计算，对于合适的模型采用合适的网格划分。在拓扑优化中可以用网格算法进行后处理。 对于CG艺术家来说，理解网格有助于提高建模质量，理解贴图、渲染等技术。 #网格的基本单元 这里我们再回到单纯形的解释，前面说了网格通常指三角形、四面体及它们所构成的图形。这里还有一些值得一提的点，我们先来设想这样的问题，对于一个球体的表达，可以有两种认识：其一是将球认为是球的外壳，即我们将这个球看作一个闭合成球这个形状的曲面，生活中举例的话比如足球，只有外皮，里面是空的；另一种是将球认为是填实的实体，举个例子就比如雪球，里面都是雪，很厚实。这两种表达方法各有优劣，对于“球壳”来说，我们对它进行一些剪切、拼贴、变形等操作都相对较容易，在网格处理上大多都会选择这样的表达方法，如Rhinoceros、Maya；对于实体球来说，一般用于结构工程的有限元分析，可以想象，结构分析当然要认定一个结构体是密闭填实的，只计算“外皮”和计算实体是完全不同的两个概念，因此在计算一些需要考虑到内部填充的操作时必须将物体考虑成密闭实体，这类软件的代表就如结构专业常用的有限元分析Abaqus。相信大家也猜到了，前者的单纯形就是三角形，后者是四面体。由于四面体填充的形式一般仅在结构分析中会涉及，非结构和计算机专业的人一般不会过多去关注这方面，本系列笔记也是介绍二维单元作为网格基本单元的情况。 理解了为什么要用二维单纯形作为网格基本单元后，我需要再补充一下，网格的基本单元除了三角形还有四边形（四边形不叫作单纯形了，但仍然是网格的基本单元）。为什么又多了一个四边形呢？这里又得提两个概念了，结构化网格和非结构化网格。**结构化网格是说，这个网格模型的大多数顶点所连接的网格面的数目（网格顶点的价）是相等的。**如果这句话现在理解不了也没关系，因为大多出情况都没有那么严格，比较直观的理解就是一张网格拟合的曲面，看上去结构线清晰分明就是结构化网格（如图5、6）。非结构化网格就是与结构化网格相对，看上去乱七八糟的单元排布就是非结构化网格。通常我们得到的三维扫描模型都是非结构化网格，这样的网格如果每个网格面片的大小近似相等的话那还好说，但是如果大小变化剧烈，就很难继续编辑，网格质量糟糕，一般必须通过算法或人工重建才能继续使用。 然而四边形，作为具有明显方向特征的基本单元，通常就被用作表达结构化网格了。四边形网格和三角形网格的特性如图8所示。 #网格的数据结构 阅读到这大概对网格有了一个感性的认识了，对于好奇心比较重的你可能会发问：计算机是如何识别出这是一个三角网格（或四边网格）的呢？（我们这里先不讨论颜色） 实际上计算机并不认识图形，要想让计算机知道，哦这里有一个三角面，那就必须用逻辑关系来表达出来。比如我们知道，三角面由三个顶点三条边和一张面构成（点线面全了），那么，顶点在空间中能表达出来，就是三个三维坐标嘛；边可以看作是两个顶点放在一起，也就是一个有顺序的，从一个三维坐标到另一个三维坐标的关系；面则可认为是三条边有序排列在一起的关系。再傻瓜一点说，就是空间三个顶点A、B、C，我把顶点A和顶点B放在一起就能表示从A到B这条边，其他边的表示依次类推，面的表示就是把三个顶点按照某一顺序放在一起就是一张面。 也就是说，网格最最底层的构成就是顶点的坐标和它们相连的关系。由此引出，网格的存储信息分为两部分，几何信息（顶点坐标）和拓扑信息（顶点连接关系）。 网格模型的数据存储有很多种方式，当前数据结构有很多，篇幅关系我这里说两个最常用的，Faced-Based Data Structures和Half-Edge Data Structure。 Faced-Based Data Structures是最简单直观的数据结构，说它直观是因为它就是上文我们所说的，只存储了顶点坐标和顶点拓扑关系两种数据的数据结构。但看似简单的东西其实也还有玄机，这里暂且不谈，下一节将焊接和法向的时候会再提出来的，现在只要有个感性的认识即可。 Half-Edge Data Structure直译就是半边数据结构，半边是非常好用的网格数据结构，它在相邻元素查找的问题上十分方便。这种数据结构是以网格边为主导的，是将每条边分成两条半边，这两条半边的长度与原网格边相同，但是方向相反。可以理解为把一条边拆成了两个方向相反、长度相同的向量，向量都是有始有终的，因此可以很容易将一个网格的所有半边首尾相接连起来，这也正是这种数据结构的妙处所在。具体的内容也在后面会详细解析，这里说太多反而容易把读者绕晕。 总之在这里我们只需要知道一点，在解决不同的网格编辑问题时，数据结构的选择非常重要，在运行程序时，合适的数据结构能大大提高运行效率，同样的算法，用不同的数据结构来实现，其运行效率会有很大的差别。 一般来说，判定一个网格的数据结构是否适合解决这个问题的依据有以下几点（现在理解不了可以后面靠实践去体会）： （1）构建数据结构的时间复杂度 （2）进行查询操作的时间复杂度 （3）网格编辑操作的时间复杂度 （4）空间复杂度 作为笔记的第一篇，我想讲的详细一些，里面内容很多是我查阅的资料加上自己的理解。由此可见，对网格有一定程度（几何上）的理解根本不需要什么高等数学，初中数学足以。相信本系列笔记会对大家有所启发。 下面是网格数据结构相关的资料链接，放出来供大家学习参考： https://mrl.nyu.edu/~dzorin/ig04/lecture24/meshes.pdf https://www.jianshu.com/p/464cda593ac1 https://blog.csdn.net/wozhengtao/article/details/51430025 作为笔记的第一篇，我说了很多我自己的底层认识，看完的朋友应该也能感觉到，理解网格根本不需要什么高等数学，初中数学足以。有了这些认识，在平常的建模和设计过程中，可能就会有更多的角度和思路去看待问题。当然仅是这一篇还远远不够，有兴趣的童鞋可以关注一下，希望本系列笔记会对大家有所启发。 ","link":"https://albertlidesign.github.io/post/meshisart1/"},{"title":"ALG_MarchingCubes_GPU v1.0","content":"Introduction ALG_MarchingCubes_GPU is an isosurface extraction plug-in for Grasshopper using Marching Cubes algorithm on GPU. Algorithm Classify voxels: Mark all the active voxels to get an active voxels array and calculate the number of vertices in each voxel by looking up vertices table. It is executed using one thread per voxel. Exclusive sum scan: Get the total number of active voxels and the total number of resulting vertices using exclusive scan algorithm. They are obtained by the sum of the last value of the exclusive scan and the last value of input array. Compact voxels: This compacts the active voxels array to get rid of empty voxels. This allows us to execute Isosurface Extraction on only the active voxels. Isosurface extraction: Calculate the position of the points in the active voxel and obtain all the result points by looking up triangle table. Generate a mesh model from result points. Reference [1] Dyken, C., Ziegler, G., Theobalt, C., &amp; Seidel, H. P. (2008, December). High‐speed marching cubes using histopyramids. In Computer Graphics Forum (Vol. 27, No. 8, pp. 2028-2039). Oxford, UK: Blackwell Publishing Ltd. [2] Lorensen W E, Cline H E. Marching cubes: A high resolution 3D surface construction algorithm. ACM SIGGRAPH Computer Graphics. 1987;21(4) [3] C. Dyken, G. Ziegler, C. Theobalt, and H.-P. Seidel. High-speed Marching Cubes using HistoPyramids. Computer Graphics Forum, 27(8):2028–2039, Dec. 2008. [4] The algorithm and lookup tables by Paul Bourke httppaulbourke.netgeometrypolygonise：http://paulbourke.net/geometry/polygonise/ [5] Marching Cubes implementation using OpenCL and OpenGL：https://www.eriksmistad.no/marching-cubes-implementation-using-opencl-and-opengl/ [6] A sample extracts a geometric isosurface from a volume dataset using the marching cubes algorithm.: https://github.com/tpn/cuda-samples/tree/master/v10.2/2_Graphics/marchingCubes [7] The introduction of marching cubes: http://www.cs.carleton.edu/cs_comps/0405/shape/marching_cubes.html [8] The introduction of marching cubes: https://medium.com/zeg-ai/voxel-to-mesh-conversion-marching-cube-algorithm-43dbb0801359 ","link":"https://albertlidesign.github.io/post/alg_marchingcubes_gpu_v10/"},{"title":"Learn CUDA Programming(2)","content":"Writing parallel kernels 在开始写kernels之前，我们需要对CUDA线程的两层结构有所了解。 一个线程是一个序列的执行单元，特点是所有的线程会执行相同的代码，而且是并行执行的。 在GPU中，多个线程组成的一个线程块，我们称之为线程块（Thread Block），而且这些线程可以通过shared memory（共享内存）进行通讯，还有一个同步函数，会对每一个线程进行同步。Block是放在流多处理器（SM）上面进行执行。 多个线程为一组组成了block，多个block就构成了线程网（Thread Grid），Grid对应的是整个GPU设备，在Grid之间不同设备之间是无法同步的，没有一个规定的同步函数对它进行同步。尽管它们之间可以通过global memory进行通讯，但是并不推荐这样，这样会影响到程序的效率。 因此对于gpu的线程分布的话，我们尽量让block与block之间相互独立。 在一个block中，线程的维度可以是三维，同理，在一个grid中，block的维度也可以是三维。所以我们对于grid和block中的多个线程，都有对应的id。 首先我们要了解一下如何分配这样的结构。如图所示，我们可以看到这个grid中是一个二维的结构，横向有3个，纵向有2个，第三个维度只有一个单位所以是1，因此我们定义一个grid初始化为(3,2,1)，表示它是一个二维的结构，x轴的维度是3，y轴的维度是2。同理在每一个block，我们同样可以定义一个类型，x轴维度是5，y轴维度是3，就有block(5,3,1)。这样我们就定义好了配置文件。kernel函数中的尖括号里面需要设置的就是配置信息，在编程的过程中，每一个block都对应了一个id，我们用一些变量来标识这些id，因此我们可以看到，x维的话我们使用threadIdx.[x]，y维就是用threadIdx.[y]，例如Thread(2,1,0)中，threadIdx.[x] = 2， threadIdx.[y] = 1。同理block也有类似的机制。除了调用的id外，还可以返回它们的维度，比如blockDim.[x]返回线程在x轴上的维度为5，gridDim[y]返回block在y轴的维度是2。 这些概念都与gpu硬件是一一对应的。一个线程会被发送到一个CUDA Core上进行计算，block就跟流多处理器进行对应，grid与Devie进行对应。 举个例子，刚才我们说每一个线程都有一个对应的id，那么我们如何读取这些数据呢？假设我们有一个data数据，里面有16个元素，那么我们给它分配16个线程，每个线程读取data中每一个对应的元素，在这里我们以4个线程为一组进行分组，因此我们知道每个block就会有4个线程，然后16个线程组成4个block。这样就有了三组id。 那么我们看一下如何利用gpu多线程来加速向量加法运算。假如我们有两个数组a和b，每个数组都有n个元素，然后我们要计算a和b中每个对应元素的和然后将其存储到c数组中。 这里我们写了一个简单的函数 void vecAdd(int n, float *a, float *b, float *c) { for (int i=0; i&lt;n; i++) { c[i]=a[i]+b[i]; } } void main() { int N = 1024; float* a, * b, * c; a = (float*)malloc(N * sizeof(float)); b = (float*)malloc(N * sizeof(float)); c = (float*)malloc(N * sizeof(float)); memset(c, 0, N * sizeof(float)); init_rand_f(a, N); init_rand_f(b, N); vecAdd(N, a, b, c); } 那么我们如何将这个函数在gpu上进行执行呢？ Step 1: 确定并行性。即我们要分析一下这个算法怎么并行，并行性在哪里？即如何对它进行二维层次结构的分配，做完这一步才能开始gpu编程 Step 2: 写GPU Kernel Step 3: 对GPU内存进行管理，如分配、初始化 Step 4: 开始调用这个函数 Step 5: 将数据从GPU端拷贝到CPU端 对于向量加法我们开始执行以上五个步骤： Step 1: 确定并行性 由于c[i]仅依赖于a[i]和b[i]，并没有使用其他计算，因此每一个c[i]是独立的，没有依赖性的，是可以并行的。我们就可以分配n个线程，使用1维的grid和1维的block，其中每个线程的对其中一个元素进行加法运算，也就是说，Thread[0] =&gt; c[0] = a[0]+b[0]，Thread[1] =&gt; c[1] = a[1]+b[1] ... 这样就完成了并行的加法运算。 接下来是对线程进行分配，这里我们以256个线程为一组，这样256个线程为1个block，那么为什么我们需要对线程进行分组呢？ 一个block最多支持1024个线程，如果大于1024我们就无法在一个block中分配线程 分配线程会有一定的计算资源，如寄存器，比如shared memory的大小。如果一个Block中的线程过多会影响它的效率，后面讲编程优化会讲到。 在这里我们简单以256为例来讲解一下整个过程。对线程进行分组以后我们需要通过刚才的方法计算出每个线程中我们要读取的数据的id，计算方法就是上文我们所说的方法：线程在block中的id+block在grid中的id*block的维度。 work index i = threadIdx.x+blockIdx.x*blockDim.x; 例如a[256]的索引 = 0+1*256 = 256 Step 2: 写GPU Kernel 首先我们要在函数前加上__global__标识符，标识这个函数是运行在gpu上的。 __global__ void vecAdd(int n, float *a, float *b, float *c) { int i = blockIdx.x * blockDim.x + threadIdx.x; //计算该线程要读取的id if (i&lt;n) //因为我们只需要从0到n-1进行计算即可，而线程数可能会大于n个 { c[i] = a[i]+b[i]; } } Step 3: 对GPU内存进行管理 void main() { int N = 1024; float *a, *b, *c; float *devA, *devB, *devC; // gpu上声明float a = (float*)malloc(N * sizeof(float)); b = (float*)malloc(N * sizeof(float)); c = (float*)malloc(N * sizeof(float)); cudaMalloc(&amp;devA, N*sizeof(float)); cudaMalloc(&amp;devB, N*sizeof(float)); cudaMalloc(&amp;devC, N*sizeof(float)); memset(c, 0, N * sizeof(float)); init_rand_f(a, N); init_rand_f(b, N); //从cpu上拷贝到gpu cudaMemcpy(devA, a, N*sizeof(float), cudaMemcpyHostToDevice); cudaMemcpy(devB, b, N*sizeof(float), cudaMemcpyHostToDevice); } Step 4: 开始调用这个函数 void main() { ... vecAdd&lt;&lt;&lt;(N+255)/256, 256&gt;&gt;&gt;(N, devA, devB, devC); ... } (N+255)/256表示一个grid中有多少个block，它是为了防止N不是256的倍数，N有可能大于256的倍数，这样可以直接取整。 Step 5: 将数据从GPU端拷贝到CPU端 void main() { ... cudaMemcpy(c, devC, N*sizeof(float), cudaMemcpyDeviceToHost); ... } 这样就完成了GPU加速的向量加法。相信大家有了更深的了解，可以写一个简单的Kernel来实现自己的代码功能。 Simple Optimization sample 实际上在GPU开发的过程中，我们开发的时间要比优化的时间短很多。我们会将更多的时间消耗在对代码的优化上。本节我们介绍一个简单的优化方案，这个例子跟gpu的内存有关，我们先来回顾一下gpu的内存架构。 首先每一个线程都有它自己的Local Memory，然后每一个Block都有所有线程所共享的Shared Memory，每个Grid和Grid之间都可以使用Global Memory进行通讯。 Global Memory 它的存储空间很大，生命周期从最开始分配空间到最后cudafree函数释放为止，因此它可以和多个Kernel进行通讯，例如Kernel 0计算完的数据暂时存在Global Memory里，Kernel 1可以调用这些数据进行进一步计算，直到我们使用完后可以调用cudafree函数释放，并且这些数据是可以被CPU进行读取的。 Shared Memory Shared Memory空间很大但是访问延迟很高，它是位于流多处理器片上的一段内存，生命周期与block一致，这是我们经常使用到的，它是block中所有线程所共享的。分配这段内存只需在前面加上__shared__标识符。block中的所有线程都可以通过Shared Memory进行通讯，但是在block与block之间不可以。 Registers 寄存器空间是gpu中访问速度最快的空间，它是由每个线程所分配的，只能由分配的线程所读写，因此它的生命周期与线程一致。 Stencil Example 在一个数组中，这个例子的功能就是计算在该数组中每一个元素对应的周边元素和。如上图所示，以黄色的元素为例，周边半径是3，那么周边元素和就是从左边第三个元素开始，一直到右边第三个，也就是要计算这7个元素的元素和，计算完以后我们存到另外一个数组里面，位置和这个元素对应。例如我们计算4的元素和，如下图所示，其结果就是1+2+3+4+5+6+7=28 这个程序其实实现的功能也很简单，这个程序其实和向量加法十分相似。相同的是每个计算都是相互独立的，无数据依赖性，因此我们仍然可以分配n个线程来计算每一个值。 //每个线程对周边元素和的计算 __global__ void stencil(int* in, int* out) { int globIdx = blockIdx.x * blockDim.x + threadIdx.x; int value = 0; //该变量位于寄存器中 for (offset = -RADIUS; offset &lt;= RADIUS; offset++) { value += in[globIdx + offset]; } out[globIdx] = value; } 这个程序并不是最优的，因为in和out都是在global memory里面，我们知道global memory延迟很大，在这里其实每个元素都会被读取很多次，我们这里可以简单看一下，半径是3的情况下它被读取了几次。我们以中间这个元素为例，最后可以得到这个元素被读取了7次，而且都是从global memory进行读取的。 我们知道Shared Memory访问速度更快，因此我们可以对它进行一个优化，优化方案就是在计算之前我们将这一段内存从global memory拷贝到shared memory，我们在shared memory进行读写，拷贝的大小是BLOCK_SIZE+2*RADIUS （1） 第一步，我们设定RADIUS = 3， BLOCK_SIZE = 16。先声明一段共享内存空间，大小为BLOCK_SIZE+2*RADIUS。声明以后，在每一个Block中都会有一段Shared Memory的空间，它是用来该Block中所有数据来计算的数据源。 （2）接着我们要计算两个id，第一个id是globIdx，即每个线程在整个Grid中的id，第二个id我们称之为locIdx，是读取数据的id。我们知道在共享内存中，我们数据的长度是比线程数多两个RADIUS的，左边右边各有RADIUS个，线程0是从最左边RADIUS个开始算的，因此我们这里要加一个半径，即 int locIdx = threadIdx.x+RAIDUS; （3）接下来是数据搬移，从Global搬移到Shared，第一步就是要搬移中间16个元素，因此我们使16个线程，每个线程都读取对应的元素，从in中拷贝到Shared里面 shared[locIdx] = in[globIdx]; 然后再拷贝两侧的数据 if (threadIdx.x&lt;RADIUS) { shared[locIdx - RADIUS] = in[globIdx - RADIUS]; shared[locIdx + BLOCK_DIMX] = in[globIdx + BLOCK_SIZE]; } （4）拷贝结束以后，我们就知道所有需要的数据都存储在shared memory里面，因此我们可以从shared里面开始计算，计算方法跟刚才一样。 __syncthreads();//线程同步函数 int value = 0; for (offset = -RADIUS; offset &lt;= RADIUS; offset++) { value += in[globIdx + offset]; } out[globIdx] = value; Thread Synchronization Function void __syncthreads(); 线程同步就是在一个block中，保证所有线程都完成了前面的工作，上述示例中，我们加入了一个 __syncthreads();，保证所有的线程都完成了线程搬移工作以后，我们才能开始进行下面的计算。我们知道线程与线程之间是并行的，但是先后顺序不可而知，如果不加入同步函数，一个线程还没有完成搬移的话，另外一个线程已经提前到计算的时候，那么它读取的数据就是错误的，这就是典型的一个“读后写”。为了防止类似这样的问题发生，我们加入这个函数。注意这个函数要对所有线程起作用，如果通过一些判断导致一些线程没有发生同步就会造成线程的死锁。 References ","link":"https://albertlidesign.github.io/post/learncudaprogramming2/"},{"title":"Learn CUDA Programming(1)","content":"Access the Power of GPU 一般有三种方法来实现加速：Libraries or OpenACC or Programming Languages 函数库： 例如： cuBLAS: 线性代数库 cuSPARSE: 稀疏矩阵的线性代数库 cuDNN: 深度神经网络 GPU Architecture 异构计算(Heterogeneous Computing) Terminology: Host: The CPU and tis memory(host memory) 主机端(CPU) Device: The GPU and its memory(device memory) 设备端(GPU) CPU与GPU的简易架构图 CPU有很多逻辑控制单元，而GPU有很多处理核心 CPU是缓存优化的处理器，缓存特别大，而且有很多控制单元 GPU是一个并行吞吐优化的处理器，计算核心特别多 因此我们知道，GPU的单精度处理能力比CPU快很多的原因其实就是GPU将更多的晶体管用于了计算而不是缓存 程序的开发中如何选择CPU和GPU 从图中可以看到CPU的缓存延迟非常短，这也对应它的架构里有很多复杂的逻辑控制 GPU则不同，图中的W是warp，1 warp = 32 Threads，GPU是以warp为单位进行切换的。我们可以通过海量的线程的切换来对缓存延迟做隐藏，如果你的程序是计算密集型的，而且有很好的并行特性，那么gpu是更好的选择。 GPU的架构有两个部分组成，第一部分是计算单元，第二部分是内存单元 图中的绿色部分就是一个CUDA Core 从图中可以看到，GPU和CPU是通过PCle进行链接的 一个GPU有很多SM（流多处理器）组成，每一个SM有很多核，因此有越多的SM，那么GPU就能在同一时间内处理更多的任务。 SM的架构 Kepler架构中的SM称为SMX，表示处理器更强劲，其逻辑控制单元是一个整体，控制192个CUDA Cores Maxwell架构中的SM称为SMM，其逻辑控制单元有4个，每个控制32个CUDA Cores 可以简单认为，一个SMM中包含了4个SMX，这样的话就避免了因为逻辑控制单元过少而造成核心的冗余，效率更高。从架构上来看，Maxwell一个CUDA核心相比于Kepler提升了35% Memory Hierarchy 因为经常要跟内存打交道，所以需要对GPU的内存架构非常熟悉，GPU的架构分为三个层次，如图所示，和CPU类似，可以看做成一个金字塔，从下往上越来越快，首先是Global Memory，有很大的内存空间，它的空间很大，但是缓存延迟很高，因此我们需要加入一个缓存，称之为L2缓存，GPU的缓存要像Global Memory一样读取数据的话，首先我们要看这些数据是否在L2中已经有缓存，有的话称之为缓存命中，这样就不用再从Global Memory里面读数据，这样就加快了读写。L2缓存是多个缓存所共享的。再上面有很多寄存器，L1缓存等。对于不同的内存有不同的使用和优化方案。 GPU in Computer System CUDA Programming Basics 配置CUDA开发环境 注意如果使用C/C++进行开发的话，我们使用的是NVCC的编译器 Heterogeneous Computing 异构编程 一般我们把GPU运行的程序构造成一个函数，在我们需要的时候进行调用，这个函数就是在GPU上运行 这个函数的之前和之后都是在CPU上单线程运行的，因此我们把整个程序分为：顺序执行，并行执行，顺序执行 顺序执行就是利用CPU的一个线程来执行 并行执行就是用GPU的多线程来进行计算 因此如果一部分功能是计算密集型的，我们就可以把它写成一个函数，对它进行并行，并不是把所有的代码都在GPU上运行，而是把需要的部分放到GPU上 CUDA Kernels 这里我们讲解一个概念，我们要将一部分代码在gpu上运行，这部分代码是一个函数，我们称为一个kernel（核函数），CPU（Host）执行functions，GPU（Device）执行kernels。 Hello World //hello_world.c #include &lt;stdio.h&gt; void hello_world_kernel() { printf(&quot;Hello World\\n&quot;); } int main() { hello_world_kernel(); } Compile &amp; Run: gcc hello_world.c ./a.out 在GPU上执行: //hello_world.cu #include &lt;stdio.h&gt; _global_ hello_world_kernel() { printf(&quot;Hello World\\n&quot;); } int main() { hello_world_kernel&lt;&lt;&lt;1.1&gt;&gt;&gt;(); } Compile &amp; Run: nvcc hello_world.cu ./a.out 不同之处 文件后缀名改为.cu _global_表示了该函数为一个核函数，标识这个函数会在gpu上运行 “&lt;&lt;&lt;...,...&gt;&gt;&gt;”具体含义是GPU中的一个配置 编译器改为nvcc GPU memory management hello world程序非常简单明了，但是在编程过程中我们经常要和内存打交道，那么如何进行GPU内存管理呢？ 我们很熟悉对CPU的内存管理，我们会使用 malloc();//动态分配内存 memset();//对空间进行初始化 free();//释放一段内存 对应于，在GPU中也有类似的函数，它们分别是 cudaMalloc(void** pointer, sie_t nbytes);//pointer指定空间，大小是nbytes cudaMemset(void* pointer, int value, size_t count); cudaFree(void* pointer); 例如： int nbytes = 1024*sizeof(int); int* d_a = 0; cudaMalloc((void**)&amp;d_a, nbytes); cudaMemset(d_a, 0, nbytes); cudaFree(d_a); 因此一定要分清楚，哪部分数据在cpu上，哪部分数据在gpu上 Data Copies __host__ cudaMemcpy(void* dst, void* src, size_t nbytes, cudaMemcpyKind direction); __global__是在gpu上进行的，__host__就是在cpu上执行 四个形参：拷贝目的指针，拷贝原指针，拷贝大小，拷贝方向 线程阻塞的，拷贝不完成，cpu不会执行下面的代码 常见拷贝方向有：cudaMemcpyHostToDevice（CPU到GPU）、cudaMemcpyDeviceToHost（GPU到CPU）、cudaMemcpyDeviceToDevice（GPU到GPU） 设定拷贝方向时，需要注意src和dst的指针是CPU的还是GPU的，要和拷贝方向一致 上述函数为线程阻塞的，同样还有一个异步的函数：cudaMemcpyAsync(); 三步流程 第一步：通过调用cudaMemcpy函数将CPU中的数据拷贝到GPU中，通过PCI Bus，方向就是HostToDevice 第二步：通过CPU启动kernel核函数，然后开始一个并行计算 Kernel&lt;&lt;&lt;grid, block&gt;&gt;&gt;(); 第三步：计算完以后，再调用cudaMemcpy将数据从GPU端拷贝到CPU端，方向是DeviceToHost 例子: 代码分为五个步骤： 在CPU端分配n个整型变量的空间 在GPU端分配n个整型变量的空间 初始化GPU内存为0 将数据从GPU拷贝到CPU 打印变量 #include&lt;stdio.h&gt; int main() { int dimx = 16; int num_bytes = dimx * sizeof(int); int *d_a = 0, *h_a = 0;//设备端和主机端的指针 //在CPU端分配n个整型变量的空间 h_a = (int*)malloc(num_bytes); //在GPU端分配n个整型变量的空间 cudaMalloc((void**) &amp;d_a, num_bytes); if(0 == h_a || 0 == d_a) { printf(&quot;Couldn't allocate memory\\n&quot;); return 1; } //初始化GPU内存为0 cudaMemset(d_a,0,num_bytes); //将数据从GPU拷贝到CPU cudaMemcpy(h_a, d_a, num_bytes, cudaMemcpyDeviceToHost); //打印变量 for(int i=0; i &lt; dimx; i++) { printf(&quot;%d\\t&quot;, h_a[i]); } free(h_a);//主机端释放 cudaFree(d_a);//设备端释放 return 0; } 这里注意，cudaMemcpy是CPU阻塞的，也就是说，不执行完该函数CPU就不会一直向下执行。如果换成一个异步的版本，CPU会在拷贝还没有完成的时候就开始打印，打印的时候很多是错误的。 上面的示例代码缺少了GPU上面的并行计算，那么我们下一面将开始讲解如何写Kernel函数。 ","link":"https://albertlidesign.github.io/post/learncudaprogramming1/"},{"title":"设计师转行做开发？来看看四位过来人怎么说","content":"Hello，大家好，我是Albert，近期有一些好朋友在询问我关于学习编程、算法等知识以及转行相关的事，我自己也经常思考一些相关的问题，于是想做一期采访，正好身边有不少从设计专业转行过来做开发的朋友，就想大家一起来探讨一下，也正好作为我们对自己过去这几年转行经历的总结。 采访的模式是这样的： （1）在采访前，我找到了一些我上大学时的学弟学妹来收集他们想问的问题，经过筛选、精简、提炼出了7个问题。 （2）提炼好7个问题后分别发给除我以外的另外三名采访对象，让他们自己根据情况来选择解答。 （3）收集每个人的答案。 那么下面我们就开始吧。 受访者简介 1、为什么转行到现在的领域了？是否基于自己的兴趣？最先考虑的因素是什么？ Panda：转行到现在的领域也算阴差阳错吧！主要是之前的事务所熬夜太严重，做了很多无意义的重复的工作，加上身体有些吃不消，就想着换了。转到做建筑机器人和三维打印，大部分原因是基于自己的兴趣，我对于机械和和编程有着很浓厚的兴趣，其次收入要比之前好点儿，所以最先考虑应当是自己的兴趣。 Noy：转行早期是非常痛苦的，但是计算机专业有一点比较好，它能及时反馈你的学习成果。当然，转行到计算机，完全是基于个人兴趣。我从小喜欢捣鼓机械，更喜欢理性的事物，而计算机又特别符合我的喜好，所以转行就是完全基于我的兴趣出发的。当然工作后也体现出，相对于设计，我更擅长于编程一些，我也喜欢编程的过程。 咪工：对于选择工作，我个人想法是：首先不讨厌这个工作，其次看职业天花板，也就是行业里混的比较好的前10%在哪个收入水平，普遍是什么年龄，如果都可以接受，就是好的工作。 用这个方法评估，去设计院画图不是一个可以接受的选择，所以我转行了。没有最优先的因素，兴趣和报酬对我来说同等重要。 Albert：我最开始只是对一些异形曲面的建筑比较好奇，因为上大学的时候就读完了很多建筑史相关的书籍，一直没想通建筑在近代究竟是如何发展成那种有机曲面形态的，为了探索这个答案，就开始接触参数化相关的内容。另一方面，我中学时代一直都极其擅长数学，甚至梦想当数学家的那种，因此也想以后能从事将设计与数学相结合的领域，于是就按照“三维打印-&gt;参数化设计-&gt;计算机编程-&gt;计算机图形学”的顺序开始慢慢向这方面靠，一个由应用到底层的学习路线吧，感觉这条路上收获了很多，很多问题也渐渐清晰了，过程中考虑太多别的，就是想知道为什么，一步步就走过来了。 2、转行专业与过去的建筑相关专业之间的不同之处在哪里？过去在学校学习的知识有运用上吗？ Panda：现在从事的行业相比于建筑学专业，现在的专业更偏重于建筑技术研究，把一些先进的设计技术引入到建筑行业中，在提高建筑生产效率同时，去寻求一种新的建筑设计方法和理念。所以说，过去在学校学的知识大都还是必须要用的，现在我们不光去解决技术问题，同时还要做前期的建筑设计。 Noy：计算机行业常看到的现象是改bug，改好bug之后技术有一个提高。这个过程像在寻找正确答案，找到答案，这个事儿就算过了。写代码要求模块化，可复用度高。建筑行业，没有所谓的正确答案，甲方基于在现有建筑规范下，更多用主观臆想去主导工作进程。有可能让你一天的劳动付诸东流，工作反复且低效。 但不论是计算机，还是建筑，背后都有庞大的知识体系，都是多学科混杂着的，这点是一样的。但是计算机是解决问题的学科-偏理性，而建筑是偏设计的学科-偏感性。学习计算机如同建筑一样需要有更多的耐心和好奇心去扩大自己的思维和知识储备。这一点我在校园时期，学习建筑的时候就已经养成了，相对于大学学习到的知识，大学让我养成独立思考的习惯更重要。 咪工：不同之处在于，技术美术工作对一个结果的好坏往往有着比较明确的评价标准，建筑专业上似乎并不是这样，同一份设计，不同的老师会打出完全不同的分数。技术美术工作更客观一点吧，设计更主观一些。 Albert：在做设计的时候我的老师们总是说做设计要有逻辑，这一点我也一直在体会，学习了一些计算机知识会发现，这个领域对逻辑的要求极高，有的时候做设计，你可以用一些花言巧语把故事讲得听上去很有逻辑，但是在计算机程序里，必须完全逻辑正确，只要有一点逻辑漏洞就会造成效率的降低甚至完全错误，近期我在学习gpu并行编程中更有深刻体会。关于过去所学的知识，无时无刻不在应用吧。我觉得从设计专业里转行出来的人还是会保留从设计的角度出发去思考、解决问题的，包括善于接受新鲜知识、善于提出新想法、对作品精益求精的追求、对工作量和完成度的把控，这些养成的习惯一直在帮助我去处理很多事，它们不是实际所学到的知识，而是潜移默化地对设计师的影响，所以无论过去学了什么，都要感激那段经历，我觉得没有什么学习是没用的，就像我中学数学好，后来学了艺术设计，现在又来到一个艺术和理工的交叉学科，没有一段经历是没用的。 3、您认为对于想转行的建筑学学弟学妹们需要做出那些努力和尝试来提高自己的核心竞争力？ Panda：要转行，目标的知识或多或少都要懂一些，因为转行的优势之一就是比其他人多懂一个专业的知识，建筑转其他行业就更不用说了，建筑学本身就学的广。我觉得如果下定决心转行之前，还是要花时间去学一学目标行业的相关知识，不必要一定学得很精，但一定多少要懂一些，这样转过去之后也会学得很快。当然目标行业的知识学得很过硬，自己的核心竞争力自然就起来了！ Noy：如果单纯从钱的角度来说，不建议转行，这俩学科差的不是一点半点，计算机时薪高是不假，问题在于本身对它本身了解多少，能否接受转行所承担的代价（学习成本和已经付出的成本），最重要的是，自己是否适合，或者热爱与否，以及学习期是否有家庭的支持。如果转过去的薪水不满意，还能否坚持下来？如果上述问题都没有明确答案，我建议慎重考虑，毕竟这俩专业交集很小，而计算机学科需要大量且不断更新你的储备知识。就计算机专业的基础知识来说，计算机导论，数据结构与算法都是需要了解的。这对你以后的发展很重要，框架性的基础知识决定了你的发展力，一定不能仅仅只会使用代码，要学会理解代码。如果你有决心搞定以上问题，支持你转行。但转行也不一定就非要离开建筑行业。我举例几个以建筑结合计算机为目的的方向，可供了解： 直接工作：找偏理性、偏编程的岗位，比如参数化设计师。 申请建筑类硕士：这种大多出来还是做建筑师，可以专攻参数化。 申请人机交互类硕士：这也是建筑系同学很常见的申请方向。理想的去处是各个科技公司的UX\\UI设计师，也有强行转行码农的。 咪工：核心竞争力我觉得有三点，分别是信息收集能力，学习能力，沟通能力。这三者缺一不可。 沟通能力往往是被忽略的一点，现今社会已经没有所谓的一个人就是一只军队了，个人能力再强在巨大的工作量面前也不值一提。作为团队合作中的一员，高效的协同上下游，是每个人都需要有的能力。提升方法也很简单，学一点PM技能即可。 另外附赠一篇知乎回答，适用于学习任何公开的技术：没专业背景的人自学houdini，可否请大佬们给我指条明路？ 包括了沟通能力和信息收集能力，可以细品。 Albert：首先如果没有非常浓厚的兴趣的话，单纯为了钱转行我认为是不太现实的。因为计算机行业的知识太过庞杂，从学习的角度上讲，为了学习这些已达到一个正常的本科计算机系毕业生的水平可能都需要像高三那样学习个两三年，这期间光苦学没人指导是非常痛苦的一件事，除非有浓厚的兴趣支撑，能建立起一个“付出就有回报”的快速反馈机制的话才有可能吧，真的需要付出极大的努力，即便我自己也觉得依然差的很远。 我觉得对于刚开始有转行想法的人，第一件事是消除疑虑，确定自己就干这个了，不把这个问题想通的话，带着犹豫是做不好的。发自内心地想做这些事的话，就不要想太多，马上开始行动（有的时候说做就做了，有什么大不了的），建立个自己的博客，每天更新自己学到的技术，买书看教程开始啃，把自己能不能转行成功这件事忘掉，只做事不带情绪，应该是没问题的。在这里推荐的学习方法就是更新博客，一方面积累自己所学的知识，另一方面给自己建立一个“付出就有回报”的快速反馈机制，有助于督促自己学习，也让自己保持兴趣吧。 4、能否对那些正在纠结转行却感到迷茫的后辈说几句话？ Noy：没有基础，没有能力，想转行并不容易。转行之前，首先给自己定一个方向：你想在互联网的哪个领域发展？产品、运营、营销或者其他。找到方向，然后沉下心去学习和积累。除此之外，现实环境因素也需要考虑，转行穷三年是很有可能的，所谓有得也有失，平衡利弊后，再做考虑。 当然如果你只是感到迷茫，想要转行，而无法做到长时间沉下心不断学习，那么我不建议你转行。在现有的知识体系下，继续坚持和努力是一个更安全的选择。 咪工：成年人做事讲究性价比，不要做性价比低的事。比如纠结这件事本身就非常没有性价比。转行前问问自己：有没有兴趣，报酬和职业天花板能不能满足自己的物质欲望，如果两个答案都是肯定的，那就转吧，转了有可能会后悔，不转一定会后悔。 Albert：先分享一下我听过的一些关于转行的疑虑：我也不知道我适不适合干这个；我比不上那些专业的人怎么办；我数学不好；我觉得我太笨了，没那个脑子；万一进了这个行业还是会被取代怎么办；万一这个行业不是我想的那么好怎么办。现在简化一下，问自己两个问题，第一，喜欢这个吗？第二，愿意为此努力吗？都是yes的话，干就完了，至少在做自己喜欢的事，至少是自己选择的，不后悔就行。很多人听了别人的建议走上了一条道路会后悔，我觉得与其这样不如从一开始所有选择都自己来作，管他前方道路如何艰险，自己喜欢自己选了，走就是了。问题的答案不是yes的就去找是yes的领域，答案在自己的心中不是别人的言语。每个领域都有大师，每个大师都热爱着自己的专业，想学好一个专业最起码的是热爱吧，20多岁的人，还有无限的可能。 5、如何突破计算机知识不足的壁垒，哪种语言最适合初学者？ Panda：说实话，现在任何一个专业都少不了计算机知识。学计算机，最困难的点在入门，所以想要突破这个壁垒还是需要下很大功夫的，现在网络上关于计算机的各种网课和资料数不胜数，随便找几个，入门是不成问题的。对于初学者，选择什么样计算机语言，还是要看以后主要从事什么类型工作，做开发的话少不了C/C++,当然，这也是众多计算机语言中最难的语言之一，C#也是开发必备，入门也很简单。运用类可选择Python、C#等入门简单，易上手，通用性比较广的语言。 咪工：如何突破计算机知识的不足：做一个结果导向的人，时常思考学这个是要解决什么问题。专注于某一个具体的问题时，往往突破是最快的。python是对初学者非常友好的语言，建议入门先学python。但不要为了学python而学python。对于我来说python可以快速的验证想法，且我的工作不看重运行效率，python应该就是最优解了。 Albert：这个主要取决于想做的领域，比如python适合做数据科学、AI、爬虫以及快速开发一些小工具，C/C++适合作为大型工程的底层以及一些计算机领域的科研开发（比如计算机图形学），C#适合做一些软件的二次开发、Unity游戏开发等。当然这些也不是绝对的，刚下定决心开始学的话建议先从简单的高级语言开始学起，Python、C#、Java等，Matlab也是不错的选择，在工科领域做科研经常使用。 6、您觉得转行需要具备哪些素质？数理化已经忘记了，还能转行吗？ Panda：我觉得转行最重要的素质就是要有求学上进的精神，学过的知识忘了不可怕，只要敢于探索和研究，再加上自己浓厚的兴趣，啥时候转行都是可以的。 Noy：转行到计算机行业，更需要你是一个好学的、能持续学习的人，不要过多担心过去所学习的知识，因为它们并不一定适用于工作。 有兴趣自然是最好的。如果真的觉得对这行业有兴趣，工作起来也不会那么累。但说实话，敲代码这样枯燥的活，估计没多少人真正感兴趣。 咪工：转行只需要动手能力。转行能不能过得更好需要学习能力，信息收集能力和沟通能力，任何行业都需要。数理化忘了很正常，捡起来就可以，而且在工程领域，并不一定需要你数学超级好，反而是交叉学科人才需求很大，所以有建筑系的背景并不是减分项，可以大胆的尝试。 Albert：敢于接触新事物，喜欢迎接挑战吧。我中学的时候就经常因为自己解决了一个数学难题而激动地睡不着觉，现在也是这样，看到一些复杂的问题就特别想去战胜它，有时候甚至忘记吃饭的那种，最后问题解决了我会非常兴奋，能原地跳起来，虽然很傻但是觉得这是人生的乐趣。关于数理化知识，我想说本科专业高数都没学过，忘记了不影响，学就完事了。 7.转行后，重新审视建筑学这个领域有什么启发？ Panda：转行之后再重新审视建筑学才发现，实践是检验真理的唯一标准！ Albert：在某些角度上，学科之间是存在底层和表层关系的，建筑学的一些概念在计算机里就是很简单的一些算法和思想，比如BIM某种程度上就是编程里的面向对象思想。这也启发我们，一些应用学科的学科发展和突破可能需要一些底层学科的推动，转行之后，很多建筑学现象就更能看到一些问题的本质了。之前有一个数学系的朋友在苦恼于他所学的知识有什么应用，我喜欢与不同学科的人交流就跟他聊起来，介绍了几个数学知识在建筑设计和工程中如何帮助节省造价如何解决设计难题的例子，他也分享了他对这些问题的看法，其实很多建筑上很复杂的问题到数学里都是很平常的数学题，并且介绍了很多更好解决这些问题的数学方法，整个聊天过程很愉快，两个人都很兴奋。多学了很多东西就有了更多的启发，能从更广的角度来认识学科，看到学科发展的路线，这样的体验是非常难得可贵的。 总结 我们四个人有共同点也有不同点，相信看了我们的回答每个人心中都已经有了答案，希望我们的分享能为你对转行这件事的认识有所帮助。 ","link":"https://albertlidesign.github.io/post/careerchanging/"},{"title":"线性代数(7)：特征值与特征向量","content":"特征值和特征向量的意义 假设给定矩阵AAA，作用在向量xxx上，结果就得到了向量AxAxAx（此时矩阵A就像一个函数f(x)f(x)f(x)），其中，我们会得到AxAxAx中的很多向量，在这些向量中，我们感兴趣的是那些线性变换前后方向保持一致的向量，这些向量是特殊的。因为对于多数向量而言，线性变换后的AxAxAx与xxx是在方向上会发生改变。对于那些特定的向量能使得AxAxAx平行于xxx的，我们称之为特征向量（Eigenvectors）。 那么平行意味着什么？我们可以用方程来表达这样的关系，即Ax=λxAx=\\lambda xAx=λx，xxx表示特征向量，λ\\lambdaλ作为向量xxx的系数，可以为负数表示平行且方向相反，也可以取000，甚至可以为复数（实数构成的矩阵可能会出现虚数特征值）。这里的λ\\lambdaλ称为特征值（Eigenvalues）。 我们现在并不知道该如何求矩阵的特征向量和特征值，但我们可以先来考虑以下几个问题： （1）当特征值为000时，意味着什么？根据我们前文所学的知识，当特征值为000时，有Ax=λx⇒Ax=0Ax=\\lambda x \\Rightarrow Ax=0Ax=λx⇒Ax=0，即特征值为000的特征向量应该位于AAA的零空间中。也就是说，如果矩阵AAA是不可逆矩阵，那么它将会有一个特征值为λ=0\\lambda = 0λ=0。 （2）我们再来看投影矩阵P=A(ATA)−1ATP = A(A^TA)^{-1}A^TP=A(ATA)−1AT的特征值和特征向量。 当向量bbb处于投影平面（AAA的列空间）中时，PbPbPb与bbb是同向的，此时bbb投影前后不变，即Pb=1⋅bPb=1·bPb=1⋅b。即在投影平面中的所有向量都是投影矩阵的特征向量，而它们的特征值均为111。 当向量bbb为投影平面的法向量时，此时bbb也就是误差向量eee。我们知道误差向量eee垂直于列空间C(A)C(A)C(A)，因此我们可以得到Pe=0⋅ePe=0·ePe=0⋅e，即特征向量eee的特征值为0。 因此投影矩阵的特征值为1,01,01,0。 （3）如何求二阶置换矩阵A=[0110]A=\\left[\\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 0\\end{matrix}\\right]A=[01​10​]的特征值和特征向量。观察矩阵我们会知道，经过置换矩阵处理过的向量，其元素会发生交换，那么就有经过矩阵交换元素前后不变的情况和方向相反的情况，分别为特征值为111的特征向量[11]\\left[\\begin{matrix} 1 \\\\ 1\\end{matrix}\\right][11​]和特征值为−1-1−1时的特征向量[1−1]\\left[\\begin{matrix} 1 \\\\ -1 \\end{matrix}\\right][1−1​]。 对于一个n×nn×nn×n的矩阵，将会有nnn个特征值，特征值与该矩阵对角线上的元素的和相同，即∑i=1nλi=∑i=1naii\\sum_{i=1}^n \\lambda_i = \\sum_{i=1}^n a_{ii}∑i=1n​λi​=∑i=1n​aii​。我们把矩阵对角线上的元素称为矩阵的迹（Trace）。在上文二阶置换矩阵的例子中，如果我们求得了一个特征值111，我们可以直接利用迹来求出另一特征值λ2=0+0−1=−1\\lambda_2 = 0+0-1=-1λ2​=0+0−1=−1。 特征值和特征向量的求解 我们的问题是如何找到特征值和特征向量，这不是一个Ax=bAx=bAx=b的求解问题，因此不能使用消元法，我们需要一个更巧妙的方法来求解它们。 观察等式Ax=λxAx=\\lambda xAx=λx，这个等式难解是因为其中有两个未知量（λ\\lambdaλ和xxx），我们的目标是将等式化成仅有一个未知量的方程，因此我们需要对其进行变形，λ\\lambdaλ可以看作是λI\\lambda IλI，这样就有 Ax=λx⇒Ax=λIx⇒(A−λI)x=0Ax=\\lambda x \\Rightarrow Ax = \\lambda Ix \\Rightarrow (A-\\lambda I)x = 0 Ax=λx⇒Ax=λIx⇒(A−λI)x=0 如果对于不为零向量的xxx该等式成立，那么意味着矩阵(A−λI)(A-\\lambda I)(A−λI)为奇异矩阵(否则向量xxx必为零向量或零矩阵)。那么我们知道奇异矩阵的判定方法是其行列式为零，即 ∣A−λI∣=0|A-\\lambda I| = 0∣A−λI∣=0 这样等式中就不含未知量xxx了，该方程仅含未知量λ\\lambdaλ，该方程称为特征方程或特征值方程。我们可以通过特征方程来求解出λ\\lambdaλ，当然λ\\lambdaλ可能有多个不同的值，也可能有重复的值，重复的λ\\lambdaλ是难点所在。 得到λ\\lambdaλ后，我们可以继续求解向量xxx，此时使用消元法，我们已知(A−λI)(A-\\lambda I)(A−λI)是个奇异矩阵，寻找其零空间，利用消元法找出主列，给自由变量赋值即可。下面我们以一个示例来具体阐述求解的步骤。 例 求A=[3113]A=\\left[\\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 3\\end{matrix}\\right]A=[31​13​]的特征值和特征向量 观察矩阵AAA，我们发现这是一个对称矩阵，对称矩阵意味着其特征值必为实数（这在后面的篇幅中会证明）。我们先来求(A−λI)(A-\\lambda I)(A−λI)的行列式，有 det(A−λI)=∣3−λ113−λ∣=(3−λ)2−1=λ2−6λ+8=(λ−4)(λ−2)=0det(A-\\lambda I)= \\begin{vmatrix} 3-\\lambda &amp; 1 \\\\ 1 &amp; 3-\\lambda\\end{vmatrix} = (3-\\lambda)^2 -1 = \\lambda^2-6\\lambda + 8=(\\lambda-4)(\\lambda -2)=0 det(A−λI)=∣∣∣∣​3−λ1​13−λ​∣∣∣∣​=(3−λ)2−1=λ2−6λ+8=(λ−4)(λ−2)=0 求解过程中我们发现(A−λI)(A-\\lambda I)(A−λI)的行列式最后化成了一元二次方程，我们可以轻松求解出λ1=4,λ2=2\\lambda_1 = 4, \\lambda_2 = 2λ1​=4,λ2​=2。在继续求解下去之前，对于二维的矩阵，我们可以观察到在一元二次方程展开后，一次项的系数其实就是矩阵AAA的迹的相反数（3+3=63+3=63+3=6），而常数项则为矩阵AAA的行列式（det(A)=3×3−1=8det(A) = 3×3-1 = 8det(A)=3×3−1=8），根据因式分解的特点，我们可以进一步得出，特征值之和就等于矩阵的迹，特征值之积等于矩阵的行列式，即 ∑i=1nλi=∑i=1naii,∏i=1nλi=det(A)\\sum_{i=1}^n \\lambda_i = \\sum_{i=1}^n a_{ii}, \\quad \\prod_{i=1}^n \\lambda_i = det(A) i=1∑n​λi​=i=1∑n​aii​,i=1∏n​λi​=det(A) 然后再来看特征向量，我们已经得到了两个特征值，现在只需要分别将两个特征值代入去求解特征向量， 当λ=4\\lambda = 4λ=4时，有A−4I=[3−4113−4]=[−111−1]A-4I = \\left[\\begin{matrix} 3-4 &amp; 1 \\\\ 1 &amp; 3-4\\end{matrix}\\right] = \\left[\\begin{matrix} -1 &amp; 1 \\\\ 1 &amp; -1\\end{matrix}\\right]A−4I=[3−41​13−4​]=[−11​1−1​]，这个矩阵是奇异的，代入到(A−λI)x=0(A-\\lambda I)x = 0(A−λI)x=0，有[−111−1][x1x2]=[00]\\left[\\begin{matrix} -1 &amp; 1 \\\\ 1 &amp; -1\\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2\\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0\\end{matrix}\\right][−11​1−1​][x1​x2​​]=[00​]，得x1=[11]x_1=\\left[\\begin{matrix} 1 \\\\ 1\\end{matrix}\\right]x1​=[11​]。 当λ=2\\lambda = 2λ=2时，有A−2I=[3−2113−2]=[1111]A-2I = \\left[\\begin{matrix} 3-2 &amp; 1 \\\\ 1 &amp; 3-2\\end{matrix}\\right] = \\left[\\begin{matrix} 1 &amp; 1 \\\\ 1 &amp; 1\\end{matrix}\\right]A−2I=[3−21​13−2​]=[11​11​]，这个矩阵是奇异的，代入到(A−λI)x=0(A-\\lambda I)x = 0(A−λI)x=0，有[1111][x1x2]=[00]\\left[\\begin{matrix} 1 &amp; 1 \\\\ 1 &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2\\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0\\end{matrix}\\right][11​11​][x1​x2​​]=[00​]，得x2=[−11]x_2=\\left[\\begin{matrix} -1 \\\\ 1\\end{matrix}\\right]x2​=[−11​]。 至此我们完成了对矩阵AAA的特征值和特征向量的求解，并且我们还发现两特征向量满足正交关系。 观察A=[3113]A=\\left[\\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 3\\end{matrix}\\right]A=[31​13​]得到的特征向量，与第一节中的置换矩阵A=[0110]A=\\left[\\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 0\\end{matrix}\\right]A=[01​10​]刚好相同，它们的特征值不相同，前者为444和222，后者为111和−1-1−1。但是我们发现，两个矩阵之间的关系可以看作[3113]=[0110]+3I\\left[\\begin{matrix} 3 &amp; 1 \\\\ 1 &amp; 3\\end{matrix}\\right] = \\left[\\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 0\\end{matrix}\\right]+3I[31​13​]=[01​10​]+3I，其特征值的关系为4=1+3，2=−1+34=1+3，2=-1+34=1+3，2=−1+3，那么两矩阵相加时，得到的矩阵的特征值之和是否等于两矩阵特征值之和？ 我们设Ax=λx，Bx=αxAx=\\lambda x，Bx=\\alpha xAx=λx，Bx=αx，只需验证(A+B)x=(λ+α)x(A+B)x=(\\lambda+\\alpha)x(A+B)x=(λ+α)x是否成立。 当B=3IB=3IB=3I时，在上述例子中我们知道，该等式是成立的，但是如果矩阵BBB为任意矩阵，则等式不一定成立。因为这两个式子中的特征向量xxx不一定相同，所以等式应该写成Ax=λx，By=αyAx=\\lambda x，By=\\alpha yAx=λx，By=αy，显然加和的等式无法成立。因此A+BA+BA+B的特征值并不一定等于AAA的特征值和BBB的特征值之和，仅当BBB为单位矩阵的倍数时成立。 复数特征值的情况 上文中还有个问题是，为什么实数构成的矩阵可能会出现虚数特征值？ 例 旋转矩阵QQQ可以使得空间中的向量旋转90°90°90°，Q=[cos90°−sin90°sin90°cos90°]=[0−110]Q=\\left[\\begin{matrix} cos90° &amp; -sin90° \\\\ sin90° &amp; cos90° \\end{matrix}\\right] = \\left[\\begin{matrix} 0 &amp; -1 \\\\ 1 &amp; 0 \\end{matrix}\\right]Q=[cos90°sin90°​−sin90°cos90°​]=[01​−10​]，用QQQ表示是因为旋转矩阵是正交矩阵。 我们观察矩阵QQQ的迹和行列式发现 {λ1+λ2=0（矩阵的迹）λ1⋅λ2=1（矩阵的行列式）\\begin{cases} \\lambda_1+\\lambda_2 = 0（矩阵的迹） \\\\ \\lambda_1·\\lambda_2=1（矩阵的行列式）\\end{cases} {λ1​+λ2​=0（矩阵的迹）λ1​⋅λ2​=1（矩阵的行列式）​ 从几何角度上，可以想象，哪些向量发生90°90°90°旋转后还是它自身，显然对于实向量是不存在的。如果我们求解(Q−λI)(Q-\\lambda I)(Q−λI)的行列式有 det(Q−λI)=[−λ−11−λ]=λ2+1=0det(Q-\\lambda I) = \\left[\\begin{matrix} -\\lambda &amp; -1 \\\\ 1 &amp; -\\lambda\\end{matrix}\\right] = \\lambda^2+1=0det(Q−λI)=[−λ1​−1−λ​]=λ2+1=0 解得λ1=i,λ2=−i\\lambda_1 = i, \\lambda_2=-iλ1​=i,λ2​=−i。两个特征值均为复数，因此我们说即使矩阵全是由实数构成的，其特征值也可能不是实数。有一个结论是：如果矩阵越接近对称，那么其特征值就是实数，相反，如果矩阵越不对称，那么其特征值就越可能有虚数存在。对于反对称矩阵QT=−QQ^T=-QQT=−Q，是一个极端情况，于是我们得到了纯虚数的特征值，通常我们见到的矩阵是介于对称与反对称之间的。 特征值相同的情况 例 求A=[3103]A = \\left[\\begin{matrix} 3 &amp; 1 \\\\ 0 &amp; 3\\end{matrix}\\right]A=[30​13​]的特征值和特征向量 首先观察矩阵AAA发现这是一个三角矩阵，三角矩阵的特征值就在其对角线元素上，因为在行列式的计算中，对角线两侧的元素不影响其行列式的值，有 det(A−λI)=∣3−λ103−λ∣=(3−λ)2=0det(A-\\lambda I) = \\begin{vmatrix} 3-\\lambda &amp; 1 \\\\ 0 &amp; 3-\\lambda\\end{vmatrix} = (3-\\lambda)^2 = 0det(A−λI)=∣∣∣∣​3−λ0​13−λ​∣∣∣∣​=(3−λ)2=0 解得λ1=λ2=3\\lambda_1 = \\lambda_2= 3λ1​=λ2​=3。下面代入特征值计算特征向量，有 (A−λI)x=[−111−1][x1x2]=[00](A-\\lambda I)x=\\left[\\begin{matrix} -1 &amp; 1 \\\\ 1 &amp; -1\\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2\\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0\\end{matrix}\\right](A−λI)x=[−11​1−1​][x1​x2​​]=[00​] 因为两个特征值都为333，因此我们只能求出一个特征向量，即x2x_2x2​，我们无法得出另一个与x1x_1x1​线性无关的特征向量了。本例中，矩阵AAA是一个退化矩阵，重复的特征值在特殊情况下可能导致特征向量的短缺。 特征值与特征向量的应用 在了解了什么是特征值与特征向量及它们的求解方法后，我们来讨论它们的应用问题。 对角化（Diagonalization） 首先给出对角化矩阵公式：S−1AS=ΛS^{-1}AS=\\LambdaS−1AS=Λ 其中，矩阵SSS是矩阵A的特征向量按列组成的，SSS称为特征向量矩阵（Eigenvector Matrices），矩阵Λ\\LambdaΛ称为对角特征值矩阵，其对角线上的元素为矩阵AAA的特征值，其余元素全部为000。 推导过程： 根据Axn=λnxnAx_n=\\lambda_n x_nAxn​=λn​xn​，我们将AS展开得到 AS=A[x1x2⋯xn]=[λ1x1λ2x2⋯λnxn]AS = A\\left[\\begin{matrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n\\end{matrix}\\right] = \\left[\\begin{matrix} \\lambda_1x_1 &amp; \\lambda_2x_2 &amp; \\cdots &amp; \\lambda_nx_n\\end{matrix}\\right] AS=A[x1​​x2​​⋯​xn​​]=[λ1​x1​​λ2​x2​​⋯​λn​xn​​] 将其写成矩阵形式 AS=[λ1x1λ2x2⋯λnxn]=[x1x2⋯xn][λ10⋯00λ2⋮⋮⋱00⋯0λn]=SΛAS = \\left[\\begin{matrix} \\lambda_1x_1 &amp; \\lambda_2x_2 &amp; \\cdots &amp; \\lambda_nx_n\\end{matrix}\\right] = \\left[\\begin{matrix} x_1 &amp; x_2 &amp; \\cdots &amp; x_n\\end{matrix}\\right]\\left[\\begin{matrix} \\lambda_1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\lambda_2 &amp; &amp; \\vdots \\\\ \\vdots &amp; &amp; \\ddots &amp; 0 \\\\ 0 &amp; \\cdots &amp; 0&amp;\\lambda_n\\end{matrix}\\right]=S\\Lambda AS=[λ1​x1​​λ2​x2​​⋯​λn​xn​​]=[x1​​x2​​⋯​xn​​]⎣⎢⎢⎢⎢⎡​λ1​0⋮0​0λ2​⋯​⋯⋱0​0⋮0λn​​⎦⎥⎥⎥⎥⎤​=SΛ 由于矩阵S中的列向量线性无关，因此矩阵S−1S^{-1}S−1必然存在，我们可以在矩阵两侧左乘逆矩阵得到 AS=SΛ⇒S−1AS=Λ⇒A=SΛS−1AS = S\\Lambda \\Rightarrow S^{-1}AS=\\Lambda \\Rightarrow A = S\\Lambda S^{-1}AS=SΛ⇒S−1AS=Λ⇒A=SΛS−1 因此我们得到了一种新的矩阵分解方式：A=SΛS−1A = S\\Lambda S^{-1}A=SΛS−1。它可以将矩阵AAA分解为特征向量矩阵、对称特征值矩阵与 特征向量矩阵的逆的乘积。我们将这一过程称为矩阵AAA的对角化。它的作用使得求解矩阵的幂变得更为方便。 矩阵的幂 矩阵AAA的对角化对求解矩阵的幂有着至关重要的作用。我们先来探讨一个问题：A2A^2A2的特征值和特征向量会有什么变化？ 考虑A2A^2A2的特征向量和特征值，我们依然从Ax=λxAx=\\lambda xAx=λx开始，我们将等式两侧同乘AAA得 A2x=λAx=λ2xA^2x=\\lambda Ax = \\lambda^2xA2x=λAx=λ2x 这说明A2A^2A2和AAA得特征向量相同，而特征值为λ2\\lambda^2λ2，写成对角化形式有 A2=SΛS−1SΛS−1=SΛ2S−1A^2 = S\\Lambda S^{-1}S\\Lambda S^{-1} = S\\Lambda^2S^{-1}A2=SΛS−1SΛS−1=SΛ2S−1 将其以此类推可知 Ak=SΛkS−1A^k = S\\Lambda^kS^{-1}Ak=SΛkS−1 即矩阵AkA^kAk与矩阵AAA的特征向量相同，特征值为λk\\lambda^kλk。这就启示我们：如果要求一个矩阵AAA的kkk次幂，我们可以先对矩阵AAA进行对角化分解，再求其对角特征值矩阵的kkk次幂即可。 因此我们可以推出一个结论：如果矩阵AAA具有nnn个线性无关的特征向量，如果所有特征值均满足∣λi∣&lt;1|\\lambda_i|&lt;1∣λi​∣&lt;1，则当k→∞k \\rightarrow \\inftyk→∞时，Ak→0A^k \\rightarrow 0Ak→0。 ","link":"https://albertlidesign.github.io/post/linearalgebra7/"},{"title":"线性代数(6)：行列式","content":"行列式及其性质 前面的章节已经学习了大量关于矩阵的知识，现在我们来集中探讨一下方阵的性质，其中行列式和特征值是重中之重，本章来单独讨论行列式。 行列式（Determinants） 行列式是每个方阵都具有的值，我们将矩阵AAA的行列式记作det(A)=∣A∣det(A) = |A|det(A)=∣A∣。行列式将很多矩阵信息压缩到这一个数值中，例如矩阵的不可逆（奇异矩阵）与行列式的值为000等价（也就是说行列式可以直接判断矩阵是否可逆）。 性质 我们先从行列式最主要的三个性质开始讲起，因为这三个性质定义了行列式，然后再拓展到其他性质上。 （1）单位矩阵的行列式为111。 例如二维单位矩阵：det(I)=∣100010001∣=1det(I) = \\begin{vmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\end{vmatrix} = 1det(I)=∣∣∣∣∣∣​100​010​001​∣∣∣∣∣∣​=1 （2）如果发生行交换，那么行列式的正负号会改变。 将性质（1）和性质（2）结合在一起，就能得到所有置换矩阵PPP的行列式。 例如 ∣1001∣=1\\begin{vmatrix} 1 &amp; 0 \\\\ 0 &amp; 1\\end{vmatrix} = 1 ∣∣∣∣​10​01​∣∣∣∣​=1 ∣0110∣=−1\\begin{vmatrix} 0 &amp; 1 \\\\ 1 &amp; 0\\end{vmatrix} = -1 ∣∣∣∣​01​10​∣∣∣∣​=−1 通过该性质还可以得出，置换矩阵PPP具有奇偶性，也就是说，一个矩阵不可能经过奇数次置换得到和偶数次置换相同的方阵。 性质（3）有两个，分别为 （3）a. ∣tatbcd∣=t∣abcd∣\\begin{vmatrix} ta &amp; tb \\\\ c &amp; d\\end{vmatrix} = t\\begin{vmatrix} a &amp; b \\\\ c &amp; d\\end{vmatrix}∣∣∣∣​tac​tbd​∣∣∣∣​=t∣∣∣∣​ac​bd​∣∣∣∣​ （3）b. ∣a+a′b+b′cd∣=∣abcd∣+∣a′b′cd∣\\begin{vmatrix} a+a&#x27; &amp; b+b&#x27; \\\\ c &amp; d\\end{vmatrix} = \\begin{vmatrix} a &amp; b \\\\ c &amp; d\\end{vmatrix}+\\begin{vmatrix} a&#x27; &amp; b&#x27; \\\\ c &amp; d\\end{vmatrix}∣∣∣∣​a+a′c​b+b′d​∣∣∣∣​=∣∣∣∣​ac​bd​∣∣∣∣​+∣∣∣∣​a′c​b′d​∣∣∣∣​ 为什么说由以上三个性质可以定义行列式，因为行列式其余的性质皆可由上述三个性质推导而出，以下是行列式其余的性质及它们的推导过程。 （4）如果矩阵中的两行相等，则它的行列式为000。 矩阵中的两行相等，意味着发生两行交换时，行列式不变，根据性质（2）：“如果发生行交换，那么行列式的正负号会改变。”，那么行列式只能为000。 （5）行列式不因消元操作而改变。 证明： ∣abc−lad−lb∣=∣abcd∣+∣ab−la−lb∣(性质(3)b.)=∣abcd∣−l∣abab∣(性质(3)a.)=∣abcd∣(性质(4))\\begin{vmatrix} a &amp; b \\\\ c-la &amp; d-lb\\end{vmatrix} = \\begin{vmatrix} a &amp; b \\\\ c &amp; d\\end{vmatrix} + \\begin{vmatrix} a &amp; b \\\\ -la &amp; -lb\\end{vmatrix}(性质(3)b.) = \\begin{vmatrix} a &amp; b \\\\ c &amp; d\\end{vmatrix} - l\\begin{vmatrix} a &amp; b \\\\ a &amp; b\\end{vmatrix}(性质(3)a.) = \\begin{vmatrix} a &amp; b \\\\ c &amp; d\\end{vmatrix}(性质(4)) ∣∣∣∣​ac−la​bd−lb​∣∣∣∣​=∣∣∣∣​ac​bd​∣∣∣∣​+∣∣∣∣​a−la​b−lb​∣∣∣∣​(性质(3)b.)=∣∣∣∣​ac​bd​∣∣∣∣​−l∣∣∣∣​aa​bb​∣∣∣∣​(性质(3)a.)=∣∣∣∣​ac​bd​∣∣∣∣​(性质(4)) （6）若矩阵中有一行是000，那么行列式为000 矩阵中有一行是0，可以看作∣00cd∣=∣a×0b×0cd∣\\begin{vmatrix} 0 &amp; 0 \\\\ c &amp; d\\end{vmatrix} = \\begin{vmatrix} a×0 &amp; b×0 \\\\ c &amp; d\\end{vmatrix}∣∣∣∣​0c​0d​∣∣∣∣​=∣∣∣∣​a×0c​b×0d​∣∣∣∣​，那么有 ∣a×0b×0cd∣=0∣abcd∣=0\\begin{vmatrix} a×0 &amp; b×0 \\\\ c &amp; d\\end{vmatrix} = 0\\begin{vmatrix} a &amp; b \\\\ c &amp; d\\end{vmatrix} = 0∣∣∣∣​a×0c​b×0d​∣∣∣∣​=0∣∣∣∣​ac​bd​∣∣∣∣​=0 （7）对于三角阵的行列式，主元的乘积等于行列式。例如在四维中，设上三角矩阵U=[u11u12u13u140u22u23u2400u33u34000u44]U=\\left[\\begin{matrix} u_{11} &amp; u_{12} &amp; u_{13} &amp; u_{14} \\\\ 0 &amp; u_{22} &amp; u_{23} &amp; u_{24} \\\\ 0 &amp; 0 &amp; u_{33} &amp; u_{34} \\\\ 0 &amp; 0 &amp; 0 &amp; u_{44}\\end{matrix}\\right]U=⎣⎢⎢⎡​u11​000​u12​u22​00​u13​u23​u33​0​u14​u24​u34​u44​​⎦⎥⎥⎤​，则det(U)=u11×u22×u33×u44det(U) = u_{11}×u_{22}×u_{33}×u_{44}det(U)=u11​×u22​×u33​×u44​，nnn维同理。 对于三角矩阵，我们可以通过不断地消元最终得到对角矩阵，例如，通过消元法可以得到 U=[u11u12u13u140u22u23u2400u33u34000u44]=[u110000u220000u330000u44]=D（对角矩阵）U=\\left[\\begin{matrix} u_{11} &amp; u_{12} &amp; u_{13} &amp; u_{14} \\\\ 0 &amp; u_{22} &amp; u_{23} &amp; u_{24} \\\\ 0 &amp; 0 &amp; u_{33} &amp; u_{34} \\\\ 0 &amp; 0 &amp; 0 &amp; u_{44}\\end{matrix}\\right] = \\left[\\begin{matrix} u_{11} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; u_{22} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; u_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; u_{44}\\end{matrix}\\right] = D（对角矩阵）U=⎣⎢⎢⎡​u11​000​u12​u22​00​u13​u23​u33​0​u14​u24​u34​u44​​⎦⎥⎥⎤​=⎣⎢⎢⎡​u11​000​0u22​00​00u33​0​000u44​​⎦⎥⎥⎤​=D（对角矩阵） 那么我们再利用性质（3）a.来证明对角矩阵的行列式就是对角线元素相乘 ∣u110000u220000u330000u44∣=u11u22u33u44∣1000010000100001∣\\begin{vmatrix} u_{11} &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; u_{22} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; u_{33} &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; u_{44}\\end{vmatrix} =u_{11} u_{22} u_{33}u_{44}\\begin{vmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{vmatrix}∣∣∣∣∣∣∣∣​u11​000​0u22​00​00u33​0​000u44​​∣∣∣∣∣∣∣∣​=u11​u22​u33​u44​∣∣∣∣∣∣∣∣​1000​0100​0010​0001​∣∣∣∣∣∣∣∣​ （8）det(A)=0det(A) = 0det(A)=0，则矩阵AAA为奇异矩阵。相反，若det(A)=0det(A) \\not= 0det(A)​=0，则矩阵AAA可逆。 因为如果A可逆，化简后能得到矩阵各列都含非0主元，得到三角矩阵，再利用性质（7）得到其行列式。 （9）det(AB)=det(A)det(B)det(AB) = det(A)det(B)det(AB)=det(A)det(B) 这意味着A−1A=I⇒det(A−1)det(A)=det(I)=1⇒det(A−1)=1det(A)A^{-1}A = I \\Rightarrow det(A^{-1})det(A) = det(I) = 1 \\Rightarrow det(A^{-1}) = \\frac{1}{det(A)}A−1A=I⇒det(A−1)det(A)=det(I)=1⇒det(A−1)=det(A)1​，这也可以作为本性质的证明，也可以用对角阵AAA和BBB，但是我们必须一步步进行消元，整个证明过程需要是非耐心，最终证明该性质对任意矩阵成立。 同时本性质还能推出 det(A2)=det(A)det(A)=det(A)2det(A^2) = det(A)det(A)=det(A)^2det(A2)=det(A)det(A)=det(A)2 这说明如果矩阵进行平方，那么它的行列式也会平方。 此外，本性质还能推出 det(2A)=2ndet(A)det(2A) = 2^ndet(A)det(2A)=2ndet(A) 因为对一个n×nn×nn×n矩阵，将矩阵翻倍意味着各列向量都翻倍，一共翻倍nnn次，因此行列式变成了2n2^n2n倍。 （10）det(AT)=det(A)det(A^T) = det(A)det(AT)=det(A) 证明： 根据A=LUA = LUA=LU，有 det(A)=det(LU)=det(L)det(U)det(A) = det(LU)=det(L)det(U)det(A)=det(LU)=det(L)det(U) det(AT)=det((LU)T)=det(UTLT)=det(UT)det(LT)det(A^T) = det((LU)^T) = det(U^TL^T)= det(U^T)det(L^T)det(AT)=det((LU)T)=det(UTLT)=det(UT)det(LT) 由于LLL和UUU都是三角矩阵，因此它们的行列式都是对角线的乘积，因此 det(L)=det(LT)det(U)=det(UT)det(L) = det(L^T) \\quad det(U) = det(U^T)det(L)=det(LT)det(U)=det(UT) 所以最后我们得出 det(AT)=det(A)det(A^T) = det(A)det(AT)=det(A) 行列式的计算 对于行列式的计算，我们先来推导二维行列式的求解过程。 ∣abcd∣=∣a0cd∣+∣0bcd∣=∣a0c0∣+∣a00d∣+∣0bc0∣+∣0b0d∣=∣a00d∣+∣0bc0∣=ad−bc\\begin{vmatrix} a &amp; b \\\\ c &amp; d\\end{vmatrix} = \\begin{vmatrix} a &amp; 0 \\\\ c &amp; d\\end{vmatrix}+\\begin{vmatrix} 0 &amp; b \\\\ c &amp; d\\end{vmatrix} = \\begin{vmatrix} a &amp; 0 \\\\ c &amp; 0\\end{vmatrix}+\\begin{vmatrix} a &amp; 0 \\\\ 0 &amp; d\\end{vmatrix}+\\begin{vmatrix} 0 &amp; b \\\\ c &amp; 0\\end{vmatrix}+\\begin{vmatrix} 0 &amp; b \\\\ 0 &amp; d\\end{vmatrix} =\\begin{vmatrix} a &amp; 0 \\\\ 0 &amp; d\\end{vmatrix} + \\begin{vmatrix} 0 &amp; b \\\\ c &amp; 0\\end{vmatrix} = ad-bc ∣∣∣∣​ac​bd​∣∣∣∣​=∣∣∣∣​ac​0d​∣∣∣∣​+∣∣∣∣​0c​bd​∣∣∣∣​=∣∣∣∣​ac​00​∣∣∣∣​+∣∣∣∣​a0​0d​∣∣∣∣​+∣∣∣∣​0c​b0​∣∣∣∣​+∣∣∣∣​00​bd​∣∣∣∣​=∣∣∣∣​a0​0d​∣∣∣∣​+∣∣∣∣​0c​b0​∣∣∣∣​=ad−bc 观察二维行列式的求解过程，我们发现，行列式的求解取决于那些分解后非零行列式的和，即各行各列均有非零元素的行列式。因此我们按照这个规律，继续推导三维行列式，我们这次只写出非000项，有 ∣a11a12a13a21a22a23a31a32a33∣=∣a11000a22000a33∣+∣a110000a230a320∣+∣0a120a210000a33∣+∣0a12000a23a3100∣\\begin{vmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{vmatrix} = \\begin{vmatrix} a_{11} &amp; 0 &amp; 0 \\\\ 0 &amp; a_{22} &amp; 0 \\\\ 0 &amp; 0 &amp; a_{33} \\end{vmatrix} + \\begin{vmatrix} a_{11} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; a_{23} \\\\ 0 &amp; a_{32} &amp; 0 \\end{vmatrix} + \\begin{vmatrix} 0 &amp; a_{12} &amp; 0 \\\\ a_{21} &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; a_{33} \\end{vmatrix}+\\begin{vmatrix} 0 &amp; a_{12} &amp; 0 \\\\ 0 &amp; 0 &amp; a_{23} \\\\ a_{31} &amp; 0 &amp; 0 \\end{vmatrix}∣∣∣∣∣∣​a11​a21​a31​​a12​a22​a32​​a13​a23​a33​​∣∣∣∣∣∣​=∣∣∣∣∣∣​a11​00​0a22​0​00a33​​∣∣∣∣∣∣​+∣∣∣∣∣∣​a11​00​00a32​​0a23​0​∣∣∣∣∣∣​+∣∣∣∣∣∣​0a21​0​a12​00​00a33​​∣∣∣∣∣∣​+∣∣∣∣∣∣​00a31​​a12​00​0a23​0​∣∣∣∣∣∣​ +∣00a13a21000a320∣+∣00a130a220a3100∣=a11a22a33−a11a23a32−a12a21a33+a12a23a31+a13a21a32−a13a22a31+ \\begin{vmatrix} 0 &amp; 0 &amp; a_{13} \\\\ a_{21} &amp; 0 &amp; 0 \\\\ 0 &amp; a_{32} &amp; 0 \\end{vmatrix} + \\begin{vmatrix} 0 &amp; 0 &amp; a_{13} \\\\ 0 &amp; a_{22} &amp; 0 \\\\ a_{31} &amp; 0 &amp; 0 \\end{vmatrix} = a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} -a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}+∣∣∣∣∣∣​0a21​0​00a32​​a13​00​∣∣∣∣∣∣​+∣∣∣∣∣∣​00a31​​0a22​0​a13​00​∣∣∣∣∣∣​=a11​a22​a33​−a11​a23​a32​−a12​a21​a33​+a12​a23​a31​+a13​a21​a32​−a13​a22​a31​ 可以发现规律，因为各行各列均需有非零元素，所以对于n×nn×nn×n的矩阵，其行列式分解后的非零项有n!n!n!个。 同理，我们根据nnn阶行列式可以分解为n!n!n!个非零行列式来推到出高维行列式的一般求解公式，即 ∣A∣=∑n!±a1αa2βa3γ...anψ(α,β,γ,...,ψ为1到n的某种排列)=Pnn|A| = \\sum_{n!}\\pm a_{1\\alpha}a_{2\\beta}a_{3\\gamma}...a_{n\\psi}(\\alpha,\\beta,\\gamma,...,\\psi为1到n的某种排列) = P^n_{n} ∣A∣=n!∑​±a1α​a2β​a3γ​...anψ​(α,β,γ,...,ψ为1到n的某种排列)=Pnn​ 例 求∣A∣=∣0011011011001001∣|A| = \\begin{vmatrix} 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 1 \\end{vmatrix}∣A∣=∣∣∣∣∣∣∣∣​0011​0110​1100​1001​∣∣∣∣∣∣∣∣​ 如果检查该行列式分解出的24项会发现其中有22项为000，剩下的非零行列式为 ∣0001001001001000∣=1和∣0010010010000001∣=−1\\begin{vmatrix} 0 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 \\end{vmatrix} = 1和\\begin{vmatrix} 0 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0 &amp; 1 \\end{vmatrix} = -1∣∣∣∣∣∣∣∣​0001​0010​0100​1000​∣∣∣∣∣∣∣∣​=1和∣∣∣∣∣∣∣∣​0010​0100​1000​0001​∣∣∣∣∣∣∣∣​=−1 因此∣A∣=1−1=0|A| = 1-1 = 0∣A∣=1−1=0。 代数余子式（Cofactors） 接下来引入代数余子式的概念，它的作用是把nnn阶行列式化简为n−1n-1n−1阶行列式。 先来看3×33×33×3行列式的情况，上一节我们得到了 ∣a11a12a13a21a22a23a31a32a33∣=a11a22a33−a11a23a32−a12a21a33+a12a23a31+a13a21a32−a13a22a31\\begin{vmatrix} a_{11} &amp; a_{12} &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; a_{23} \\\\ a_{31} &amp; a_{32} &amp; a_{33} \\end{vmatrix} = a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} -a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31}∣∣∣∣∣∣​a11​a21​a31​​a12​a22​a32​​a13​a23​a33​​∣∣∣∣∣∣​=a11​a22​a33​−a11​a23​a32​−a12​a21​a33​+a12​a23​a31​+a13​a21​a32​−a13​a22​a31​ 那么我们以行列式第一行的三个元素来合并同类项，可以得到 a11a22a33−a11a23a32−a12a21a33+a12a23a31+a13a21a32−a13a22a31=a11(a22a33−a23a32)+a12(−a21a33+a23a31)+a13(a21a32−a22a31)a_{11}a_{22}a_{33} - a_{11}a_{23}a_{32} -a_{12}a_{21}a_{33} + a_{12}a_{23}a_{31}+a_{13}a_{21}a_{32}-a_{13}a_{22}a_{31} \\\\ = a_{11}(a_{22}a_{33} - a_{23}a_{32}) + a_{12}(-a_{21}a_{33} + a_{23}a_{31}) + a_{13}(a_{21}a_{32}-a_{22}a_{31})a11​a22​a33​−a11​a23​a32​−a12​a21​a33​+a12​a23​a31​+a13​a21​a32​−a13​a22​a31​=a11​(a22​a33​−a23​a32​)+a12​(−a21​a33​+a23​a31​)+a13​(a21​a32​−a22​a31​) 合并同类项后，我们又可以把新的三个项看作是三个矩阵的行列式 a11(a22a33−a23a32)+a12(−a21a33+a23a31)+a13(a21a32−a22a31)=∣a11000a22a230a32a33∣−∣0a120a210a23a310a33∣+∣00a13a21a220a31a320∣a_{11}(a_{22}a_{33} - a_{23}a_{32}) + a_{12}(-a_{21}a_{33} + a_{23}a_{31}) + a_{13}(a_{21}a_{32}-a_{22}a_{31}) \\\\= \\begin{vmatrix} a_{11} &amp; 0 &amp; 0 \\\\ 0 &amp; a_{22} &amp; a_{23} \\\\ 0 &amp; a_{32} &amp; a_{33} \\end{vmatrix} - \\begin{vmatrix} 0 &amp; a_{12} &amp; 0 \\\\ a_{21} &amp; 0 &amp; a_{23} \\\\ a_{31} &amp; 0 &amp; a_{33} \\end{vmatrix} + \\begin{vmatrix} 0 &amp; 0 &amp; a_{13} \\\\ a_{21} &amp; a_{22} &amp; 0 \\\\ a_{31} &amp; a_{32} &amp; 0 \\end{vmatrix}a11​(a22​a33​−a23​a32​)+a12​(−a21​a33​+a23​a31​)+a13​(a21​a32​−a22​a31​)=∣∣∣∣∣∣​a11​00​0a22​a32​​0a23​a33​​∣∣∣∣∣∣​−∣∣∣∣∣∣​0a21​a31​​a12​00​0a23​a33​​∣∣∣∣∣∣​+∣∣∣∣∣∣​0a21​a31​​0a22​a32​​a13​00​∣∣∣∣∣∣​ 由此我们定义aija_{ij}aij​的代数余子式：将原行列式的第iii行与第jjj列抹去后得到的n−1n-1n−1阶行列式记为CijC_{ij}Cij​，i+ji+ji+j为偶数时，该项前的符号为+++，i+ji+ji+j为奇数时，该项前的符号为−-−，规律如下 ∣+−+−+−+−+∣\\begin{vmatrix} + &amp; - &amp; + \\\\ - &amp; + &amp; - \\\\ + &amp; - &amp; + \\end{vmatrix}∣∣∣∣∣∣​+−+​−+−​+−+​∣∣∣∣∣∣​ 例a11a_{11}a11​的代数余子式为 C11=(−1)1+1∣a22a23a32a33∣C_{11} = (-1)^{1+1}\\begin{vmatrix} a_{22} &amp; a_{23} \\\\ a_{32} &amp; a_{33} \\end{vmatrix}C11​=(−1)1+1∣∣∣∣​a22​a32​​a23​a33​​∣∣∣∣​ 因此，将矩阵AAA沿第一行展开的公式为 ∣A∣=a11C11+a12C12+...+a1nC1n|A| = a_{11}C_{11} + a_{12}C_{12}+...+a_{1n}C_{1n}∣A∣=a11​C11​+a12​C12​+...+a1n​C1n​ 例 ∣A1∣=1∣A2∣=∣1111∣=0|A_1| = 1 \\quad |A_2| = \\begin{vmatrix} 1 &amp; 1 \\\\ 1 &amp; 1 \\end{vmatrix}= 0∣A1​∣=1∣A2​∣=∣∣∣∣​11​11​∣∣∣∣​=0 ∣A3∣=∣110111011∣=∣100011011∣+∣010101001∣+∣000110010∣=0−1+0=−1|A_3| = \\begin{vmatrix} 1 &amp; 1 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\end{vmatrix}= \\begin{vmatrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\end{vmatrix} + \\begin{vmatrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 \\end{vmatrix} + \\begin{vmatrix} 0 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\end{vmatrix} = 0-1+0 = -1∣A3​∣=∣∣∣∣∣∣​110​111​011​∣∣∣∣∣∣​=∣∣∣∣∣∣​100​011​011​∣∣∣∣∣∣​+∣∣∣∣∣∣​010​100​011​∣∣∣∣∣∣​+∣∣∣∣∣∣​010​011​000​∣∣∣∣∣∣​=0−1+0=−1 求∣A4∣|A_4|∣A4​∣、∣A5∣|A_5|∣A5​∣、∣A6∣|A_6|∣A6​∣、∣A7∣|A_7|∣A7​∣ ∣A4∣=∣1100111001110011∣=∣1000011001110011∣+∣0100101000110011∣=1⋅∣A3∣+(−1)⋅∣A2∣=0|A_4| = \\begin{vmatrix} 1 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{vmatrix} = \\begin{vmatrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{vmatrix} + \\begin{vmatrix} 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 1 &amp; 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 1 &amp; 1 \\end{vmatrix} = 1·|A_3| + (-1)·|A_2| = 0∣A4​∣=∣∣∣∣∣∣∣∣​1100​1110​0111​0011​∣∣∣∣∣∣∣∣​=∣∣∣∣∣∣∣∣​1000​0110​0111​0011​∣∣∣∣∣∣∣∣​+∣∣∣∣∣∣∣∣​0100​1000​0111​0011​∣∣∣∣∣∣∣∣​=1⋅∣A3​∣+(−1)⋅∣A2​∣=0 发现规律：∣An∣=∣An−1∣−∣An−2∣|A_n| = |A_{n-1}| - |A_{n-2}|∣An​∣=∣An−1​∣−∣An−2​∣，因此可知 ∣A5∣=0∣A6∣=1∣A7∣=1|A_5| = 0 \\quad |A_6| = 1 \\quad |A_7| = 1∣A5​∣=0∣A6​∣=1∣A7​∣=1 会发现，随着维度增加，行列式的值呈现1，0，−1，−1，0，11，0，-1，-1，0，11，0，−1，−1，0，1，以这样666个值循环，因此周期为666。 小节 至此，我们掌握了三种方法来求一个方阵的行列式： 消元法（LULULU分解）将矩阵AAA化为UUU，∣A∣|A|∣A∣就是主元的乘积（最简单） 使用代数余子式按某一行展开（稍复杂） 按行列式公式完全展开计算，需要求n!n!n!项之积（很复杂） 行列式的应用 逆矩阵公式 我们已经接触到很多逆矩阵了，但是一直没有给出逆矩阵的公式，你可以通过Gauss-Jordan消元法来求矩阵的逆，不过现在学习了行列式，可以直接求逆矩阵。 我们已经知道二阶逆矩阵的公式为： [abcd]−1=1ad−bc[d−b−ca]\\left[\\begin{matrix} a &amp; b \\\\ c &amp; d \\end{matrix}\\right] ^{-1} = \\frac{1}{ad-bc}\\left[\\begin{matrix} d &amp; -b \\\\ -c &amp; a \\end{matrix}\\right][ac​bd​]−1=ad−bc1​[d−c​−ba​] 那么我们能否通过二阶公式来推导至更高维度？ 通过观察公式我们发现： 1ad−bc\\frac{1}{ad-bc}ad−bc1​实际就是矩阵AAA的行列式的倒数 后面的矩阵中的ddd实际上是矩阵AAA中的aaa的代数余子式，同理，−b-b−b实际上就是ccc的代数余子式 因此可以得出，逆矩阵公式为 A−1=1det(A)CTA^{-1} = \\frac{1}{det(A)}C^TA−1=det(A)1​CT 等式右侧矩阵外的因子，其分母是矩阵的行列式，而矩阵为代数余子式矩阵（Cofactor Matrix）CCC的转置，称为伴随矩阵（Adjoint Matrix）。因此矩阵AAA的逆就是矩阵行列式的倒数与其伴随矩阵的乘积。 那么为什么是这个公式呢？我们来验证一下，假设等式成立，首先将等式两边都乘上矩阵AAA得到 AA−1=A1det(A)CT⇒ACT=det(A)IAA^{-1} = A\\frac{1}{det(A)}C^T \\Rightarrow AC^T = det(A)IAA−1=Adet(A)1​CT⇒ACT=det(A)I 因此，若逆矩阵公式成立其实就是判断ACTAC^TACT是否与det(A)Idet(A)Idet(A)I相等。 ACT=[a11⋯a1n⋮⋱⋮an1⋯ann][C11⋯Cn1⋮⋱⋮C1n⋯Cnn]AC^T = \\left[\\begin{matrix} a_{11} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{matrix}\\right]\\left[\\begin{matrix} C_{11} &amp; \\cdots &amp; C_{n1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ C_{1n} &amp; \\cdots &amp; C_{nn} \\end{matrix}\\right]ACT=⎣⎢⎡​a11​⋮an1​​⋯⋱⋯​a1n​⋮ann​​⎦⎥⎤​⎣⎢⎡​C11​⋮C1n​​⋯⋱⋯​Cn1​⋮Cnn​​⎦⎥⎤​ 根据矩阵相乘，我们观察发现，矩阵ACTAC^TACT的第一行第一列元素等于矩阵AAA第一行和矩阵CTC^TCT第一列进行点积，计算可得 ∑j=1na1jC1j=det(A)\\sum_{j=1}^{n}a_{1j}C_{1j} = det(A)∑j=1n​a1j​C1j​=det(A) 也就是说，它们的点积其实就是矩阵AAA的行列式计算公式，而ACTAC^TACT对角线上的所有元素都是如此，因此我们可以得到，它们相乘后的矩阵，其对角线处全部都是行列式。那么非对角元素呢？以第二行第一列为例，相乘我们发现，各个代数余子式的形式不变，但是与代数余子式相乘的变为了矩阵AAA第二行第jjj列元素。因此这个形式相当于用矩阵AAA第二行的元素替代第一行的元素得到的矩阵，前两行的元素相同，因此按照行列式性质（4），其值为000。 ∑j=1na2jC1j=∣a21a22⋯⋯a2na21a22⋯⋯a2na31a32⋱a3n⋮⋱⋮an1an2⋯⋯ann∣=0\\sum_{j=1}^{n}a_{2j}C_{1j} = \\begin{vmatrix} a_{21} &amp; a_{22} &amp; \\cdots &amp; \\cdots &amp; a_{2n} \\\\ a_{21} &amp; a_{22} &amp; \\cdots &amp; \\cdots &amp; a_{2n} \\\\ a_{31} &amp; a_{32} &amp; \\ddots &amp; &amp; a_{3n} \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; a_{n2} &amp; \\cdots &amp; \\cdots &amp; a_{nn} \\end{vmatrix}= 0∑j=1n​a2j​C1j​=∣∣∣∣∣∣∣∣∣∣∣​a21​a21​a31​⋮an1​​a22​a22​a32​an2​​⋯⋯⋱⋯​⋯⋯⋱⋯​a2n​a2n​a3n​⋮ann​​∣∣∣∣∣∣∣∣∣∣∣​=0 因此最后我们得到 ACT=[a11⋯a1n⋮⋱⋮an1⋯ann][C11⋯Cn1⋮⋱⋮C1n⋯Cnn]=[det(A)0⋯00det(A)⋯0⋮⋮⋱⋮00⋯det(A)]I=det(A)IAC^T = \\left[\\begin{matrix} a_{11} &amp; \\cdots &amp; a_{1n} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ a_{n1} &amp; \\cdots &amp; a_{nn} \\end{matrix}\\right]\\left[\\begin{matrix} C_{11} &amp; \\cdots &amp; C_{n1} \\\\ \\vdots &amp; \\ddots &amp; \\vdots \\\\ C_{1n} &amp; \\cdots &amp; C_{nn} \\end{matrix}\\right] = \\left[\\begin{matrix} det(A) &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; det(A) &amp; \\cdots &amp; 0\\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; det(A) \\end{matrix}\\right]I= det(A)IACT=⎣⎢⎡​a11​⋮an1​​⋯⋱⋯​a1n​⋮ann​​⎦⎥⎤​⎣⎢⎡​C11​⋮C1n​​⋯⋱⋯​Cn1​⋮Cnn​​⎦⎥⎤​=⎣⎢⎢⎢⎡​det(A)0⋮0​0det(A)⋮0​⋯⋯⋱⋯​00⋮det(A)​⎦⎥⎥⎥⎤​I=det(A)I 克莱姆法则（Cramer's Rule） 对于可逆矩阵AAA，方程Ax=bAx=bAx=b必有解x=A−1bx=A^{-1}bx=A−1b，将逆矩阵的公式代入，那么 x=A−1b=1det(A)CTbx=A^{-1}b=\\frac{1}{det(A)}C^Tbx=A−1b=det(A)1​CTb 克莱姆法则（Cramer's Rule）则是从另一个角度来看待这个公式，即xxx的分量xjx_jxj​为xj=det(Bj)det(A)x_j = \\frac{det(B_j)}{det(A)}xj​=det(A)det(Bj​)​ 其中，矩阵BjB_jBj​为用向量bbb替换矩阵AAA的第jjj列所得到的新矩阵。例如 B1=[b1a12⋯⋯a1nb2a22⋯⋯a2nb3a32⋱a3n⋮⋱⋮bnan2⋯⋯ann]B_1 = \\left[\\begin{matrix} b_1 &amp; a_{12} &amp; \\cdots &amp; \\cdots &amp; a_{1n} \\\\ b_2 &amp; a_{22} &amp; \\cdots &amp; \\cdots &amp; a_{2n}\\\\ b_3 &amp; a_{32} &amp; \\ddots &amp; &amp; a_{3n} \\\\ \\vdots &amp; &amp; &amp; \\ddots &amp; \\vdots \\\\ b_n &amp; a_{n2} &amp; \\cdots &amp; \\cdots &amp; a_{nn} \\end{matrix}\\right]B1​=⎣⎢⎢⎢⎢⎢⎡​b1​b2​b3​⋮bn​​a12​a22​a32​an2​​⋯⋯⋱⋯​⋯⋯⋱⋯​a1n​a2n​a3n​⋮ann​​⎦⎥⎥⎥⎥⎥⎤​ 矩阵BjB_jBj​的行列式的值从第j列用代数余子式进行展开计算，正好是伴随矩阵CTC^TCT的第j行，与向量bbb点积的结果。 但是相较于高斯消元法，克莱姆法则计算方程的解的效率较低，它仅仅只是提供了一个代数表达式，让人们能代数运算而不是写算法。 行列式的几何意义 在二维中，行列式的几何意义其实就是矩阵所对应的线性变换所改变由空间中两基向量构成的矩形的面积的比例，对应到三维就是对应空间中三个基向量对应的平行六面体的体积的比例。 例：我们在二维空间中以一组基向量x=[10],y=[01]x=\\left[\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right],y = \\left[\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right]x=[10​],y=[01​]为例，两个基向量构成了一个单位矩形面积为111，假设现在要进行线性变换，变换矩阵为[3202]\\left[\\begin{matrix} 3 &amp; 2 \\\\ 0 &amp; 2 \\end{matrix}\\right][30​22​]，即变换后xxx轴单位向量由x=[10]x=\\left[\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right]x=[10​]变为[30]\\left[\\begin{matrix} 3 \\\\ 0 \\end{matrix}\\right][30​]，对应地，y轴单位向量由y=[01]y = \\left[\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right]y=[01​]变为[22]\\left[\\begin{matrix} 2 \\\\ 2 \\end{matrix}\\right][22​]，变换后我们发现，矩形面积变成了666，是原单位矩形面积的666倍，其值其实就是行列式∣3202∣=6\\begin{vmatrix} 3 &amp; 2 \\\\ 0 &amp; 2 \\end{vmatrix} = 6∣∣∣∣​30​22​∣∣∣∣​=6。 当矩阵的行列式为000时，这意味着线性变换后，原空间被压缩成了一条线或一个点（二维情况下），即原空间的维度降低。这也解释了为何矩阵列向量线性相关时，经过它的线性变换后，原空间发生维度降低，其实就是因为它是奇异矩阵，它的行列式为0。 当矩阵的行列式为负数时，表示矩阵的线性变换将原空间的定向发生了改变，如平面翻转。更为直观的就是变换后坐标轴的相对位置会发生变换。在三维中，一般情况下我们使用右手定则来构建坐标系，若此时现线性变换的矩阵行列式为负数时，线性变换后新的坐标系就会变成满足左手定则。但是行列式的绝对值大小仍然决定其体积的缩放比例。 ","link":"https://albertlidesign.github.io/post/linearalgebra(6)/"},{"title":"线性代数(5)：QR分解","content":"前言 前文我们讲解了投影矩阵和最小二乘法，本节我们深化正交基和正交矩阵的概念和性质，讨论QR分解以及将一组向量转化为标准正交向量组的方法：Gram-Schmidt正交化。 矩阵分解 QR分解 分解形式 A=QRA=QRA=QR(QQQ代表标准正交矩阵，RRR代表非奇异上三角矩阵) 目的 (1)求解A的特征值；(2)求解A的逆；(3)求解线性最小二乘问题。 标准正交矩阵 标准正交向量（Orthonormal Vector） 我们用qqq表示单位向量，那么有q1,q2,q3,...,qnq_1,q_2,q_3,...,q_nq1​,q2​,q3​,...,qn​，若所有的向量qqq满足 qiTqj={0i=j1i=jq_i^Tq_j = \\begin{cases} 0 \\quad i\\not=j \\\\1 \\quad i=j \\end{cases} qiT​qj​={0i​=j1i=j​ 此时，我们称qqq为标准正交向量（Orthonormal Vector）。当i=ji=ji=j时，qiTqj=1q_i^Tq_j = 1qiT​qj​=1表示向量长度为单位长度。当i=ji\\not= ji​=j时，qiTqj=0q_i^Tq_j=0qiT​qj​=0表示不同的向量之间是正交的。 标准正交矩阵（Orthonormal Matrix） 将标准正交向量qqq放入矩阵QQQ中，得到Q=[q1q2...qn]Q=\\left[\\begin{matrix}q_1&amp; q_2&amp; ...&amp; q_n\\end{matrix}\\right]Q=[q1​​q2​​...​qn​​]，我们称这样的矩阵QQQ为标准正交矩阵（Orthonormal Matrices）。标准正交矩阵满足以下性质： QTQ=[q1Tq2T...qnT][q1q2...qn]=[10⋯001⋯⋯⋮⋮⋱⋮00⋯1]=IQ^TQ = \\left[\\begin{matrix}q_1^T \\\\ q_2^T \\\\ ...\\\\ q_n^T\\end{matrix}\\right]\\left[\\begin{matrix}q_1&amp; q_2&amp; ...&amp; q_n\\end{matrix}\\right] = \\left[\\begin{matrix}1 &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; 1 &amp; \\cdots &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp;\\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; 1 \\end{matrix}\\right] = IQTQ=⎣⎢⎢⎡​q1T​q2T​...qnT​​⎦⎥⎥⎤​[q1​​q2​​...​qn​​]=⎣⎢⎢⎢⎡​10⋮0​01⋮0​⋯⋯⋱⋯​0⋯⋮1​⎦⎥⎥⎥⎤​=I 当矩阵QQQ恰好为方阵时，由于其正交性，可知矩阵QQQ是可逆的，而QTQ=IQ^TQ = IQTQ=I，所以 Q^T = Q^{-1}，那么此时我们称，那么此时我们称，那么此时我们称Q$为正交矩阵(Orthogonal Matrix)。 示例 (1)Q=[010100001](1) \\quad Q = \\left[\\begin{matrix} 0 &amp; 1 &amp; 0 \\\\ 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1\\end{matrix}\\right](1)Q=⎣⎡​010​100​001​⎦⎤​ 此时QT=Q−1=[010001100]Q^T = Q^{-1} = \\left[\\begin{matrix} 0 &amp; 1 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 \\\\ 1 &amp; 0 &amp; 0\\end{matrix}\\right]QT=Q−1=⎣⎡​001​100​010​⎦⎤​，易得QTQ=IQ^TQ = IQTQ=I (2)Q=[cosθ−sinθsinθcosθ](2) \\quad Q = \\left[\\begin{matrix} cos\\theta &amp; -sin\\theta \\\\ sin\\theta &amp; cos\\theta \\end{matrix}\\right](2)Q=[cosθsinθ​−sinθcosθ​] 此时列向量长度为111，列向量相互正交 (3)Q=12[1−11−1](3) \\quad Q = \\frac{1}{\\sqrt{2}}\\left[\\begin{matrix} 1 &amp; -1 \\\\ 1 &amp; -1 \\end{matrix}\\right](3)Q=2​1​[11​−1−1​] 此时列向量长度为111，列向量相互正交 (4)Q=12[11111−11−111−1−11−1−11](4) \\quad Q = \\frac{1}{2}\\left[\\begin{matrix} 1 &amp; 1 &amp; 1 &amp; 1 \\\\ 1 &amp; -1 &amp; 1 &amp; -1 \\\\ 1&amp; 1 &amp; -1 &amp; -1 \\\\ 1 &amp; -1 &amp; -1 &amp; 1\\end{matrix}\\right](4)Q=21​⎣⎢⎢⎡​1111​1−11−1​11−1−1​1−1−11​⎦⎥⎥⎤​ (5)Q=13[1−222−1−2221](5) \\quad Q = \\frac{1}{3}\\left[\\begin{matrix} 1 &amp; -2 &amp; 2 \\\\ 2 &amp; -1 &amp; -2 \\\\ 2 &amp; 2 &amp; 1\\end{matrix}\\right](5)Q=31​⎣⎡​122​−2−12​2−21​⎦⎤​ 用途 上一篇文章我们知道投影矩阵P=A(ATA)−1ATP=A(A^TA)^{-1}A^TP=A(ATA)−1AT，当矩阵AAA为标准正交矩阵QQQ时，有 P=Q(QTQ)−1QT=QQTP = Q(Q^TQ)^{-1}Q^T = QQ^TP=Q(QTQ)−1QT=QQT 当QQQ是方阵时，QQT=IQQ^T=IQQT=I，那么其投影矩阵P=IP=IP=I。 验证： PT=P:(QTQ)T=(QT)TQT=QQTP^T = P: \\quad (Q^TQ)^T = (Q^T)^TQ^T = QQ^TPT=P:(QTQ)T=(QT)TQT=QQT P2=P:(QQT)2=QQTQQT=Q(QTQ)QT=QQTP^2 = P: \\quad (QQ^T)^2 = QQ^TQQ^T = Q(Q^TQ)Q^T = QQ^TP2=P:(QQT)2=QQTQQT=Q(QTQ)QT=QQT 将Q带入到最小二乘法公式（ATAx^=ATbA^TA\\hat{x} = A^TbATAx^=ATb）中，得到： QTQx^=QTb=&gt;x^=QTbQ^TQ\\hat{x} = Q^Tb =&gt; \\hat{x} = Q^TbQTQx^=QTb=&gt;x^=QTb 分解开即为 xi^=qiTb\\hat{x_i} = q_i^Tbxi​^​=qiT​b Gram-Schmidt正交化 两个向量的单位正交化 已知两个线性无关的向量a,ba,ba,b无法满足标准的正交（如图1所示），现在我们想通过Gram-Schmidt方法进行单位化和正交化，将其转化为标准的单位正交基q1,q2q_1,q_2q1​,q2​。 图1：无法满足标准正交的两个向量a,b 设正交向量为A,BA, BA,B，接着我们以向量aaa为其中的正交向量AAA，则需求出正交向量BBB。 若要求出正交向量BBB，其实就是将bbb投影到aaa上，再求bbb到aaa的距离，也就是误差向量eee，也就是用向量bbb去减bbb在向量aaa上的分量，如图2所示 图2：向量b与向量a的误差向量e 根据上一节关于向量投影的知识可知： B=e=b−p=b−ATbATAAB=e=b-p=b-\\frac{A^Tb}{A^TA}AB=e=b−p=b−ATAATb​A 检验A⊥BA\\perp BA⊥B： ATB=ATb−ATATbATAA=ATb−ATbATAAb=0A^TB = A^Tb- A^T\\frac{A^Tb}{A^TA}A = A^Tb-\\frac{A^Tb}{A^TA}Ab=0ATB=ATb−ATATAATb​A=ATb−ATAATb​Ab=0 得到两个正交向量q1,q2q_1,q_2q1​,q2​： q1=A∣∣A∣∣,q2=B∣∣B∣∣q_1 = \\frac{A}{||A||}, \\quad q_2 = \\frac{B}{||B||}q1​=∣∣A∣∣A​,q2​=∣∣B∣∣B​ 三个向量的单位正交化 如果有三个向量要做单位正交化，那第三个向量需要垂直于前两个向量，我们设三个向量分别为a,b,ca,b,ca,b,c，三个相互正交的向量为A,B,CA,B,CA,B,C，三个相互正交的单位向量为q1,q2,q3q_1,q_2,q_3q1​,q2​,q3​，并令A=aA=aA=a，那么根据上一小节的内容，可以推出： B=b−ATbATAAC=c−p=c−pa−pb=c−ATcATAA−BTcBTBBB=b-\\frac{A^Tb}{A^TA}A \\\\ C=c-p=c-p_a-p_b = c-\\frac{A^Tc}{A^TA}A-\\frac{B^Tc}{B^TB}BB=b−ATAATb​AC=c−p=c−pa​−pb​=c−ATAATc​A−BTBBTc​B 示例 设有线性无关的非正交向量a,ba,ba,b，其中a=[111],b=[102]a = \\left[\\begin{matrix} 1 \\\\ 1 \\\\ 1 \\end{matrix}\\right],b = \\left[\\begin{matrix} 1 \\\\ 0 \\\\ 2 \\end{matrix}\\right]a=⎣⎡​111​⎦⎤​,b=⎣⎡​102​⎦⎤​，求两向量的标准正交矩阵QQQ。 正交化：我们设两正交向量为A,BA,BA,B，其中 A=a=[111]B=b−pa=b−ATbATAA=[102]−[111][112][111][111][111]=[102]−[111]=[0−11]A = a = \\left[\\begin{matrix} 1 \\\\ 1 \\\\ 1 \\end{matrix}\\right]\\\\ B=b-p_a = b-\\frac{A^Tb}{A^TA}A = \\left[\\begin{matrix} 1 \\\\ 0 \\\\ 2 \\end{matrix}\\right] - \\frac{\\left[\\begin{matrix} 1 &amp; 1 &amp; 1 \\end{matrix}\\right]\\left[\\begin{matrix} 1 \\\\ 1 \\\\ 2 \\end{matrix}\\right]}{\\left[\\begin{matrix} 1 &amp; 1 &amp; 1 \\end{matrix}\\right]\\left[\\begin{matrix} 1 \\\\ 1 \\\\ 1 \\end{matrix}\\right]}\\left[\\begin{matrix} 1 \\\\ 1 \\\\ 1 \\end{matrix}\\right]= \\left[\\begin{matrix} 1 \\\\ 0 \\\\ 2 \\end{matrix}\\right] -\\left[\\begin{matrix} 1 \\\\ 1 \\\\ 1 \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ -1 \\\\ 1 \\end{matrix}\\right]A=a=⎣⎡​111​⎦⎤​B=b−pa​=b−ATAATb​A=⎣⎡​102​⎦⎤​−[1​1​1​][111​][1​1​1​][112​]​⎣⎡​111​⎦⎤​=⎣⎡​102​⎦⎤​−⎣⎡​111​⎦⎤​=⎣⎡​0−11​⎦⎤​ 单位化：设两单位正交向量为q1,q2q_1,q_2q1​,q2​，则 q1=13[111]q2=12[0−11]q_1 = \\frac{1}{\\sqrt3}\\left[\\begin{matrix} 1 \\\\ 1 \\\\ 1 \\end{matrix}\\right] \\\\ q_2 = \\frac{1}{\\sqrt2}\\left[\\begin{matrix} 0 \\\\ -1 \\\\ 1 \\end{matrix}\\right]q1​=3​1​⎣⎡​111​⎦⎤​q2​=2​1​⎣⎡​0−11​⎦⎤​ 得到矩阵QQQ： Q=[13013−121312]Q = \\left[\\begin{matrix} \\frac{1}{\\sqrt3} &amp; 0 \\\\ \\frac{1}{\\sqrt3} &amp; -\\frac{1}{\\sqrt2} \\\\ \\frac{1}{\\sqrt3} &amp; \\frac{1}{\\sqrt2} \\end{matrix}\\right]Q=⎣⎢⎡​3​1​3​1​3​1​​0−2​1​2​1​​⎦⎥⎤​ QR分解 我们继续上文所述的示例，我们设原来的矩阵为A=[111012]A= \\left[\\begin{matrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ 1 &amp; 2 \\end{matrix}\\right]A=⎣⎡​111​102​⎦⎤​，然后来对比一下矩阵AAA和QQQ： A=[111012]Q=[13013−121312]A= \\left[\\begin{matrix} 1 &amp; 1 \\\\ 1 &amp; 0 \\\\ 1 &amp; 2 \\end{matrix}\\right] \\quad Q = \\left[\\begin{matrix} \\frac{1}{\\sqrt3} &amp; 0 \\\\ \\frac{1}{\\sqrt3} &amp; -\\frac{1}{\\sqrt2} \\\\ \\frac{1}{\\sqrt3} &amp; \\frac{1}{\\sqrt2} \\end{matrix}\\right]A=⎣⎡​111​102​⎦⎤​Q=⎣⎢⎡​3​1​3​1​3​1​​0−2​1​2​1​​⎦⎥⎤​ 会发现，AAA和QQQ的列空间是相同的，当然这是因为我们只是将原来的基标准正交化了。那么和LU分解表达了高斯消元法类似，上述示例中使用Gram-Schmidt正交化方法的过程也可以用矩阵的形式来表达，即A=QRA=QRA=QR，其中QQQ为标准正交矩阵，RRR为非奇异上三角矩阵。具体地，设矩阵AAA有列a,ba,ba,b，即A=[ab]A = \\left[\\begin{matrix} a &amp; b \\end{matrix}\\right]A=[a​b​]，那么我们可以直接写出矩阵QQQ和矩阵RRR，即 A=[ab]=QR=[q1q2][aTq1bTq1aTq2bTq2]A = \\left[\\begin{matrix} a &amp; b \\end{matrix}\\right] = QR = \\left[\\begin{matrix} q_1 &amp; q_2 \\end{matrix}\\right]\\left[\\begin{matrix} a^T q_1 &amp; b^T q_1 \\\\a^Tq_2 &amp; b^T q_2\\end{matrix}\\right]A=[a​b​]=QR=[q1​​q2​​][aTq1​aTq2​​bTq1​bTq2​​] 由于我们令aaa为两正交向量中的一个，因此a⊥ba\\perp ba⊥b，所以aTq2=0a^Tq_2 =0aTq2​=0，因此矩阵RRR是一个上三角矩阵，就有 A=[ab]=QR=[q1q2][aTq1bTq10bTq2]A = \\left[\\begin{matrix} a &amp; b \\end{matrix}\\right] = QR = \\left[\\begin{matrix} q_1 &amp; q_2 \\end{matrix}\\right]\\left[\\begin{matrix} a^T q_1 &amp; b^T q_1 \\\\0 &amp; b^T q_2\\end{matrix}\\right]A=[a​b​]=QR=[q1​​q2​​][aTq1​0​bTq1​bTq2​​] ","link":"https://albertlidesign.github.io/post/linearalgebra(5)/"},{"title":"线性代数(4)：最小二乘法","content":"在很多时候，我们在求解Ax=bAx=bAx=b方程时，如果A的列数太多，混入一些不准确的数据，此时方程时无解的，我们需要求解出最优解。这时候就需要使用最小二乘法来进行拟合。在讲解最小二乘法之前需要一些知识作为铺垫。 四个基本子空间 首先我们知道，对于一个m×nm×nm×n的矩阵AAA有四个子空间：列空间、行空间、零空间、左零空间。 列空间 (C(A)C(A)C(A))：列空间是RmR^mRm的子空间，是矩阵AAA的列向量的线性组合的集合构成的空间，每个列向量有mmm个分量，设矩阵AAA的秩为rrr，则AAA有rrr个主列，这rrr个主列就是列空间的一组基，一组基中有rrr个向量，所以列空间维数是rrr。 行空间（C(AT)C(A^T)C(AT)）：行空间是RnR^nRn的子空间，是矩阵AAA各行向量的线性组合的集合构成的空间，可以化为AAA转置的列空间，因此行空间的秩也是rrr，每个行向量有nnn个分量，所以行空间的维数也是r。 零空间（N(A)N(A)N(A)）：零空间是RnR^nRn的子空间,是由Ax=0Ax=0Ax=0的解所构成的空间。由于x本质是A列向量的线性组合，AAA一共有nnn个列向量，所以AAA的秩为rrr时，自由列有n−rn-rn−r列，xxx中有n−rn-rn−r个自由变元，也就有n−rn-rn−r个基向量，因此维数也是n−rn-rn−r。 左零空间（N(AT)N(A^T)N(AT)）：左零空间是RmR^mRm的子空间，是由ATy=0A^Ty = 0ATy=0的解的线性组合构成的空间，维数为m−rm-rm−r。 子空间与正交 接着，我们会发现，这四个子空间中存在正交关系。我们知道，在空间中证明两个向量是正交关系只需要判断两向量的内积是否为0。那么在子空间中也是如此。空间的正交定义是：一个空间中的任意一个向量都与另一个空间中的任意一个向量正交。 零空间与行空间是正交的，证明起来非常容易。我们将Ax=0Ax=0Ax=0写成下图的形式，很轻易就能证明，每一个行向量乘上xxx向量都为0，由于行向量是行空间的一组基，因此可以代表全部的行向量，这就满足了空间正交的定义。 [R1R2......Rm][x1x2......]=[R1(x1,x2...)R2(x1,x2...)........................Rm(x1,x2...)]=[00..0]\\left[\\begin{matrix}R_1 \\\\ R_2 \\\\ ... \\\\ ... \\\\ R_m\\end{matrix} \\right] \\left[\\begin{matrix}x_1 \\\\ x_2 \\\\ ... \\\\ ... \\end{matrix} \\right] = \\left[\\begin{matrix}R_1(x_1,x_2 ...) \\\\ R_2(x_1,x_2 ...) \\\\ ............ \\\\ ............ \\\\R_m(x_1,x_2 ...) \\end{matrix} \\right] = \\left[\\begin{matrix}0 \\\\ 0 \\\\ . \\\\.\\\\0 \\end{matrix} \\right] ⎣⎢⎢⎢⎢⎡​R1​R2​......Rm​​⎦⎥⎥⎥⎥⎤​⎣⎢⎢⎡​x1​x2​......​⎦⎥⎥⎤​=⎣⎢⎢⎢⎢⎡​R1​(x1​,x2​...)R2​(x1​,x2​...)........................Rm​(x1​,x2​...)​⎦⎥⎥⎥⎥⎤​=⎣⎢⎢⎢⎢⎡​00..0​⎦⎥⎥⎥⎥⎤​ 不光行空间与零空间是正交关系，我们也可以轻易推出列空间和左零空间也是正交关系。 我们假设在一个R3R^3R3空间中，不难想象，假设矩阵AAA的行空间是二维平面（r=2r=2r=2），那么其零空间就是一条线（n−r=1n-r = 1n−r=1），又因为是正交关系，所以可知，零空间就是行空间二维平面的法向。因此，行空间与零空间之间关系类似于将空间一分为二，得到两个正交的子空间，我们把这称为RnR^nRn空间的正交补。 子空间投影 向量-向量的投影问题 图1: 向量-向量投影问题。图片来源：作者自制 如图1，ppp为bbb在aaa上的投影，写作p=xap = xap=xa（xxx为倍数），根据向量减法得到，投影方向e=b−pe = b-pe=b−p。根据向量正交则内积为零的性质（aaa垂直于eee）可得 aTe=aT(b−p)=aT(b−ax)=0a^Te = a^T(b-p) = a^T(b-ax) = 0 aTe=aT(b−p)=aT(b−ax)=0 这时再将p=xap=xap=xa带入，可得 p=aaTbaTa=aaTaTabp = a \\frac{a^Tb}{a^Ta} = \\frac{aa^T}{a^Ta} b p=aaTaaTb​=aTaaaT​b 这里我们可以将aaTaTa\\frac{aa^T}{a^Ta}aTaaaT​看作一个矩阵，称为投影矩阵PPP。 向量-平面的投影问题 如图2，a1a1a1，a2a2a2为构成平面的一组基，ppp在平面上。所以有p=x1^a1+x2^a2p = \\hat{x_1}a_1 + \\hat{x_2}a_2p=x1​^​a1​+x2​^​a2​，或者直接写作p=Ax^p = A\\hat{x}p=Ax^，A=[a1a2]A=\\left[\\begin{matrix}a_1&amp;a_2\\end{matrix} \\right]A=[a1​​a2​​](其中ana_nan​为列向量)，x^=[x1^x2^]\\hat{x} = \\left[\\begin{matrix}\\hat{x_1}\\\\\\hat{x_2}\\end{matrix} \\right]x^=[x1​^​x2​^​​]。 图2: 向量-平面投影问题。图片来源：作者自制 由于eee是垂直于平面的向量，e=b−pe = b-pe=b−p，那么我们可以用内积形式表达eee和a1a_1a1​、a2a_2a2​的垂直关系：a1T(b−Ax^)=0a_1^T(b-A\\hat{x}) = 0a1T​(b−Ax^)=0、a2T(b−Ax^)=0a_2^T(b-A\\hat{x}) = 0a2T​(b−Ax^)=0，写成矩阵形式得： [a1Ta2T](b−Ax^)=[00]\\left[\\begin{matrix}a_1^T\\\\a_2^T\\end{matrix} \\right](b-A\\hat{x}) = \\left[\\begin{matrix}0\\\\0\\end{matrix} \\right] [a1T​a2T​​](b−Ax^)=[00​] 即 AT(b−Ax^)=0A^T(b-A\\hat{x}) = 0 AT(b−Ax^)=0 上文已述，列空间与左零空间是正交关系，图2中的向量a1a1a1、a2a2a2正是列空间中的向量，eee又垂直于这个平面，也就是说，eee位于AAA的左零空间中。 我们继续展开式子： ATb−ATAx^=0A^Tb-A^TA\\hat{x} = 0 ATb−ATAx^=0 得到： x^=(ATA)−1ATb\\hat{x} = (A^TA)^{-1}A^Tb x^=(ATA)−1ATb 代入p=Ax^p=A\\hat{x}p=Ax^得： p=Ax^=A(ATA)−1ATbp = A\\hat{x} = A(A^TA)^{-1}A^Tb p=Ax^=A(ATA)−1ATb 这样我们就得到了向量p与向量b之间的投影关系，进而得到了投影矩阵： P=A(ATA)−1ATP = A(A^TA)^{-1}A^T P=A(ATA)−1AT 这也是投影矩阵的一般情况。那么PPP有哪些性质呢？我们知道ATAA^TAATA是一个n×nn×nn×n矩阵，并且是可逆的对称矩阵。对称矩阵的转置还是它本身，因此观察公式我们能得出 PT=PP^T = PPT=P 接着，根据图像，如果我们将b投影到A的列空间上两次，得到的结果依然是ppp，因此有 P2=PP^2 = PP2=P 证明： P2=P×P=A(ATA)−1ATA(ATA)−1AT=A(ATA)−1AT=PP^2 = P×P = A(A^TA)^{-1}A^T A(A^TA)^{-1}A^T = A(A^TA)^{-1}A^T=PP2=P×P=A(ATA)−1ATA(ATA)−1AT=A(ATA)−1AT=P 那么投影矩阵有什么用呢？举例来说，它应该能产生出一个投影，如果让PPP和bbb相乘，此时在极端情况下，如果bbb位于列空间中时，那么有Pb=bPb=bPb=b；如果bbb垂直于列空间，那么有Pb=0Pb=0Pb=0，此时的bbb就在左零空间中。一般情况下会有一个分量在列空间里，另一个分量在左零空间中。 因此，一个向量bbb总有两个分量，一个分量在AAA的列空间中（也就是ppp），另一个分量垂直于AAA的列空间（位于左零空间，也就是eee）。而投影矩阵的作用就是保留列空间中的那个分量，拿掉垂直于列空间的分量。 由此我们可以分别表示出ppp和eee： p=Pbp = Pb p=Pb e=b−p=b−Pb=(I−P)be = b-p = b-Pb = (I-P)b e=b−p=b−Pb=(I−P)b 这里(I-P)也可以看作是一个投影矩阵，作用于bbb向量，投影到左零空间中。eee被表示出来后，我们就可以控制误差了。 最小二乘法 有了上述知识的铺垫接下来就可以进入最小二乘法了。其实最小二乘法就是一种投影，最后就是保证误差最小，使用最小二乘法其实就是为了解决Ax=bAx=bAx=b无解的情况。 我们从一个案例开始： 例. 求解：三个点(1,1)(1,1)(1,1)，(2,2)(2,2)(2,2)，(3,2)(3,2)(3,2)拟合的直线方程 我们设最优直线方程为：b=C+Dtb=C+Dtb=C+Dt，带入三个点得：C+D=1C+D=1C+D=1、C+2D=2C+2D=2C+2D=2、C+3D=2C+3D=2C+3D=2 写成矩阵形式为： [111213][CD]=[122]\\left[\\begin{matrix}1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{matrix} \\right] \\left[\\begin{matrix}C \\\\ D \\end{matrix} \\right] = \\left[\\begin{matrix}1 \\\\ 2\\\\2 \\end{matrix} \\right] ⎣⎡​111​123​⎦⎤​[CD​]=⎣⎡​122​⎦⎤​ 其中， A=[111213],x^=[C^D^],b=[122]A = \\left[\\begin{matrix}1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{matrix} \\right], \\hat{x} = \\left[\\begin{matrix}\\hat{C} \\\\ \\hat{D} \\end{matrix} \\right], b= \\left[\\begin{matrix}1 \\\\ 2\\\\2 \\end{matrix} \\right] A=⎣⎡​111​123​⎦⎤​,x^=[C^D^​],b=⎣⎡​122​⎦⎤​ 列出矩阵方程后会发现Ax=bAx=bAx=b无解，为了求近似的x^\\hat{x}x^，由于Ax=bAx = bAx=b可能无解，那么只能求解最接近的那个可解问题，因为AxAxAx总是在列空间里，而bbb不一定在，所以我们要把bbb变为列空间中最为接近的向量，这样就把问题转化为了Ax^=pA\\hat{x} = pAx^=p，误差就是投影距离e=b−pe = b-pe=b−p。就有 ATb=ATAx^A^Tb = A^TA\\hat{x} ATb=ATAx^ 求解： ATA=[111123][111213]=[36614]A^TA = \\left[\\begin{matrix}1 &amp; 1 &amp;1 \\\\ 1 &amp; 2 &amp; 3 \\end{matrix} \\right] \\left[\\begin{matrix}1 &amp; 1 \\\\ 1 &amp; 2 \\\\ 1 &amp; 3 \\end{matrix} \\right] = \\left[\\begin{matrix}3 &amp; 6 \\\\ 6 &amp; 14 \\end{matrix} \\right] ATA=[11​12​13​]⎣⎡​111​123​⎦⎤​=[36​614​] ATb=[111123][122]=[511]A^Tb = \\left[\\begin{matrix}1 &amp; 1 &amp;1 \\\\ 1 &amp; 2 &amp; 3 \\end{matrix} \\right] \\left[\\begin{matrix}1\\\\2 \\\\ 2\\end{matrix} \\right] = \\left[\\begin{matrix} 5 \\\\ 11 \\end{matrix} \\right] ATb=[11​12​13​]⎣⎡​122​⎦⎤​=[511​] 解得： x^=[2312]\\hat{x} = \\left[\\begin{matrix} \\frac{2}{3} \\\\ \\frac{1}{2} \\end{matrix} \\right] x^=[32​21​​] 固拟合直线为：y=C^+D^x=23+12xy =\\hat{C} +\\hat{D}x = \\frac{2}{3}+ \\frac{1}{2}xy=C^+D^x=32​+21​x 我们也可以通过求偏导，求极值，从导数的角度来求得拟合直线，可以先表示出误差∣Ax−b∣|Ax-b|∣Ax−b∣，为了便于计算，我们去求它们的平方和： ∣e∣2=∣Ax−b∣2|e|^2 = |Ax-b|^2 ∣e∣2=∣Ax−b∣2 那么总误差E=e12+e22+e32E = e_1^2+e_2^2+e_3^2E=e12​+e22​+e32​，求EEE的最小值EminE_{min}Emin​即可。 求出C^\\hat{C}C^和D^\\hat{D}D^后三个点各自的误差也可以求得，只需带入三个点的坐标即可得到三个ppp，通过e=b−pe = b-pe=b−p，最终我们得到了： b=[122],p=[76106136],e=[−1626−16]b = \\left[\\begin{matrix} 1 \\\\ 2\\\\2 \\end{matrix} \\right], p = \\left[\\begin{matrix} \\frac{7}{6}\\\\\\frac{10}{6}\\\\\\frac{13}{6} \\end{matrix} \\right], e = \\left[\\begin{matrix} -\\frac{1}{6}\\\\\\frac{2}{6}\\\\-\\frac{1}{6} \\end{matrix} \\right] b=⎣⎡​122​⎦⎤​,p=⎣⎡​67​610​613​​⎦⎤​,e=⎣⎡​−61​62​−61​​⎦⎤​ 我们就得到了如下性质： 误差向量eee与投影向量ppp垂直； 误差向量eee还垂直于列空间中的每一个向量。 最后需要注意，最小二乘法存在一定的局限性，那就是最小二乘法很容易受到离群量的影响。如果数据中有误差过大的点，那么它会对整个结果带来巨大的影响。 ","link":"https://albertlidesign.github.io/post/linearalgebra(4)/"},{"title":"线性代数(3)：Ax=0和Ax=b","content":"求解Ax=0 消元法求解零空间 那么我们如何求解Ax=0Ax=0Ax=0呢？还是使用消元法，之前我们说使用消元法求解方程Ax=bAx=bAx=b时，我们对一种情况是无法处理的，那就是矩阵AAA不可逆的情况，之前对这种情况的解释是求出的解不唯一，这其实正好对应了现在我们所认识到的“空间”的概念。我们从最简单的零空间（b=0b=0b=0）的计算谈起。 例1：Ax=[1222246836810]Ax = \\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 2 &amp; 4&amp; 6 &amp; 8 \\\\ 3 &amp; 6 &amp; 8 &amp; 10 \\end{matrix}\\right]Ax=⎣⎡​123​246​268​2810​⎦⎤​，求Ax=0Ax=0Ax=0中的xxx构成的零空间 先将方程写出，如下 Ax=[1222246836810][x1x2x3x4]=0Ax = \\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 2 &amp; 4&amp; 6 &amp; 8 \\\\ 3 &amp; 6 &amp; 8 &amp; 10 \\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{matrix}\\right] = 0 Ax=⎣⎡​123​246​268​2810​⎦⎤​⎣⎢⎢⎡​x1​x2​x3​x4​​⎦⎥⎥⎤​=0 首先观察矩阵AAA我们发现，第三行是前两行的和，这意味着即使主元为000，我们也得继续消元下去。那么按部就班，有 [1222246836810]⇒[122200240024]⇒[122200240000](阶梯形式)=U\\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 2 &amp; 4&amp; 6 &amp; 8 \\\\ 3 &amp; 6 &amp; 8 &amp; 10 \\end{matrix}\\right] \\Rightarrow \\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 0 &amp; 0&amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 2 &amp; 4 \\end{matrix}\\right] \\Rightarrow \\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 0 &amp; 0&amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right] (阶梯形式)= U ⎣⎡​123​246​268​2810​⎦⎤​⇒⎣⎡​100​200​222​244​⎦⎤​⇒⎣⎡​100​200​220​240​⎦⎤​(阶梯形式)=U 在消元的过程中，我们发现矩阵AAA的主元（Pivot）数量为222（111和222），主元的个数称为矩阵的秩（Rank)，因此在本题中矩阵AAA的秩为222。 接下来就是回代求解了，由于消元得到的UUU不是一个严格的上三角矩阵，对角线上的000给我们造成了解不唯一的麻烦，所以这里我们先来声明几个概念 [122200240000]\\left[\\begin{array}{c|c|c|c} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 0 &amp; 0&amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right]⎣⎡​100​200​220​240​⎦⎤​中，列[100]\\left[\\begin{matrix} 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right]⎣⎡​100​⎦⎤​和[220]\\left[\\begin{matrix} 2 \\\\ 2 \\\\ 0 \\end{matrix}\\right]⎣⎡​220​⎦⎤​被称为主列（Pivot Columns，主元所在的列），其余两列[200]\\left[\\begin{matrix}2 \\\\ 0 \\\\ 0 \\end{matrix}\\right]⎣⎡​200​⎦⎤​和[240]\\left[\\begin{matrix} 2 \\\\ 4 \\\\ 0 \\end{matrix}\\right]⎣⎡​240​⎦⎤​被称为自由列（Free Columns），所谓自由列就是表示其对应的未知变量xnx_nxn​（nnn表示自由列是第nnn列）可以被任意分配值。因为回代求解时，只有主列对应的未知数的解有确定值。因此矩阵AAA中的主变量（主元）为x1x_1x1​和x3x_3x3​，x2x_2x2​和x4x_4x4​为自由变量。 （1）我们假设，令[x2x4]=[10]\\left[\\begin{matrix} x_2 \\\\ x_4 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right][x2​x4​​]=[10​]，代入方程 {x1+2x2+2x3+2x4=02x3+4x4=0\\begin{cases} x_1+2x_2+2x_3+2x_4=0 \\\\2x_3+4x_4=0 \\end{cases} {x1​+2x2​+2x3​+2x4​=02x3​+4x4​=0​ 解得 {x1=−2x3=0\\begin{cases} x_1=-2 \\\\x_3= 0 \\end{cases} {x1​=−2x3​=0​ 因此当[x2x4]=[10]\\left[\\begin{matrix} x_2 \\\\ x_4 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right][x2​x4​​]=[10​]时，解向量为[−2100]\\left[\\begin{matrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right]⎣⎢⎢⎡​−2100​⎦⎥⎥⎤​，这只是零空间中的一个解，这个解表示−2-2−2倍的列111+++111倍的列222=0=0=0，如果想找出更多零向量中的解，我们只需要求它的倍数，所以x=c[−2100](c为任意实数)x=c\\left[\\begin{matrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right](c为任意实数)x=c⎣⎢⎢⎡​−2100​⎦⎥⎥⎤​(c为任意实数)，这是一条在四维空间中无限延伸的直线，但它不是整个零空间。 （2）我们再令[x2x4]=[01]\\left[\\begin{matrix} x_2 \\\\ x_4 \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right][x2​x4​​]=[01​]，代入方程 {x1+2x2+2x3+2x4=02x3+4x4=0\\begin{cases} x_1+2x_2+2x_3+2x_4=0 \\\\2x_3+4x_4=0 \\end{cases} {x1​+2x2​+2x3​+2x4​=02x3​+4x4​=0​ 解得 {x1=2x3=−2\\begin{cases} x_1=2 \\\\x_3=-2 \\end{cases} {x1​=2x3​=−2​ 因此当[x2x4]=[01]\\left[\\begin{matrix} x_2 \\\\ x_4 \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right][x2​x4​​]=[01​]时，解向量为[20−21]\\left[\\begin{matrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{matrix}\\right]⎣⎢⎢⎡​20−21​⎦⎥⎥⎤​，因此另一条在四维空间中的直线为x=d[20−21]（d为任意实数）x=d\\left[\\begin{matrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{matrix}\\right]（d为任意实数）x=d⎣⎢⎢⎡​20−21​⎦⎥⎥⎤​（d为任意实数） 那么还能为[x2x4]\\left[\\begin{matrix} x_2 \\\\ x_4 \\end{matrix}\\right][x2​x4​​]赋其他值吗？很明显其他情况都可以被[10]\\left[\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right][10​]和[01]\\left[\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right][01​]的线性组合所涵盖，所以这两个解向量足够代表空间的特征了，我们称这两个解向量为特解，其特殊之处在于我们给自由变量赋值为[10]\\left[\\begin{matrix} 1 \\\\ 0 \\end{matrix}\\right][10​]和[01]\\left[\\begin{matrix} 0 \\\\ 1 \\end{matrix}\\right][01​]。通过特解的任意倍的线性组合，可以构造出整个零空间。因此便得出了矩阵AAA的零空间 x=c[−2100]+d[20−21](c和d为常数)x=c\\left[\\begin{matrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right]+d\\left[\\begin{matrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{matrix}\\right](c和d为常数)x=c⎣⎢⎢⎡​−2100​⎦⎥⎥⎤​+d⎣⎢⎢⎡​20−21​⎦⎥⎥⎤​(c和d为常数) 算法总结 对于一个m×nm×nm×n的矩阵A，若其秩为rrr，那么意味着其主变量为rrr个，而自由变量为n−rn-rn−r个。也就是说，只有rrr列起作用。我们需要先对矩阵AAA进行消元，得到rrr个主元，由于有nnn个变量xxx，我们再将其中的n−rn-rn−r个自由变量依次赋值为[10⋮0]、[01⋮0]、[00⋮1]\\left[\\begin{matrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{matrix}\\right]、\\left[\\begin{matrix} 0 \\\\ 1 \\\\ \\vdots \\\\ 0 \\end{matrix}\\right]、\\left[\\begin{matrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\end{matrix}\\right]⎣⎢⎢⎢⎡​10⋮0​⎦⎥⎥⎥⎤​、⎣⎢⎢⎢⎡​01⋮0​⎦⎥⎥⎥⎤​、⎣⎢⎢⎢⎡​00⋮1​⎦⎥⎥⎥⎤​。接着求解方程的特解，将特解的任意倍进行线性组合即可得到矩阵AAA的零空间。 简化阶梯形式 尽管上面的消元法看上去已经很完美了，但事实上仍有化简的余地，最后得到的UUU矩阵仍可以被进一步化简。我们以上文中的U=[122200240000]U=\\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 0 &amp; 0&amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right]U=⎣⎡​100​200​220​240​⎦⎤​为例，继续化简的目标是令对角线上的主元为1，并且通过列交换将主元放在一起，把自由列放在一起来构成新的矩阵，操作如下 U=[122200240000]=[120−200240000](向上消元)=[120−200120000](提第二行公倍数)=[102−201020000](列交换)=RU=\\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 0 &amp; 0&amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 &amp; 2 &amp; 0 &amp; -2 \\\\ 0 &amp; 0&amp; 2 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right](向上消元) = \\left[\\begin{matrix} 1 &amp; 2 &amp; 0 &amp; -2 \\\\ 0 &amp; 0&amp; 1 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right](提第二行公倍数) = \\left[\\begin{matrix} 1 &amp; 0 &amp; 2 &amp; -2 \\\\ 0 &amp; 1 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right](列交换) = RU=⎣⎡​100​200​220​240​⎦⎤​=⎣⎡​100​200​020​−240​⎦⎤​(向上消元)=⎣⎡​100​200​010​−220​⎦⎤​(提第二行公倍数)=⎣⎡​100​010​200​−220​⎦⎤​(列交换)=R 也就是说最终我们能将上三角矩阵UUU化简成矩阵RRR，矩阵RRR的一般形式为 R=[IF00]R = \\left[\\begin{matrix} I &amp; F \\\\ 0 &amp; 0\\end{matrix}\\right] R=[I0​F0​] 其中，III表示主列，由于rrr个主列的主元被化简成了111，因此这部分变成了rrr维单位矩阵，FFF表示自由列，共有n−rn-rn−r个自由列。有了矩阵RRR我们可以改写AxAxAx的表达形式 Ax=Rx=[IF00][x主元x自由变量]=RN=0Ax = Rx = \\left[\\begin{matrix} I &amp; F \\\\ 0 &amp; 0\\end{matrix}\\right] \\left[\\begin{matrix} x_{主元} \\\\ x_{自由变量} \\end{matrix}\\right] = RN = 0 Ax=Rx=[I0​F0​][x主元​x自由变量​​]=RN=0 这里的NNN为零空间矩阵，即各列向量由特解组成的矩阵 N=[−FI]N = \\left[\\begin{matrix} -F \\\\ I \\end{matrix}\\right] N=[−FI​] 需要注意的是，这里的单位矩阵和矩阵RRR中的有所不同，这里的III是n−rn-rn−r维的，是将n−rn-rn−r个自由变量分别赋值为000或111得到的。将上文中的示例代入到RRR和NNN，得到 R=[102−201020000],N=[−220−21001]R = \\left[\\begin{matrix} 1 &amp; 0 &amp; 2 &amp; -2 \\\\ 0 &amp; 1 &amp; 0 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 \\end{matrix}\\right],\\quad N=\\left[\\begin{matrix} -2 &amp; 2 \\\\ 0 &amp; -2 \\\\ 1 &amp; 0\\\\ 0 &amp; 1\\end{matrix}\\right] R=⎣⎡​100​010​200​−220​⎦⎤​,N=⎣⎢⎢⎡​−2010​2−201​⎦⎥⎥⎤​ 由于x1x_1x1​和x3x_3x3​是主列，x2x_2x2​和x4x_4x4​是自由列，因此只需交换零空间矩阵中的第2、3行即可得到特解[−2100]\\left[\\begin{matrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right]⎣⎢⎢⎡​−2100​⎦⎥⎥⎤​和[20−21]\\left[\\begin{matrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{matrix}\\right]⎣⎢⎢⎡​20−21​⎦⎥⎥⎤​。**因此将矩阵UUU化简称矩阵RRR可以直接求解零空间。**我们用下面一个例题来试验一下： 例 A=[1232462682810]A = \\left[\\begin{matrix} 1 &amp; 2 &amp; 3 \\\\ 2 &amp; 4 &amp; 6 \\\\ 2 &amp; 6 &amp; 8 \\\\ 2 &amp; 8 &amp; 10\\end{matrix}\\right]A=⎣⎢⎢⎡​1222​2468​36810​⎦⎥⎥⎤​，求解Ax=0Ax=0Ax=0中xxx构成的零空间。 （1）将AAA消元为UUU：$A = \\left[\\begin{matrix} 1 &amp; 2 &amp; 3 \\ 2 &amp; 4 &amp; 6 \\ 2 &amp; 6 &amp; 8 \\ 2 &amp; 8 &amp; 10\\end{matrix}\\right] = \\left[\\begin{matrix} 1 &amp; 2 &amp; 3 \\ 0 &amp; 2 &amp; 2 \\ 0 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; 0\\end{matrix}\\right] = U (r = 2) $ （2）将UUU化简为RRR：U=[123022000000]=[101011000000]=RU = \\left[\\begin{matrix} 1 &amp; 2 &amp; 3 \\\\ 0 &amp; 2 &amp; 2 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{matrix}\\right] = \\left[\\begin{matrix} 1 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 1 \\\\ 0 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 0\\end{matrix}\\right] = RU=⎣⎢⎢⎡​1000​2200​3200​⎦⎥⎥⎤​=⎣⎢⎢⎡​1000​0100​1100​⎦⎥⎥⎤​=R （3）得到零空间矩阵NNN：N=[−FI]=[−1−11]N = \\left[\\begin{matrix} -F \\\\ I \\end{matrix}\\right] = \\left[\\begin{matrix} -1 \\\\ -1 \\\\ 1 \\end{matrix}\\right]N=[−FI​]=⎣⎡​−1−11​⎦⎤​ （4）得到零空间：x=c[−1−11](c为任意实数)x=c\\left[\\begin{matrix} -1 \\\\ -1 \\\\ 1 \\end{matrix}\\right](c为任意实数)x=c⎣⎡​−1−11​⎦⎤​(c为任意实数) 求解Ax=b Ax=b的可解性 对于Ax=bAx=bAx=b我们知道这个方程不一定有解，在之前的章节中说明了Ax=bAx=bAx=b是否有解取决于bbb是否在AAA的列空间中，我们再通过一个例子来说明一下 例 求方程[1222246836810][x1x2x3x4]=[b1b2b3]\\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 2 &amp; 4 &amp; 6 &amp; 8 \\\\ 3 &amp; 6 &amp; 8 &amp; 10 \\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{matrix}\\right] = \\left[\\begin{matrix} b_1 \\\\ b_2 \\\\ b_3 \\end{matrix}\\right]⎣⎡​123​246​268​2810​⎦⎤​⎣⎢⎢⎡​x1​x2​x3​x4​​⎦⎥⎥⎤​=⎣⎡​b1​b2​b3​​⎦⎤​的可解条件。 在这个方程中，观察矩阵A，发现矩阵中第三行为第一行和第二行的和。根据之前的Gauss-Jordan消元法，我们可以得到 [1222b10024b2−b10000b3−b2−b1]\\left[\\begin{array}{cccc|c} 1 &amp; 2 &amp; 2 &amp; 2 &amp; b_1 \\\\ 0 &amp; 0 &amp; 2 &amp; 4 &amp; b_2-b_1 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; b_3- b_2-b_1 \\end{array}\\right]⎣⎡​100​200​220​240​b1​b2​−b1​b3​−b2​−b1​​⎦⎤​ 代入方程，会发现最后一行0=b3−b2−b10 = b_3-b_2-b_10=b3​−b2​−b1​，这一行方程必须成立，因此这一行就是方程的可解条件。同时，它还反映了bbb向量的第三个分量是前两个分量之和，这也与矩阵AAA的特点一致，这也印证了Ax=bAx=bAx=b是否有解取决于bbb是否在AAA的列空间中。 结合之前的章节总结出Ax=bAx=bAx=b有解条件： 列空间角度：当且仅当bbb属于AAA的列空间时成立 线性组合角度：当且仅当bbb是AAA各列的线性组合时成立 矩阵变换角度：如果AAA各行线性组合后得到零行，那么bbb取相同运算方式也必将得到000 求解Ax=b 接下来介绍通解和特解，通解就是满足方程所有的解，将“无穷解”用一种形式表达出来，对于Ax=bAx=bAx=b这个方程 通解=矩阵零空间向量+矩阵特解通解=矩阵零空间向量+矩阵特解 通解=矩阵零空间向量+矩阵特解 因为矩阵零空间向量代入方程最后结果等于000，所以它不会影响等式，而是把方程的解向量扩展到一个类似子空间上，使我们求出的解更具有普遍意义，而求解零空间我们在上文也已经介绍，下面我们只需要关注如何求特解即可。在之前求解Ax=0Ax=0Ax=0方程的特解时，我们分别将自由变量赋值为000或111，得到 x=c[−2100]+d[20−21](c和d为任意实数)x=c\\left[\\begin{matrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right]+d\\left[\\begin{matrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{matrix}\\right](c和d为任意实数)x=c⎣⎢⎢⎡​−2100​⎦⎥⎥⎤​+d⎣⎢⎢⎡​20−21​⎦⎥⎥⎤​(c和d为任意实数) 观察这个表达式会发现，只要将系数ccc和ddd定为000就可以得到零空间中的零向量，而且我们不能在求解Ax=0Ax=0Ax=0时将自由变元都赋为000。但是在Ax=bAx=bAx=b中，只要bbb不是000，我们就可以将自由变元全部赋为000，使用此方法即可得到特解。 接下来补充上述例题中方程的条件 [1222246836810][x1x2x3x4]=[156]\\left[\\begin{matrix} 1 &amp; 2 &amp; 2 &amp; 2 \\\\ 2 &amp; 4 &amp; 6 &amp; 8 \\\\ 3 &amp; 6 &amp; 8 &amp; 10 \\end{matrix}\\right]\\left[\\begin{matrix} x_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 \\\\ 5 \\\\ 6 \\end{matrix}\\right]⎣⎡​123​246​268​2810​⎦⎤​⎣⎢⎢⎡​x1​x2​x3​x4​​⎦⎥⎥⎤​=⎣⎡​156​⎦⎤​ Gauss-Jordan消元后得到 [122210024400000]\\left[\\begin{array}{cccc|c} 1 &amp; 2 &amp; 2 &amp; 2 &amp; 1 \\\\ 0 &amp; 0 &amp; 2 &amp; 4 &amp; 4 \\\\ 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\end{array}\\right]⎣⎡​100​200​220​240​140​⎦⎤​ 将[x2x4]=[00]\\left[\\begin{matrix} x_2 \\\\ x_4 \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 0 \\end{matrix}\\right][x2​x4​​]=[00​]回代方程得到 {x1+2x3=12x3=3\\begin{cases} x_1+2x_3=1 \\\\2x_3=3 \\end{cases} {x1​+2x3​=12x3​=3​ 解得特解为[−20320]\\left[\\begin{matrix} -2 \\\\ 0 \\\\ \\frac{3}{2} \\\\ 0 \\end{matrix}\\right]⎣⎢⎢⎡​−2023​0​⎦⎥⎥⎤​ 利用上一节的知识我们很容易求出AAA的零空间为 c1[−2100]+c2[20−21](c1和c2为任意实数)c_1\\left[\\begin{matrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right]+c_2\\left[\\begin{matrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{matrix}\\right](c_1和c_2为任意实数)c1​⎣⎢⎢⎡​−2100​⎦⎥⎥⎤​+c2​⎣⎢⎢⎡​20−21​⎦⎥⎥⎤​(c1​和c2​为任意实数) 因此Ax=bAx=bAx=b的解为 特解+零空间任意向量=[−20320]+c1[−2100]+c2[20−21](c1和c2为任意实数)特解+零空间任意向量=\\left[\\begin{matrix} -2 \\\\ 0 \\\\ \\frac{3}{2} \\\\ 0 \\end{matrix}\\right] + c_1\\left[\\begin{matrix} -2 \\\\ 1 \\\\ 0 \\\\ 0 \\end{matrix}\\right]+c_2\\left[\\begin{matrix} 2 \\\\ 0 \\\\ -2 \\\\ 1 \\end{matrix}\\right](c_1和c_2为任意实数) 特解+零空间任意向量=⎣⎢⎢⎡​−2023​0​⎦⎥⎥⎤​+c1​⎣⎢⎢⎡​−2100​⎦⎥⎥⎤​+c2​⎣⎢⎢⎡​20−21​⎦⎥⎥⎤​(c1​和c2​为任意实数) 这个解集在几何角度的解释是R4R^4R4上的一个不过原点的二维平面，显然这个解集无法构成一个向量空间，因为解集中不包含零向量。 矩阵的秩与解的关系 我们在消元求Ax=bAx=bAx=b的过程中会发现，矩阵的秩对最后解的形式有着重要的影响，下面我们来总结一下其中的规律。 列满秩 对于m×nm×nm×n的矩阵AAA，列满秩时，意味着没有自由列，r=n&lt;mr=n&lt;mr=n&lt;m，此时零空间中只有零向量（不需要求零空间），Ax=bAx=bAx=b的解要么有解且唯一（特解xpx_pxp​），要么无解。例如 A=[13216151]r=2=n&lt;mA = \\left[\\begin{matrix} 1 &amp; 3 \\\\ 2 &amp; 1 \\\\ 6 &amp; 1 \\\\ 5 &amp; 1 \\end{matrix}\\right] \\quad r=2=n&lt;mA=⎣⎢⎢⎡​1265​3111​⎦⎥⎥⎤​r=2=n&lt;m 在消元过程中，由于两列线性无关，因此只有两个主元，逐行减去第一行的若干倍，行三和行四清零，得到第二个主元，然后各行都减去第二个主元的若干倍，最终第二个主元化为111的得到矩阵RRR A=[13216151]=[10010000]=[I0]=RA = \\left[\\begin{matrix} 1 &amp; 3 \\\\ 2 &amp; 1 \\\\ 6 &amp; 1 \\\\ 5 &amp; 1 \\end{matrix}\\right] = \\left[\\begin{matrix} 1 &amp; 0 \\\\ 0 &amp; 1 \\\\ 0 &amp; 0 \\\\ 0 &amp; 0 \\end{matrix}\\right] = \\left[\\begin{matrix} I \\\\ 0 \\end{matrix}\\right]=RA=⎣⎢⎢⎡​1265​3111​⎦⎥⎥⎤​=⎣⎢⎢⎡​1000​0100​⎦⎥⎥⎤​=[I0​]=R 行满秩 对于m×nm×nm×n的矩阵AAA，行满秩时，意味着有mmm个主元（每一行各一个），r=m&lt;nr = m&lt;nr=m&lt;n，此时自由变元有n−rn-rn−r个，必然有解而且有无穷多解，例如 A=[12653111]r=2=m&lt;nA = \\left[\\begin{matrix} 1 &amp; 2 &amp; 6 &amp; 5 \\\\ 3 &amp; 1 &amp; 1 &amp; 1 \\end{matrix}\\right] \\quad r=2=m&lt;nA=[13​21​61​51​]r=2=m&lt;n 最后我们会消元得到R=[IF]R=\\left[\\begin{matrix} I &amp; F \\end{matrix}\\right]R=[I​F​] 行列满秩 对于m×nm×nm×n的矩阵AAA，行列满秩时，意味着矩阵可逆，r=m=nr = m = nr=m=n，此时自由变元有000个，经过消元，最终矩阵可化为单位矩阵III，即一个全是主元的方程组，最终只能有一个唯一解。例如 A=[1231]r=2=m=nA = \\left[\\begin{matrix} 1 &amp; 2 \\\\ 3 &amp; 1\\end{matrix}\\right] \\quad r=2=m=nA=[13​21​]r=2=m=n 最后消元得到R=IR=IR=I 不满秩 对于m×nm×nm×n的矩阵AAA，不满秩时，意味着通过消元最终会得到R=[IF00]R = \\left[\\begin{matrix} I &amp; F \\\\ 0 &amp; 0\\end{matrix}\\right]R=[I0​F0​]，因此方程的解要么无解，要么无穷多解（特解+零空间所有向量） 小结 综上所述，会发现自由变量总为n−rn-rn−r个，所以通过判断自由变元的个数可以初步判断Ax=bAx=bAx=b的解的结构：如果没有自由变元，意味着方程的解唯一或者无解；如果存在自由变元，意味着方程的解有无穷多解或者无解。也就是说，自由变元是否存在决定了方程的解是否唯一。另一点是，可以通过观察消元后矩阵AAA是否存在000行来进一步判断方程是否有解：如果矩阵AAA中没有零行时，意味着方程一定有解；如果存在零行，则需要考虑方程是否满足可解条件。 除此之外，我们还发现了零空间实际上就是用来判断矩阵AAA的各列向量是否是线性无关的，如果各列向量是线性无关的，那么零空间中只有零向量，如果各列向量是线性相关的，那么零空间中除了零向量还有其他向量。因此零空间反映的就是AAA各列向量的线性组合。 关于Ax=b的另一种解释 当我们求解方程时，例如 {2x−y=0−x+2y=3\\begin{cases} 2x-y=0 \\\\ -x+2y=3 \\end{cases} {2x−y=0−x+2y=3​ 矩阵表达如下 [2−1−12][xy]=[03]\\left[\\begin{matrix} 2 &amp; -1 \\\\ -1 &amp; 2\\end{matrix}\\right]\\left[\\begin{matrix} x \\\\ y \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 3 \\end{matrix}\\right][2−1​−12​][xy​]=[03​] 除了使用消元法或判断矩阵是否满秩以外，我们还可以从列空间的角度来看这个方程，改写一些这个矩阵表达如下 x[2−1]+y[−12]=[03]x\\left[\\begin{matrix} 2 \\\\ -1 \\end{matrix}\\right] + y \\left[\\begin{matrix} -1 \\\\ 2 \\end{matrix}\\right] = \\left[\\begin{matrix} 0 \\\\ 3 \\end{matrix}\\right]x[2−1​]+y[−12​]=[03​] 那么我们判断这个方程是否有解的条件实际上就是判断向量[03]\\left[\\begin{matrix} 0 \\\\ 3 \\end{matrix}\\right][03​]是否在以向量[2−1]\\left[\\begin{matrix} 2 \\\\ -1 \\end{matrix}\\right][2−1​]和向量[−12]\\left[\\begin{matrix} -1 \\\\ 2 \\end{matrix}\\right][−12​]构成的列空间中，换句话说，向量[03]\\left[\\begin{matrix} 0 \\\\ 3 \\end{matrix}\\right][03​]是否可以表达成向量[2−1]\\left[\\begin{matrix} 2 \\\\ -1 \\end{matrix}\\right][2−1​]和向量[−12]\\left[\\begin{matrix} -1 \\\\ 2 \\end{matrix}\\right][−12​]的线性组合。由于向量[2−1]\\left[\\begin{matrix} 2 \\\\ -1 \\end{matrix}\\right][2−1​]和向量[−12]\\left[\\begin{matrix} -1 \\\\ 2 \\end{matrix}\\right][−12​]是线性相关的，因此可以张成一个二维平面，而向量[03]\\left[\\begin{matrix} 0 \\\\ 3 \\end{matrix}\\right][03​]只是其中的一个二维向量，因此可以推断出方程一定有解。 ","link":"https://albertlidesign.github.io/post/linearalgebra(3)/"},{"title":"线性代数(2)：列空间与零空间","content":"向量空间 向量空间的概念 向量空间表示一整个空间的向量，然而，这并不意味着任意向量的集合就能被称为向量空间。所谓向量空间，必须满足空间对线性运算封闭（也即是相加和数乘）这一原则。 举个例子，R2R^2R2就是一个向量空间，它表示所有的二维实向量，如果你对它们做线性运算，会发现得到的结果仍然位于R2R^2R2空间中，反应在图像上如图所示： 图1. 二维向量空间中的两个向量 很明显，R2R^2R2向量空间构成了一个平面，[32],[00],[πe]\\left[\\begin{matrix}3 \\\\ 2\\end{matrix}\\right],\\left[\\begin{matrix}0\\\\ 0\\end{matrix}\\right],\\left[\\begin{matrix}\\pi \\\\ e\\end{matrix}\\right][32​],[00​],[πe​]均在R2R^2R2的实数二维向量空间中，对它们做线性运算，得到的结果仍然在R2R^2R2空间中。这个向量空间存在的关键在于，平面上任何向量都在R2R^2R2向量空间中，尤其零向量。因为线性运算是“数乘”和“相加”，而任何向量乘上000或者加上其相反的反向向量得到的都是零向量，所以零向量必然存在于所有向量空间中。 推广到高维空间中，则RnR^nRn空间中包含所有的nnn维向量，每个列向量中有nnn个分量，且分量均为实数。 那么如果我们把xoyxoyxoy坐标平面上的第一象限单独拿出来，这个区域DDD仍然是向量空间吗？ 不是的，因为该空间无法满足“线性组合仍在空间中”的要求，比如做数乘运算时，随便取个负数得到的向量就会位于第三象限，因此空间DDD不能称为向量空间，也就是说，在向量空间中任取一部分，得到的结果可能不是向量空间。 那么是否我们在一个向量空间中取子区域一定无法构成向量空间呢？ 也不是的。例如图2，我们在二维向量空间中取一条过原点的直线，即可得到二维向量空间中的子空间，这条直线上的任意向量，进行线性运算后所得到的结果依然在这条直线上，并且过原点，包含了零向量，因此它是一个子空间。相反，如果这个直线不过原点，那么就不能称为子空间，因为它不包含零向量。 图2. 二维向量空间中的子空间 除此之外，R2R^2R2空间中还存在其他子空间，比如零向量本身也是一个子空间。推广至三维，我们就可以得到R3R^3R3空间中的子空间：（1）过原点的平面；（2）过原点的直线；（3）零向量。 那么两个向量空间的并集，其结果可以构成子空间吗？ 答案是否定的，假设我们有一条过原点的直线LLL和一个过原点的平面PPP（直线LLL不在平面PPP上），二者均可以视作向量空间，而两者的并集并不能满足线性运算封闭，因为在直线LLL上任取一向量aaa，在平面PPP上任取一向量bbb，两向量的和会位于直线与平面之间，脱离了两空间并集的范围。 那么两个向量空间的交集，其结果可以构成子空间吗？ 答案是肯定的，因为两个向量空间本身就必包含零向量，并且已经称为向量空间，其交集的条件将更为严苛，也必满足两个向量空间的条件，因此可以构成一个子空间。如上一个例子，直线LLL和平面PPP的交基是零向量，零向量依然为向量空间。 列空间与零空间 以上向量空间都是通过图像的形式来描述的，但是对于高维度，我们无法通过作图的方式来绘制其图像，因此需要借助矩阵来描述向量空间。 列空间 列空间是指由矩阵的列向量所构造出的空间。 例如A=[112213314415]A = \\left[\\begin{matrix}1 &amp;1 &amp;2 \\\\2&amp;1&amp;3\\\\ 3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]A=⎣⎢⎢⎡​1234​1111​2345​⎦⎥⎥⎤​，矩阵的列向量均是R4R^4R4空间中的四维向量，所以可以说**AAA的列空间是R4R^4R4的子空间**。在这个列空间中，除了包含了三个列向量外，还包含了它们的各种线性组合，也就是说，A的列空间是由[1234],[1111],[2345]\\left[\\begin{matrix}1 \\\\2\\\\ 3 \\\\4\\end{matrix}\\right],\\left[\\begin{matrix}1 \\\\1\\\\ 1 \\\\1\\end{matrix}\\right],\\left[\\begin{matrix}2 \\\\3\\\\ 4 \\\\5\\end{matrix}\\right]⎣⎢⎢⎡​1234​⎦⎥⎥⎤​,⎣⎢⎢⎡​1111​⎦⎥⎥⎤​,⎣⎢⎢⎡​2345​⎦⎥⎥⎤​三个向量所张成的子空间。那么这个空间有多大呢？这就需要用Ax=bAx=bAx=b来解释了。 Ax=b的空间解释（从A的角度） 还是以A=[112213314415]A = \\left[\\begin{matrix}1 &amp;1 &amp;2 \\\\2&amp;1&amp;3\\\\ 3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]A=⎣⎢⎢⎡​1234​1111​2345​⎦⎥⎥⎤​为例，假设有一个方程Ax=bAx=bAx=b如下： Ax=[112213314415][x1x2x3]=[b1b2b3b4]=bAx = \\left[\\begin{matrix}1 &amp;1 &amp;2 \\\\2&amp;1&amp;3\\\\ 3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]\\left[\\begin{matrix}x_1 \\\\x_2\\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix}b_1 \\\\b_2\\\\ b_3 \\\\b_4\\end{matrix}\\right]=bAx=⎣⎢⎢⎡​1234​1111​2345​⎦⎥⎥⎤​⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎢⎢⎡​b1​b2​b3​b4​​⎦⎥⎥⎤​=b 那么第一个问题是这个方程是否有解？ 我们看到，我们可以把方程改写成如下形式： Ax=[112213314415][x1x2x3]=x1[1234]+x2[1111]+x3[2345]Ax = \\left[\\begin{matrix}1 &amp;1 &amp;2 \\\\2&amp;1&amp;3\\\\ 3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]\\left[\\begin{matrix}x_1 \\\\x_2\\\\ x_3 \\end{matrix}\\right] = x_1\\left[\\begin{matrix}1 \\\\2\\\\ 3 \\\\4\\end{matrix}\\right] + x_2\\left[\\begin{matrix}1 \\\\1\\\\ 1 \\\\1\\end{matrix}\\right] + x_3\\left[\\begin{matrix}2 \\\\3\\\\ 4 \\\\5\\end{matrix}\\right]Ax=⎣⎢⎢⎡​1234​1111​2345​⎦⎥⎥⎤​⎣⎡​x1​x2​x3​​⎦⎤​=x1​⎣⎢⎢⎡​1234​⎦⎥⎥⎤​+x2​⎣⎢⎢⎡​1111​⎦⎥⎥⎤​+x3​⎣⎢⎢⎡​2345​⎦⎥⎥⎤​ AxAxAx的本质就是对AAA的列向量进行线性组合，或者可以认为，AxAxAx就代表着AAA的列空间。显然，三个四维向量的线性组合是无法填满整个四维空间的，就如同两个三维向量无法张成一个三维空间一样。因此，这里的AxAxAx只能是R4R^4R4空间的部分子空间，也就是说，无法保证任意拿出一个四维向量b=[b1b2b3b4]∈R4b = \\left[\\begin{matrix}b_1 \\\\b_2\\\\ b_3 \\\\b_4\\end{matrix}\\right]\\in R^4b=⎣⎢⎢⎡​b1​b2​b3​b4​​⎦⎥⎥⎤​∈R4，都能找到AAA列向量的一种线性组合，使Ax=bAx=bAx=b。 第二个问题是什么样的bbb可以使Ax=bAx=bAx=b有解？ 上面介绍过，AxAxAx的本质就是对AAA的列向量进行线性组合​，也是AAA的列空间，且是R4R^4R4空间中的子空间，那么只要向量bbb位于矩阵AAA的列空间中，就可以找到一种由AAA的列向量通过线性组合来构成的向量bbb，也就使得Ax=bAx=bAx=b有解。 第三个问题是能否去掉AAA中的一列，却不影响AAA的列空间呢？ 首先来观察这三个向量[1234],[1111],[2345]\\left[\\begin{matrix}1 \\\\2\\\\ 3 \\\\4\\end{matrix}\\right],\\left[\\begin{matrix}1 \\\\1\\\\ 1 \\\\1\\end{matrix}\\right],\\left[\\begin{matrix}2 \\\\3\\\\ 4 \\\\5\\end{matrix}\\right]⎣⎢⎢⎡​1234​⎦⎥⎥⎤​,⎣⎢⎢⎡​1111​⎦⎥⎥⎤​,⎣⎢⎢⎡​2345​⎦⎥⎥⎤​，显然满足等式：[2345]=[1234]+[1111]\\left[\\begin{matrix}2 \\\\3\\\\ 4 \\\\5\\end{matrix}\\right] = \\left[\\begin{matrix}1 \\\\2\\\\ 3 \\\\4\\end{matrix}\\right]+\\left[\\begin{matrix}1 \\\\1\\\\ 1 \\\\1\\end{matrix}\\right]⎣⎢⎢⎡​2345​⎦⎥⎥⎤​=⎣⎢⎢⎡​1234​⎦⎥⎥⎤​+⎣⎢⎢⎡​1111​⎦⎥⎥⎤​，也就是说第三列向量本身就是前两列向量的和，因此第三列向量对张成空间没有做任何贡献，仅仅依靠前两列就足以构成AAA的列空间，我们称前两列为主列，所以去掉第三列不影响AAA的列空间的构成。最后，从矩阵AAA的角度，AAA的列空间可以描述为R4R^4R4中的二维子空间。 零空间 零空间是Ax=0Ax=0Ax=0的所有解所构成的一个空间。 我们还是以A=[112213314415]A = \\left[\\begin{matrix}1 &amp;1 &amp;2 \\\\2&amp;1&amp;3\\\\ 3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]A=⎣⎢⎢⎡​1234​1111​2345​⎦⎥⎥⎤​为例，其零空间就是下面这个方程的解所构成的空间： Ax=[112213314415][x1x2x3]=[0000]=0Ax = \\left[\\begin{matrix}1 &amp;1 &amp;2 \\\\2&amp;1&amp;3\\\\ 3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]\\left[\\begin{matrix}x_1 \\\\x_2\\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix}0 \\\\0\\\\ 0 \\\\0\\end{matrix}\\right] = 0Ax=⎣⎢⎢⎡​1234​1111​2345​⎦⎥⎥⎤​⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎢⎢⎡​0000​⎦⎥⎥⎤​=0 由于零空间是解所构成的空间，因此我们要从xxx的角度来看，可以看到xxx有三个分量，所以其零空间是R3R^3R3的子空间。所以，对于m×nm×nm×n的矩阵来说，列空间是RmR^mRm的子空间，零空间是RnR^nRn的子空间。列空间取决于列向量的维数，零空间取决于列向量的个数。 那么我们来验证一下为什么[x1x2x3]\\left[\\begin{matrix}x_1 \\\\x_2\\\\ x_3 \\end{matrix}\\right]⎣⎡​x1​x2​x3​​⎦⎤​可以构成向量空间呢？首先它满足了加法封闭：在零空间中任取两向量v,wv,wv,w，都有Av=Aw=0Av=Aw=0Av=Aw=0，显然A(v+w)=0A(v+w)=0A(v+w)=0，所以向量(v+w)(v+w)(v+w)也属于零空间。其次它满足了数乘封闭：还是在零空间中任取一向量vvv，Av=0Av=0Av=0，则cAv=0cAv=0cAv=0。由于矩阵AAA和常数ccc的位置可以交换，所以A(cv)=0A(cv)=0A(cv)=0，所以向量cvcvcv也在零空间中。 回到我们的示例，我们知道矩阵AAA中的三个向量[1234],[1111],[2345]\\left[\\begin{matrix}1 \\\\2\\\\ 3 \\\\4\\end{matrix}\\right],\\left[\\begin{matrix}1 \\\\1\\\\ 1 \\\\1\\end{matrix}\\right],\\left[\\begin{matrix}2 \\\\3\\\\ 4 \\\\5\\end{matrix}\\right]⎣⎢⎢⎡​1234​⎦⎥⎥⎤​,⎣⎢⎢⎡​1111​⎦⎥⎥⎤​,⎣⎢⎢⎡​2345​⎦⎥⎥⎤​，显然满足等式：[2345]=[1234]+[1111]\\left[\\begin{matrix}2 \\\\3\\\\ 4 \\\\5\\end{matrix}\\right] = \\left[\\begin{matrix}1 \\\\2\\\\ 3 \\\\4\\end{matrix}\\right]+\\left[\\begin{matrix}1 \\\\1\\\\ 1 \\\\1\\end{matrix}\\right]⎣⎢⎢⎡​2345​⎦⎥⎥⎤​=⎣⎢⎢⎡​1234​⎦⎥⎥⎤​+⎣⎢⎢⎡​1111​⎦⎥⎥⎤​，因此我们可以直接写出Ax=0Ax=0Ax=0的一个解：[11−1]\\left[\\begin{matrix}1 \\\\1 \\\\-1\\end{matrix}\\right]⎣⎡​11−1​⎦⎤​，所以其零空间即为c[11−1]c\\left[\\begin{matrix}1 \\\\1 \\\\-1\\end{matrix}\\right]c⎣⎡​11−1​⎦⎤​（ccc为任意常数），反应在图像上，就是R3R^3R3中的一条过原点的直线。 Ax=b的空间解释（从x的角度） 如果将构造零空间的方程中等号右侧变为任意向量的话，其解xxx还能构成向量空间吗？ Ax=[112213314415][x1x2x3]=[1234]=bAx = \\left[\\begin{matrix}1 &amp;1 &amp;2 \\\\2&amp;1&amp;3\\\\ 3&amp;1&amp;4\\\\4&amp;1&amp;5\\end{matrix}\\right]\\left[\\begin{matrix}x_1 \\\\x_2\\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix}1 \\\\2\\\\ 3 \\\\4\\end{matrix}\\right] = bAx=⎣⎢⎢⎡​1234​1111​2345​⎦⎥⎥⎤​⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎢⎢⎡​1234​⎦⎥⎥⎤​=b 显然不是。因为如果我们将[x1x2x3]=[000]\\left[\\begin{matrix}x_1 \\\\x_2\\\\ x_3 \\end{matrix}\\right] = \\left[\\begin{matrix}0 \\\\0\\\\ 0 \\end{matrix}\\right]⎣⎡​x1​x2​x3​​⎦⎤​=⎣⎡​000​⎦⎤​代入方程，会发现零向量并不是方程的解，也就是说解集中没有零向量，也就无法构成向量空间，反映在图像上，这里所有的解构成了一个不过原点的平面。这也告诉我们，想从xxx的角度来研究Ax=bAx=bAx=b，只有当bbb是零向量时，xxx的解集才能构成空间（零空间），其他情况中连零向量都不在解集中，也就无法构成向量空间了。所有，要么从AAA的列向量入手，已知列向量，根据其线性组合构成子空间，要么从方程Ax=0Ax=0Ax=0入手，从满足条件的xxx的解集来构造子空间。 ","link":"https://albertlidesign.github.io/post/linearalgebra(2)/"},{"title":"线性代数(1)：LU分解","content":"矩阵分解 矩阵分解（Matrix Factorizations）就是将一个矩阵用两个以上的矩阵相乘的等式来表达。而矩阵乘法涉及到数据的合成(即将两个或多个线性变换的效果组合成一个矩阵)，因此可以说，矩阵分解是对数据的一种分析。 矩阵分解 LU分解 分解形式 A=LUA=LUA=LU(LLL代表下三角矩阵，UUU代表上三角矩阵) 目的 提高计算效率 前提 (1)矩阵是方阵（LULULU分解主要是针对方阵）；(2)矩阵是可逆的，也就是该矩阵是满秩矩阵，每一行都是独立向量；(3)消元过程中没有000主元出现，也就是消元过程中不能出现行交换的初等变换。 LU分解 分解形式 A=LU=[1000l21100l31l3210l41l42l431][u11u12u13u140u22u23u2400u33u34000u44]A = LU = \\left[\\begin{matrix} 1 &amp; 0 &amp; 0 &amp; 0 \\\\ l_{21} &amp; 1 &amp; 0 &amp; 0 \\\\ l_{31} &amp; l_{32} &amp; 1 &amp; 0 \\\\ l_{41} &amp; l_{42} &amp; l_{43} &amp; 1\\end{matrix}\\right]\\left[\\begin{matrix} u_{11} &amp; u_{12} &amp; u_{13} &amp; u_{14} \\\\ 0 &amp; u_{22} &amp; u_{23} &amp; u_{24} \\\\ 0 &amp; 0 &amp; u_{33} &amp; u_{34} \\\\ 0 &amp; 0 &amp; 0 &amp; u_{44}\\end{matrix}\\right] A=LU=⎣⎢⎢⎡​1l21​l31​l41​​01l32​l42​​001l43​​0001​⎦⎥⎥⎤​⎣⎢⎢⎡​u11​000​u12​u22​00​u13​u23​u33​0​u14​u24​u34​u44​​⎦⎥⎥⎤​ 简述 LULULU分解常用于求解工业和商业问题中的序列方程。它是最常见的求解线性系统 Ax=bAx=bAx=b 的方法，主要思路是：把AAA 分解成一个下三角矩阵（Lower Triangular Matrix）和一个上三角矩阵（Upper Triangular Matrix），简称LULULU。分解后，等式Ax=bAx=bAx=b可以写成L(Ux)=bL(Ux)=bL(Ux)=b，这样我们令y=Uxy=Uxy=Ux，这样就可以通过分开求解两个等式来得到xxx： Ly=bUx=yLy=b \\\\ Ux=y Ly=bUx=y 即可以先通过Ly=bLy=bLy=b求出yyy，最后再用Ux=yUx=yUx=y，求出xxx。 本质上，LULULU分解是高斯消元法的一种表达方式，为了更好地理解LULULU分解，我们需要解释一下高斯消元法。 高斯消元法（Gaussian Elimination） 简述 为了求解线性系统，我们基本的策略是用相等的式子去替代要求解的式子，来让求解变得更容易。对于一个线性系统中有若干个未知数，我们需要将方程组中的一方程的未知数用含有另一未知数的代数式表示，并将其代入到另一方程中，这就消去了一未知数，得到一解；或将方程组中的一方程倍乘某个常数加到另外一方程中去，也可达到消去一未知数的目的，并最终得到线性系统的解。这一求解算法称之为高斯消元法（Gaussian Elimination）。 示例 下文将展示高斯消元法的求解过程，设需求解的线性方程组如下： {x1−2x2+x3=02x2−8x3=85x1−5x3=10\\begin{cases} x_1-2x_2+x_3=0 \\\\ 2x_2-8x_3=8\\\\5x_1-5x_3=10 \\end{cases} ⎩⎪⎨⎪⎧​x1​−2x2​+x3​=02x2​−8x3​=85x1​−5x3​=10​ 将其表达为矩阵形式为： [1−21002−8850−510]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 5 &amp; 0 &amp; -5 &amp;10 \\end{array}\\right] ⎣⎡​105​−220​1−8−5​0810​⎦⎤​ 为了将多余的未知数x1x_1x1​消除，我们需要将第三行的555消除，矩阵的第一行乘−5-5−5倍再和第三行相加，得到： [1−21002−88010−1010]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 10 &amp; -10 &amp; 10 \\end{array}\\right] ⎣⎡​100​−2210​1−8−10​0810​⎦⎤​ 接着再进行对多余的未知数x2x_2x2​的消除，这里将第二行乘−5-5−5倍再与第三行相加，得到： [1−21002−880030−30]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 0 &amp; 30 &amp; -30 \\end{array}\\right] ⎣⎡​100​−220​1−830​08−30​⎦⎤​ 至此，我们得到了一个新的三角形矩阵，转换为线性方程组的形式为： {x1−2x2+x3=0x2−4x3=4x3=−1\\begin{cases} x_1-2x_2+x_3=0 \\\\ x_2-4x_3=4\\\\x_3=-1 \\end{cases} ⎩⎪⎨⎪⎧​x1​−2x2​+x3​=0x2​−4x3​=4x3​=−1​ 通过简单的回代，我们最终可以得到线性方程组的解： {x1=1x2=0x3=−1[10010100001−1]\\begin{cases} x_1=1 \\\\ x_2=0\\\\x_3=-1 \\end{cases} \\quad\\quad\\quad \\left[\\begin{array}{ccc|c} 1 &amp; 0 &amp; 0 &amp; 1 \\\\ 0 &amp; 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right] ⎩⎪⎨⎪⎧​x1​=1x2​=0x3​=−1​⎣⎡​100​010​001​10−1​⎦⎤​ 高斯消元法与LU分解之间的联系 那么为什么说LULULU分解其实就是高斯消元法的一种表达方式呢？这是因为我们可以将上述示例中每一步的操作用矩阵相乘的形式（行变换的本质就是左乘矩阵）表达出来。 例如上文示例中的第一步：“为了将多余的未知数x1x_1x1​消除，我们需要将第三行的555消除，矩阵的第一行乘−5-5−5倍再和第三行相加” ，可以表达为： [100010−501][1−21002−8850−510]=[1−21002−88010−1010]\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -5 &amp; 0 &amp; 1 \\end{matrix}\\right] \\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 5 &amp; 0 &amp; -5 &amp;10 \\end{array}\\right]=\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 10 &amp; -10 &amp; 10 \\end{array}\\right] ⎣⎡​10−5​010​001​⎦⎤​⎣⎡​105​−220​1−8−5​0810​⎦⎤​=⎣⎡​100​−2210​1−8−10​0810​⎦⎤​ 同样地，第二步：“继续进行对多余的未知数x2x_2x2​的消除，这里将第二行乘−5-5−5倍再与第三行相加” 可以表达为： [1000100−51][1−21002−88010−1010]=[1−21002−880030−30]\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -5 &amp; 1 \\end{matrix}\\right] \\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 10 &amp; -10 &amp; 10 \\end{array}\\right] = \\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 0 &amp; 30 &amp; -30 \\end{array}\\right] ⎣⎡​100​01−5​001​⎦⎤​⎣⎡​100​−2210​1−8−10​0810​⎦⎤​=⎣⎡​100​−220​1−830​08−30​⎦⎤​ 因此，我们可以用一个矩阵相乘的等式来记录整个高斯消元的过程： [1000100−51][100010−501][1−21002−8850−510]=[1−21002−880030−30]\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 0 &amp; -5 &amp; 1 \\end{matrix}\\right]\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -5 &amp; 0 &amp; 1 \\end{matrix}\\right]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 5 &amp; 0 &amp; -5 &amp;10 \\end{array}\\right]=\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 0 &amp; 30 &amp; -30 \\end{array}\\right] ⎣⎡​100​01−5​001​⎦⎤​⎣⎡​10−5​010​001​⎦⎤​⎣⎡​105​−220​1−8−5​0810​⎦⎤​=⎣⎡​100​−220​1−830​08−30​⎦⎤​ 将三个消元矩阵合并，用EEE表示则为： EA=[100010−5−51][1−21002−8850−510]=[1−21002−880030−30]=UEA =\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ -5 &amp; -5 &amp; 1 \\end{matrix}\\right]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 5 &amp; 0 &amp; -5 &amp;10 \\end{array}\\right]=\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 0 &amp; 30 &amp; -30 \\end{array}\\right] = U EA=⎣⎡​10−5​01−5​001​⎦⎤​⎣⎡​105​−220​1−8−5​0810​⎦⎤​=⎣⎡​100​−220​1−830​08−30​⎦⎤​=U 那么我们可以得到关于AAA的表达式： A=[1−21002−8850−510]=E−1U=[100010551][1−21002−880030−30]=LUA = \\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 5 &amp; 0 &amp; -5 &amp;10 \\end{array}\\right] = E^{-1}U = \\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 5 &amp; 5 &amp; 1 \\end{matrix}\\right]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 0 &amp; 30 &amp; -30 \\end{array}\\right] = LU A=⎣⎡​105​−220​1−8−5​0810​⎦⎤​=E−1U=⎣⎡​105​015​001​⎦⎤​⎣⎡​100​−220​1−830​08−30​⎦⎤​=LU 这样一来我们就得到了AAA的LULULU分解，即将矩阵A分解为一个下三角矩阵（LLL）和一个上三角矩阵（UUU）的乘积。 伪代码 U = A, L = I for j = 1 : n - 1 do for i = j + 1 : n do l_ij = u_ij / u_jj for k = j : n do u_ik = u_ik - l_ij * u_jk end for end for end for 此处是矩阵index从1开始计数。 Credit to http://iacs-courses.seas.harvard.edu/courses/am205/slides/am205_lec07.pdf 算法效率 接下来我们来思考一下算法效率的问题，假如我们有一个100∗100100*100100∗100的矩阵，并且各项均非000，那我们需要计算多少次才能得到矩阵UUU呢？ 对于这个问题我们先从列的角度来考虑，第一列消元运算结束以后，矩阵变为： [1⋯⋯⋯0⋱⋯⋯⋮⋮⋱⋮0⋯⋯⋱]\\left[\\begin{matrix} 1 &amp; \\cdots &amp; \\cdots &amp; \\cdots \\\\ 0 &amp; \\ddots &amp; \\cdots &amp; \\cdots \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; \\cdots &amp; \\cdots &amp; \\ddots\\end{matrix}\\right] ⎣⎢⎢⎢⎡​10⋮0​⋯⋱⋮⋯​⋯⋯⋱⋯​⋯⋯⋮⋱​⎦⎥⎥⎥⎤​ 这一步中，第一列的元素运算了100100100次，而第一行共有100100100个元素，于是仅第一行与第一列消元结束后，我们就计算了1002100^21002次。之后我们要研究剩下的99∗9999*9999∗99的矩阵，以此类推可知，最后的计算量为∑k=1nk2\\sum^{n}_{k=1}{k^2}∑k=1n​k2。 接着，我们可以利用微积分计算得到，AAA的操作次数为13n3\\frac{1}{3}n^331​n3，bbb的操作次数为n2n^2n2次。 结合上文伪代码， 因为有三重循环嵌套，因此LULULU分解的复杂度为O(n3)O(n^3)O(n3)，其中加法操作为13n3\\frac{1}{3}n^331​n3，乘法操作为13n3\\frac{1}{3}n^331​n3，总共为23n3\\frac{2}{3}n^332​n3次，求解bbb和yyy各需要n2n^2n2次 前提条件 (1)矩阵是方阵（LU分解主要是针对方阵）；(2)矩阵是可逆的，也就是该矩阵是满秩矩阵，每一行都是独立向量；(3)消元过程中没有0主元出现，也就是消元过程中不能出现行交换的初等变换。 LUD分解（LDU Decomposition） 上文中我们已经得到了A=LUA=LUA=LU，我们还可以继续分解下去得到A=LDUA=LDUA=LDU，DDD为对角矩阵(Diagonal Matrix)，矩阵的LDULDULDU分解是在LULULU分解之后，把UUU再次分解，目的是把UUU的对角线元素都化为111，上文示例中我们得到的A=LUA=LUA=LU如下 A=[1−21002−8850−510]=LU=[100010551][1−21002−880030−30]A = \\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 5 &amp; 0 &amp; -5 &amp;10 \\end{array}\\right] = LU = \\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 5 &amp; 5 &amp; 1 \\end{matrix}\\right]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 0 &amp; 30 &amp; -30 \\end{array}\\right] A=⎣⎡​105​−220​1−8−5​0810​⎦⎤​=LU=⎣⎡​105​015​001​⎦⎤​⎣⎡​100​−220​1−830​08−30​⎦⎤​ 接下来，让UUU矩阵中位于对角线上的元素都为111，就可以提出一个对角矩阵DDD LU=[100010551][1−21002−880030−30]=[100010551][1000200030][1−21001−44001−1]=LDULU = \\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 5 &amp; 5 &amp; 1 \\end{matrix}\\right]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 2 &amp; -8 &amp; 8 \\\\ 0 &amp; 0 &amp; 30 &amp; -30 \\end{array}\\right] =\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 1 &amp; 0 \\\\ 5 &amp; 5 &amp; 1 \\end{matrix}\\right]\\left[\\begin{matrix} 1 &amp; 0 &amp; 0 \\\\ 0 &amp; 2 &amp; 0 \\\\ 0 &amp; 0 &amp; 30 \\end{matrix}\\right]\\left[\\begin{array}{ccc|c} 1 &amp; -2 &amp; 1 &amp; 0 \\\\ 0 &amp; 1 &amp; -4 &amp; 4 \\\\ 0 &amp; 0 &amp; 1 &amp; -1 \\end{array}\\right]=LDU LU=⎣⎡​105​015​001​⎦⎤​⎣⎡​100​−220​1−830​08−30​⎦⎤​=⎣⎡​105​015​001​⎦⎤​⎣⎡​100​020​0030​⎦⎤​⎣⎡​100​−210​1−41​04−1​⎦⎤​=LDU LUP分解（LU Decomposition with Partial Pivoting） 我们来考虑一个矩阵$ A=\\left[\\begin{matrix} 0 &amp; 1 \\ 1 &amp; 1 \\end{matrix}\\right]，尽管矩阵，尽管矩阵，尽管矩阵A非奇异，并且很简单，但是使用非奇异，并且很简单，但是使用非奇异，并且很简单，但是使用LU分解的算法会失败，因为第一个主元分解的算法会失败，因为第一个主元分解的算法会失败，因为第一个主元A_{0,0} = 0$。在高斯消元法中，我们常常通过人为的行变换来消除主元为000时的影响。例如我们需要把矩阵A的第一行和第二行进行交换得到$ A=\\left[\\begin{matrix} 1 &amp; 1 \\ 0 &amp; 1 \\end{matrix}\\right]，这样我们就能将消元进行下去。这样的操作也可以用矩阵相乘的方法来表达，那么在该示例中，即为，这样我们就能将消元进行下去。这样的操作也可以用矩阵相乘的方法来表达，那么在该示例中，即为，这样我们就能将消元进行下去。这样的操作也可以用矩阵相乘的方法来表达，那么在该示例中，即为PA=[0110][0111]=[1101]PA=\\left[\\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 0 \\end{matrix}\\right]\\left[\\begin{matrix} 0 &amp; 1 \\\\ 1 &amp; 1 \\end{matrix}\\right]=\\left[\\begin{matrix} 1 &amp; 1 \\\\ 0 &amp; 1 \\end{matrix}\\right]PA=[01​10​][01​11​]=[10​11​]，这里的矩阵，这里的矩阵，这里的矩阵P称为置换矩阵（PermutationMatrix）。那么就有了称为置换矩阵（Permutation Matrix）。那么就有了称为置换矩阵（PermutationMatrix）。那么就有了LUP分解（LUdecompositionwithpartialpivoting）：分解（LU decomposition with partial pivoting）：分解（LUdecompositionwithpartialpivoting）：PA = LU。这一算法的出现显著改善了。这一算法的出现显著改善了。这一算法的出现显著改善了LU$分解算法的稳定性。 伪代码 U = A, L = I, P = I for j = 1 : n - 1 do Select i (&gt;= j) that maximizes |u_ij| Interchange rows of U: u_(j,j:n) &lt;-&gt; u_(i,j:n) Interchange rows of L: l_(j,1:j-1) &lt;-&gt; l_(i,1:j-1) Interchange rows of P: p_(j,:) &lt;-&gt; p_(i,:) for i = j + 1 : n do l_ij = u_ij / u_jj for k = j : n do u_ik = u_ik - l_ij * u_jk end for end for end for ","link":"https://albertlidesign.github.io/post/LinearAlgebra(1)/"},{"title":"About Me","content":"Welcome to AlbertLiDesign. This is a blog for random braindumps on my programming adventures, graphics and design research, some of my more serious writings and other interesting things. About Me I'm a prospective PhD student in RMIT. My research invovles the intersection of architectural design and computer graphics, especially in mesh algorithms. I am interested in understanding both the theoretical properties and implements of these problems, as well as experimental results from real architectural applications. My current goal is to develop fast geometric processing library and a topology optimization program to improve the possibility of applying BESO algorithm to actual architectural design. Experiences Studied in Tianjin University Bachelor of Art, Environmental Design, School of Architecture (2014 – 2018) Worked for Heroes Architects &amp; Engineers Co., Ltd. Intern Architect (2016 - 2017) Participated in making an animation of research and development project about a high-rise fire-fighting robot. Working about rendering and animation using Grasshopper and Lumion. Worked for XIE Archi-Structure Design (Shanghai) Co., Ltd. Assistant Research &amp; Development Engineer, XIE Design Lab (2018 – 2019) Participated in Ameba development project. Ameba is a software based on the Bi-directional Evolutionary Structural Optimization (BESO) technique originally proposed by Professor Yi-Min (Mike) Xie. The user may, according to design requirements, apply different loading and boundary conditions to the initial design domain. During the computational process by the software, the design domain will evolve and eventually reach an organic form that is structurally efficient; Developed plenty of mesh optimization functions. As a system of Ameba's post-process, it is efficient to provide a complete set of solutions for optimizing results of topology optimization. Will Study in RMIT Prospective PhD student, Civil Engineering, School of Engineering (2020 – ) Senior Supervisor Yimin Xie, Associate Supervisor Roland Snooks BTW I have been waiting for my PhD visa for more than 8 months, but no result or response. ","link":"https://albertlidesign.github.io/post/aboutme/"}]}